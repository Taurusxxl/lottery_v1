{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6df2e0d6-ef49-43ac-ae15-b95190004353",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "配置验证：\n",
      "数据库配置: {'host': 'localhost', 'user': 'root', 'port': 3306, 'password': 'tt198803'}\n",
      "训练配置: {'max_epochs': 1, 'batch_size': 1}\n"
     ]
    }
   ],
   "source": [
    "#1 配置管理器\\config_manager.py\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, Any\n",
    "\n",
    "class ConfigManager:\n",
    "    \"\"\"配置管理器\"\"\"\n",
    "    _instance = None\n",
    "    \n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self):\n",
    "        if not hasattr(self, 'initialized'):\n",
    "            # 基础路径配置\n",
    "            self.BASE_DIR = 'D:\\\\JupyterWork'\n",
    "            self.LOG_DIR = os.path.join(self.BASE_DIR, 'logs')\n",
    "            self.MODEL_DIR = os.path.join(self.BASE_DIR, 'models')\n",
    "            self.DATA_DIR = os.path.join(self.BASE_DIR, 'data')\n",
    "            self.CHECKPOINT_DIR = os.path.join(self.BASE_DIR, 'checkpoints')\n",
    "            \n",
    "            # 创建必要的目录\n",
    "            for dir_path in [self.LOG_DIR, self.MODEL_DIR, self.DATA_DIR, self.CHECKPOINT_DIR]:\n",
    "                os.makedirs(dir_path, exist_ok=True)\n",
    "            \n",
    "            # 数据库配置\n",
    "            self.DB_CONFIG: dict = {\n",
    "                'host': 'localhost',\n",
    "                'user': 'root',  # 修改为MySQL用户\n",
    "                'port': 3306,    # 修改为MySQL端口\n",
    "                'password': 'tt198803'  # 添加密码\n",
    "            }\n",
    "            \n",
    "            # 训练配置\n",
    "            self.TRAINING_CONFIG: dict = {\n",
    "                'max_epochs': 1,\n",
    "                'batch_size': 1\n",
    "            }\n",
    "            \n",
    "            # 系统配置\n",
    "            self.SYSTEM_CONFIG = {\n",
    "                'memory_limit': 8000,  # MB\n",
    "                'gpu_memory_limit': 4000,  # MB\n",
    "                'cleanup_interval': 300,  # seconds\n",
    "                'log_retention_days': 7,\n",
    "                'check_interval': 60,  # 检查新期号的间隔\n",
    "                'AUTO_TUNING': {\n",
    "                    'enable_per_sample': True,  # 启用逐样本调整\n",
    "                    'adjustment_steps': 5,      # 每个样本最大调整次数\n",
    "                    'learning_rate_range': (1e-5, 1e-2)  # 学习率调整范围\n",
    "                },\n",
    "                'DATA_CONFIG': {\n",
    "                    'cache_size': 10000,\n",
    "                    'normalize_range': (-1, 1)\n",
    "                },\n",
    "                'max_sequence_gap': 1,          # 允许的最大期号间隔\n",
    "                'max_threads': 8,  # 留出4线程给系统\n",
    "                'base_batch_size': 16,  # 初始批次\n",
    "                'gpu_mem_limit': 1536,  # MB (保留500MB给系统)\n",
    "                'cpu_util_threshold': 70,  # CPU使用率阈值\n",
    "                'SAMPLE_CONFIG': {\n",
    "                    'input_length': 144000,  # 修改这里\n",
    "                    'target_length': 2880,   # 修改这里\n",
    "                    'total_fetch': lambda: (  # 自动计算总获取量\n",
    "                        self.SYSTEM_CONFIG['SAMPLE_CONFIG']['input_length'] \n",
    "                        + self.SYSTEM_CONFIG['SAMPLE_CONFIG']['target_length']\n",
    "                    )\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # 在配置中添加\n",
    "            self.OPTUNA_CONFIG = {\n",
    "                \"storage\": \"sqlite:///optuna.db\",\n",
    "                \"study_name\": \"prod_study_v1\",\n",
    "                \"timeout\": 3600  # 设置优化超时\n",
    "            }\n",
    "            \n",
    "            self.initialized = True\n",
    "    \n",
    "    def get_db_config(self) -> Dict[str, str]:\n",
    "        \"\"\"获取数据库配置\"\"\"\n",
    "        return self.DB_CONFIG.copy()\n",
    "    \n",
    "    def get_training_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取训练配置\"\"\"\n",
    "        return self.TRAINING_CONFIG.copy()\n",
    "    \n",
    "    def get_system_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取系统配置\"\"\"\n",
    "        return self.SYSTEM_CONFIG.copy()\n",
    "    \n",
    "    def update_config(self, config_name: str, updates: Dict[str, Any]) -> bool:\n",
    "        \"\"\"更新指定配置\"\"\"\n",
    "        try:\n",
    "            config = getattr(self, f'{config_name}_CONFIG')\n",
    "            config.update(updates)\n",
    "            return True\n",
    "        except AttributeError:\n",
    "            logging.error(f\"配置 {config_name} 不存在\")\n",
    "            return False\n",
    "    \n",
    "    def save_config(self, config_name: str) -> bool:\n",
    "        \"\"\"保存配置到文件\"\"\"\n",
    "        try:\n",
    "            config = getattr(self, f'{config_name}_CONFIG')\n",
    "            save_path = os.path.join(self.BASE_DIR, 'configs', f'{config_name.lower()}_config.json')\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            \n",
    "            with open(save_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(config, f, indent=4, ensure_ascii=False)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"保存配置失败: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def load_config(self, config_name: str) -> bool:\n",
    "        \"\"\"从文件加载配置\"\"\"\n",
    "        try:\n",
    "            load_path = os.path.join(self.BASE_DIR, 'configs', f'{config_name.lower()}_config.json')\n",
    "            if not os.path.exists(load_path):\n",
    "                return False\n",
    "            \n",
    "            with open(load_path, 'r', encoding='utf-8') as f:\n",
    "                config = json.load(f)\n",
    "            \n",
    "            setattr(self, f'{config_name}_CONFIG', config)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"加载配置失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "class ConfigValidator:\n",
    "    \"\"\"配置验证器\"\"\"\n",
    "    @staticmethod\n",
    "    def validate_db_config(config: Dict[str, str]) -> bool:\n",
    "        \"\"\"验证数据库配置\"\"\"\n",
    "        required_fields = ['host', 'user', 'password', 'database', 'charset']\n",
    "        return all(field in config for field in required_fields)\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_training_config(config: Dict[str, Any]) -> bool:\n",
    "        \"\"\"验证训练配置\"\"\"\n",
    "        try:\n",
    "            assert config['batch_size'] > 0\n",
    "            assert 0 < config['learning_rate'] < 1\n",
    "            assert config['epochs'] > 0\n",
    "            return True\n",
    "        except (AssertionError, KeyError):\n",
    "            return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_system_config(config: Dict[str, Any]) -> bool:\n",
    "        \"\"\"验证系统配置\"\"\"\n",
    "        try:\n",
    "            assert config['memory_limit'] > 0\n",
    "            assert config['gpu_memory_limit'] > 0\n",
    "            assert config['cleanup_interval'] > 0\n",
    "            assert config['log_retention_days'] > 0\n",
    "            return True\n",
    "        except (AssertionError, KeyError):\n",
    "            return False\n",
    "\n",
    "# 创建全局实例\n",
    "config_instance = ConfigManager()\n",
    "\n",
    "# 导出常用配置变量\n",
    "BASE_DIR = config_instance.BASE_DIR\n",
    "LOG_DIR = config_instance.LOG_DIR\n",
    "MODEL_DIR = config_instance.MODEL_DIR\n",
    "DATA_DIR = config_instance.DATA_DIR\n",
    "CHECKPOINT_DIR = config_instance.CHECKPOINT_DIR\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"配置验证：\")\n",
    "    print(f\"数据库配置: {config_instance.DB_CONFIG}\")\n",
    "    print(f\"训练配置: {config_instance.TRAINING_CONFIG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ae10f9-5218-4d2a-adc7-90da41c5bc98",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#2 日期处理工具\\date_utils.py\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "def parse_issue(issue_str):\n",
    "    \"\"\"解析期号字符串\"\"\"\n",
    "    match = re.match(r\"(\\d{8})-(\\d{4})\", issue_str)\n",
    "    if not match:\n",
    "        raise ValueError(\"无效的期号格式\")\n",
    "    return match.group(1), int(match.group(2))\n",
    "\n",
    "def get_next_issue(current_issue):\n",
    "    \"\"\"获取下一期号\"\"\"\n",
    "    date_str, period = parse_issue(current_issue)\n",
    "    date = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "    \n",
    "    if period == 1440:\n",
    "        new_date = date + timedelta(days=1)\n",
    "        new_period = 1\n",
    "    else:\n",
    "        new_date = date\n",
    "        new_period = period + 1\n",
    "    \n",
    "    return f\"{new_date.strftime('%Y%m%d')}-{new_period:04d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d72feb2-f04a-43d6-b4c1-71b9e037e11b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ruanjiananzhuang\\anaconda\\envs\\ace\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3 内存管理工具\\memory_utils.py\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import psutil\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "# 获取logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class MemoryManager:\n",
    "    \"\"\"内存管理器 - 负责内存监控、优化和清理\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                warning_threshold_mb: int = 8000,    # 8GB警告阈值\n",
    "                critical_threshold_mb: int = 10000,  # 10GB临界阈值\n",
    "                cleanup_interval: int = 300,         # 5分钟执行常规清理\n",
    "                full_cleanup_interval: int = 14400): # 4小时执行全面清理\n",
    "        \"\"\"\n",
    "        初始化内存管理器\n",
    "        Args:\n",
    "            warning_threshold_mb: 警告阈值(MB)\n",
    "            critical_threshold_mb: 临界阈值(MB)\n",
    "            cleanup_interval: 常规清理间隔(秒)\n",
    "            full_cleanup_interval: 全面清理间隔(秒)\n",
    "        \"\"\"\n",
    "        self.warning_threshold = warning_threshold_mb * 1024 * 1024  # 转换为字节\n",
    "        self.critical_threshold = critical_threshold_mb * 1024 * 1024\n",
    "        self.cleanup_interval = cleanup_interval\n",
    "        self.full_cleanup_interval = full_cleanup_interval\n",
    "        \n",
    "        self.last_cleanup_time = time.time()\n",
    "        self.last_full_cleanup_time = time.time()\n",
    "        \n",
    "        logger.info(f\"内存管理器初始化完成 - 警告阈值:{warning_threshold_mb}MB, 临界阈值:{critical_threshold_mb}MB\")\n",
    "\n",
    "    def check_memory_status(self) -> bool:\n",
    "        \"\"\"\n",
    "        检查内存状态并在必要时执行清理\n",
    "        Returns:\n",
    "            bool: 内存状态是否正常\n",
    "        \"\"\"\n",
    "        try:\n",
    "            current_usage = self.get_memory_usage()\n",
    "            current_time = time.time()\n",
    "\n",
    "            # 检查是否需要执行清理\n",
    "            if current_time - self.last_cleanup_time > self.cleanup_interval:\n",
    "                self._regular_cleanup()\n",
    "                self.last_cleanup_time = current_time\n",
    "\n",
    "            if current_time - self.last_full_cleanup_time > self.full_cleanup_interval:\n",
    "                self._full_cleanup()\n",
    "                self.last_full_cleanup_time = current_time\n",
    "\n",
    "            # 检查内存使用是否超过阈值\n",
    "            if current_usage > self.critical_threshold:\n",
    "                logger.warning(f\"内存使用超过临界值: {current_usage/1024/1024:.1f}MB\")\n",
    "                self._emergency_cleanup()\n",
    "                return False\n",
    "            elif current_usage > self.warning_threshold:\n",
    "                logger.warning(f\"内存使用超过警告值: {current_usage/1024/1024:.1f}MB\")\n",
    "                self._optimize_memory()\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"检查内存状态时出错: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def get_memory_usage(self) -> int:\n",
    "        \"\"\"\n",
    "        获取当前进程的内存使用量(字节)\n",
    "        Returns:\n",
    "            int: 内存使用量(字节)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            process = psutil.Process(os.getpid())\n",
    "            return process.memory_info().rss\n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取内存使用量时出错: {str(e)}\")\n",
    "            return 0\n",
    "\n",
    "    def get_memory_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        获取详细的内存使用信息\n",
    "        Returns:\n",
    "            Dict: 内存使用信息\n",
    "        \"\"\"\n",
    "        try:\n",
    "            memory = psutil.virtual_memory()\n",
    "            process = psutil.Process(os.getpid())\n",
    "            \n",
    "            return {\n",
    "                'total': memory.total,\n",
    "                'available': memory.available,\n",
    "                'used': memory.used,\n",
    "                'free': memory.free,\n",
    "                'percent': memory.percent,\n",
    "                'process_usage': process.memory_info().rss,\n",
    "                'process_percent': process.memory_percent()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取内存信息时出错: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "    def _regular_cleanup(self):\n",
    "        \"\"\"执行常规清理\"\"\"\n",
    "        try:\n",
    "            # 1. 清理Python垃圾\n",
    "            gc.collect()\n",
    "            \n",
    "            # 2. 清理TF会话\n",
    "            tf.keras.backend.clear_session()\n",
    "            \n",
    "            # 3. 清理不用的变量\n",
    "            for name in list(globals().keys()):\n",
    "                if name.startswith('_temp_'):\n",
    "                    del globals()[name]\n",
    "                    \n",
    "            logger.info(\"完成常规内存清理\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"常规清理时出错: {str(e)}\")\n",
    "\n",
    "    def _full_cleanup(self):\n",
    "        \"\"\"执行全面清理\"\"\"\n",
    "        try:\n",
    "            # 1. 执行常规清理\n",
    "            self._regular_cleanup()\n",
    "            \n",
    "            # 2. 重置TensorFlow状态\n",
    "            gpus = tf.config.list_physical_devices('GPU')\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.reset_memory_stats(gpu.name)\n",
    "            \n",
    "            # 3. 清理模型缓存\n",
    "            self._cleanup_model_cache()\n",
    "            \n",
    "            logger.info(\"完成全面内存清理\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"全面清理时出错: {str(e)}\")\n",
    "\n",
    "    def _emergency_cleanup(self):\n",
    "        \"\"\"执行紧急清理\"\"\"\n",
    "        try:\n",
    "            # 三级清理策略\n",
    "            gc.collect(2)  # 强制回收老年代内存\n",
    "            tf.keras.backend.clear_session()\n",
    "            # 释放多GPU内存\n",
    "            for gpu in tf.config.list_physical_devices('GPU'):\n",
    "                tf.config.experimental.reset_memory_stats(gpu.name)\n",
    "            \n",
    "            logger.warning(\"执行紧急内存清理\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"紧急清理时出错: {str(e)}\")\n",
    "\n",
    "    def _optimize_memory(self):\n",
    "        \"\"\"优化内存使用\"\"\"\n",
    "        try:\n",
    "            # 1. 检查并清理大对象\n",
    "            for obj in gc.get_objects():\n",
    "                if hasattr(obj, 'nbytes') and getattr(obj, 'nbytes', 0) > 1e8:  # >100MB\n",
    "                    del obj\n",
    "            \n",
    "            # 2. 执行垃圾回收\n",
    "            gc.collect()\n",
    "            \n",
    "            logger.info(\"完成内存优化\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"内存优化时出错: {str(e)}\")\n",
    "\n",
    "    def _cleanup_model_cache(self):\n",
    "        \"\"\"清理模型缓存\"\"\"\n",
    "        try:\n",
    "            # 清理Keras后端缓存\n",
    "            tf.keras.backend.clear_session()\n",
    "            \n",
    "            # 清理模型检查点文件\n",
    "            checkpoint_dir = os.path.join(os.getcwd(), 'checkpoints')\n",
    "            if os.path.exists(checkpoint_dir):\n",
    "                for item in os.listdir(checkpoint_dir):\n",
    "                    if item.endswith('.temp'):\n",
    "                        os.remove(os.path.join(checkpoint_dir, item))\n",
    "                        \n",
    "            # 释放TensorFlow占用的缓存\n",
    "            gpus = tf.config.list_physical_devices('GPU')\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.reset_memory_stats(gpu.name)\n",
    "            \n",
    "            logger.info(\"完成模型缓存清理\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"清理模型缓存时出错: {str(e)}\")\n",
    "\n",
    "    def optimize_for_large_data(self):\n",
    "        \"\"\"针对大样本的优化策略\"\"\"\n",
    "        # 新增大样本优化策略\n",
    "        self.enable_memmap = True  # 启用内存映射\n",
    "        self.chunk_size = 10000    # 分块加载\n",
    "        tf.keras.backend.set_floatx('float16')  # 压缩精度 \n",
    "\n",
    "    def optimize_for_hardware(self):\n",
    "        \"\"\"硬件定制优化\"\"\"\n",
    "        # 1. 限制TensorFlow内存使用\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            tf.config.set_logical_device_configuration(\n",
    "                gpus[0],\n",
    "                [tf.config.LogicalDeviceConfiguration(memory_limit=1536)]  # 限制1.5G显存\n",
    "            )\n",
    "        \n",
    "        # 2. 配置CPU并行线程\n",
    "        tf.config.threading.set_intra_op_parallelism_threads(6)  # 每个操作6线程\n",
    "        tf.config.threading.set_inter_op_parallelism_threads(4)   # 并行操作4线程\n",
    "        \n",
    "        # 3. 启用内存映射\n",
    "        self.enable_memmap = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37e16a1f-572f-46ff-8637-d18a09f4f84b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#4 日志系统初始化\\logging_manager.py\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import logging.handlers\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "from IPython.display import clear_output\n",
    "from pathlib import Path\n",
    "\n",
    "# 定义日志目录\n",
    "LOG_DIR = os.path.join('D:', 'JupyterWork', 'logs')\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "class DailyRotatingFileHandler(logging.FileHandler):\n",
    "    \"\"\"每日自动分文件的日志处理器\"\"\"\n",
    "    def __init__(self, base_dir, prefix='log', max_bytes=50*1024*1024):\n",
    "        self.base_dir = base_dir\n",
    "        self.prefix = prefix\n",
    "        self.max_bytes = max_bytes\n",
    "        self.current_date = None\n",
    "        self.current_file = None\n",
    "        self.current_size = 0\n",
    "        self.file_count = 1\n",
    "        \n",
    "        os.makedirs(base_dir, exist_ok=True)\n",
    "        self._init_file()\n",
    "        super().__init__(self.current_file, mode='a', encoding='utf-8')\n",
    "    \n",
    "    def _get_file_path(self):\n",
    "        \"\"\"获取当前日志文件路径\"\"\"\n",
    "        today = datetime.now().strftime('%Y%m%d')\n",
    "        if self.current_size >= self.max_bytes:\n",
    "            self.file_count += 1\n",
    "            return os.path.join(self.base_dir, f'{self.prefix}_{today}_{self.file_count}.log')\n",
    "        elif today != self.current_date:\n",
    "            self.file_count = 1\n",
    "            self.current_date = today\n",
    "            return os.path.join(self.base_dir, f'{self.prefix}_{today}.log')\n",
    "        return self.current_file\n",
    "    \n",
    "    def _init_file(self):\n",
    "        \"\"\"初始化日志文件\"\"\"\n",
    "        self.current_file = self._get_file_path()\n",
    "        self.current_date = datetime.now().strftime('%Y%m%d')\n",
    "        if os.path.exists(self.current_file):\n",
    "            self.current_size = os.path.getsize(self.current_file)\n",
    "        else:\n",
    "            self.current_size = 0\n",
    "    \n",
    "    def emit(self, record):\n",
    "        \"\"\"重写emit方法，在写入日志前检查文件状态\"\"\"\n",
    "        try:\n",
    "            new_file = self._get_file_path()\n",
    "            if new_file != self.current_file:\n",
    "                if self.stream:\n",
    "                    self.stream.close()\n",
    "                self.current_file = new_file\n",
    "                self.baseFilename = new_file\n",
    "                self.current_size = 0\n",
    "                self.stream = self._open()\n",
    "            \n",
    "            msg = self.format(record) + '\\n'\n",
    "            self.stream.write(msg)\n",
    "            self.stream.flush()\n",
    "            self.current_size += len(msg.encode('utf-8'))\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.handleError(record)\n",
    "\n",
    "class ProgressHandler(logging.Handler):\n",
    "    \"\"\"进度条处理器\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.progress = 0\n",
    "        \n",
    "    def emit(self, record):\n",
    "        if hasattr(record, 'progress'):\n",
    "            print('\\r' + ' ' * 80, end='\\r')\n",
    "            progress = int(record.progress * 50)\n",
    "            print(f'\\rTraining Progress: [{\"=\"*progress}{\" \"*(50-progress)}] {record.progress*100:.1f}%', end='')\n",
    "\n",
    "class LogDisplayManager:\n",
    "    \"\"\"日志显示管理器\"\"\"\n",
    "    def __init__(self, max_lines=10):\n",
    "        self.max_lines = max_lines\n",
    "        self.log_buffer = []\n",
    "        self.progress_bars = {i: 0.0 for i in range(6)}\n",
    "        self._clear_output()\n",
    "    \n",
    "    def _clear_output(self):\n",
    "        \"\"\"清空输出\"\"\"\n",
    "        clear_output(wait=True)\n",
    "    \n",
    "    def _display_logs(self):\n",
    "        \"\"\"显示日志\"\"\"\n",
    "        self._clear_output()\n",
    "        \n",
    "        start_idx = max(0, len(self.log_buffer) - self.max_lines)\n",
    "        for log in self.log_buffer[start_idx:]:\n",
    "            if log.strip():\n",
    "                print(log)\n",
    "        \n",
    "        print('-' * 80)\n",
    "        \n",
    "        for model_idx, progress in self.progress_bars.items():\n",
    "            bar_length = 50\n",
    "            filled = int(progress * bar_length)\n",
    "            bar = f\"Model {model_idx + 1}: [{'='*filled}{' '*(bar_length-filled)}] {progress*100:.1f}%\"\n",
    "            print(bar)\n",
    "\n",
    "class CustomFormatter(logging.Formatter):\n",
    "    \"\"\"自定义日志格式化器\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.formatters = {\n",
    "            logging.DEBUG: logging.Formatter(\n",
    "                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "            ),\n",
    "            logging.INFO: logging.Formatter(\n",
    "                '%(asctime)s - %(levelname)s - %(message)s'\n",
    "            ),\n",
    "            logging.WARNING: logging.Formatter(\n",
    "                '%(asctime)s - %(levelname)s - WARNING: %(message)s'\n",
    "            ),\n",
    "            logging.ERROR: logging.Formatter(\n",
    "                '%(asctime)s - %(levelname)s - ERROR: %(message)s\\n%(pathname)s:%(lineno)d'\n",
    "            ),\n",
    "            logging.CRITICAL: logging.Formatter(\n",
    "                '%(asctime)s - %(levelname)s - CRITICAL: %(message)s\\n%(pathname)s:%(lineno)d\\n%(exc_info)s'\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    def format(self, record):\n",
    "        formatter = self.formatters.get(record.levelno)\n",
    "        return formatter.format(record)\n",
    "\n",
    "class LoggingManager:\n",
    "    \"\"\"日志管理器 - 单例模式\"\"\"\n",
    "    _instance = None\n",
    "    \n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "            cls._instance._configure_logging()\n",
    "        return cls._instance\n",
    "    \n",
    "    def _configure_logging(self):\n",
    "        \"\"\"配置日志系统\"\"\"\n",
    "        # 使用之前定义的LOG_DIR\n",
    "        BASE_DIR = Path(LOG_DIR)\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s [%(levelname)s] %(name)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(BASE_DIR / \"system.log\"),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        logging.captureWarnings(True)\n",
    "        self.logger = logging.getLogger('ACE_System')\n",
    "\n",
    "class LogManager:\n",
    "    \"\"\"日志管理器\"\"\"\n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger('ContinuousTraining')\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        self.log_buffer = deque(maxlen=100)\n",
    "        self.display_manager = LogDisplayManager()\n",
    "        self._setup_handlers()\n",
    "    \n",
    "    def _setup_handlers(self):\n",
    "        \"\"\"设置日志处理器\"\"\"\n",
    "        file_handler = DailyRotatingFileHandler(\n",
    "            base_dir=LOG_DIR,\n",
    "            prefix='continuous_training'\n",
    "        )\n",
    "        \n",
    "        formatter = logging.Formatter(\n",
    "            '[%(asctime)s] %(levelname)s: %(message)s',\n",
    "            datefmt='%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "        file_handler.setFormatter(formatter)\n",
    "        \n",
    "        progress_handler = ProgressHandler()\n",
    "        progress_handler.setFormatter(formatter)\n",
    "        \n",
    "        self.logger.addHandler(file_handler)\n",
    "        self.logger.addHandler(progress_handler)\n",
    "    \n",
    "    def get_logger(self):\n",
    "        \"\"\"获取logger实例\"\"\"\n",
    "        return self.logger\n",
    "    \n",
    "    def update_progress(self, model_idx: int, progress: float):\n",
    "        \"\"\"更新进度条\"\"\"\n",
    "        self.display_manager.progress_bars[model_idx] = progress\n",
    "        self.display_manager._display_logs()\n",
    "    \n",
    "    def add_log(self, message: str):\n",
    "        \"\"\"添加日志到缓冲区\"\"\"\n",
    "        self.log_buffer.append(message)\n",
    "        self.display_manager.log_buffer.append(message)\n",
    "        self.display_manager._display_logs()\n",
    "\n",
    "# 创建全局实例\n",
    "logger = LoggingManager().logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53728f26-2338-4107-8eb4-fee3fb003f31",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 20:34:36,254 [INFO] __main__ - 数据库连接池创建成功\n",
      "2025-02-14 20:34:36,255 [INFO] __main__ - 数据库管理器初始化完成\n"
     ]
    }
   ],
   "source": [
    "#5 数据库管理器\\database_manager.py\n",
    "import pymysql\n",
    "import logging\n",
    "import threading\n",
    "from pymysql.cursors import DictCursor\n",
    "from sqlalchemy.pool import QueuePool  # 使用SQLAlchemy自带的连接池\n",
    "from datetime import datetime, timedelta\n",
    "from cell1 import ConfigManager  # 修改导入方式\n",
    "\n",
    "# 获取logger实例\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DatabaseManager:\n",
    "    \"\"\"数据库管理器 - 单例模式\"\"\"\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "    \n",
    "    def __new__(cls, db_config=None):\n",
    "        if cls._instance is None:\n",
    "            with cls._lock:\n",
    "                if cls._instance is None:\n",
    "                    cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self, db_config=None):\n",
    "        if not hasattr(self, 'initialized'):\n",
    "            # 使用ConfigManager的配置\n",
    "            config_instance = ConfigManager()\n",
    "            self.DB_CONFIG = config_instance.get_db_config()\n",
    "            \n",
    "            # 添加必要的数据库配置\n",
    "            self.DB_CONFIG.update({\n",
    "                'database': 'admin_data',\n",
    "                'charset': 'utf8mb4'\n",
    "            })\n",
    "            \n",
    "            # 创建数据库连接池\n",
    "            self.pool = self._create_pool()\n",
    "            \n",
    "            # 初始化查询缓存\n",
    "            self.query_cache = {}\n",
    "            self.cache_timeout = 300  # 5分钟缓存超时\n",
    "            \n",
    "            # 标记初始化完成\n",
    "            self.initialized = True\n",
    "            logger.info(\"数据库管理器初始化完成\")\n",
    "    \n",
    "    def _create_pool(self):\n",
    "        \"\"\"创建数据库连接池\"\"\"\n",
    "        try:\n",
    "            self.pool = QueuePool(\n",
    "                creator=lambda: pymysql.connect(**self.DB_CONFIG),\n",
    "                pool_size=10,\n",
    "                max_overflow=20,\n",
    "                timeout=30\n",
    "            )\n",
    "            logger.info(\"数据库连接池创建成功\")\n",
    "            return self.pool\n",
    "        except Exception as e:\n",
    "            logger.error(f\"创建数据库连接池失败: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def execute_query(self, query, params=None, use_cache=False):\n",
    "        \"\"\"执行查询\"\"\"\n",
    "        try:\n",
    "            # 检查缓存\n",
    "            if use_cache:\n",
    "                cache_key = f\"{query}_{str(params)}\"\n",
    "                cached_result = self._get_from_cache(cache_key)\n",
    "                if cached_result is not None:\n",
    "                    return cached_result\n",
    "            \n",
    "            # 获取连接和游标\n",
    "            connection = self.pool.connect()\n",
    "            try:\n",
    "                cursor = connection.cursor(DictCursor)\n",
    "                # 执行查询\n",
    "                cursor.execute(query, params)\n",
    "                result = cursor.fetchall()\n",
    "                \n",
    "                # 更新缓存\n",
    "                if use_cache:\n",
    "                    self._update_cache(cache_key, result)\n",
    "                \n",
    "                return result\n",
    "            finally:\n",
    "                cursor.close()\n",
    "                connection.close()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"执行查询失败: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def execute_batch(self, query, params_list):\n",
    "        \"\"\"批量执行查询\"\"\"\n",
    "        try:\n",
    "            connection = self.pool.connect()\n",
    "            cursor = connection.cursor()\n",
    "            \n",
    "            try:\n",
    "                cursor.executemany(query, params_list)\n",
    "                connection.commit()\n",
    "                return True\n",
    "            finally:\n",
    "                cursor.close()\n",
    "                connection.close()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"批量执行查询失败: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def _get_from_cache(self, key):\n",
    "        \"\"\"从缓存获取数据\"\"\"\n",
    "        if key in self.query_cache:\n",
    "            timestamp, data = self.query_cache[key]\n",
    "            if datetime.now() - timestamp < timedelta(seconds=self.cache_timeout):\n",
    "                return data\n",
    "            else:\n",
    "                del self.query_cache[key]\n",
    "        return None\n",
    "    \n",
    "    def _update_cache(self, key, data):\n",
    "        \"\"\"更新缓存\"\"\"\n",
    "        self.query_cache[key] = (datetime.now(), data)\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"清理缓存\"\"\"\n",
    "        self.query_cache.clear()\n",
    "        logger.info(\"查询缓存已清理\")\n",
    "\n",
    "    def get_records_by_issue(self, start_issue, limit):\n",
    "        \"\"\"按期号范围获取记录\"\"\"\n",
    "        query = f\"\"\"\n",
    "            SELECT * FROM admin_tab \n",
    "            WHERE date_period >= %s\n",
    "            ORDER BY date_period ASC\n",
    "            LIMIT %s\n",
    "        \"\"\"\n",
    "        return self.execute_query(query, (start_issue, limit))\n",
    "\n",
    "    def close_all(self):\n",
    "        \"\"\"关闭所有数据库连接\"\"\"\n",
    "        if self.pool:\n",
    "            self.pool.dispose()\n",
    "            logger.info(\"已关闭所有数据库连接\")\n",
    "\n",
    "# 创建全局实例\n",
    "db_manager = DatabaseManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0d6d2e5-c654-4129-8195-0e768c80f0e2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 20:34:36,751 [INFO] cell5 - 数据库连接池创建成功\n",
      "2025-02-14 20:34:36,752 [INFO] cell5 - 数据库管理器初始化完成\n",
      "2025-02-14 20:34:36,756 [INFO] __main__ - 数据管理器初始化完成\n"
     ]
    }
   ],
   "source": [
    "#6 数据管道构建\\data_manager.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import pymysql\n",
    "from datetime import datetime, timedelta\n",
    "import threading\n",
    "from collections import deque\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from cell5 import db_manager  # 修改导入方式\n",
    "from cell1 import config_instance  # 修改导入方式\n",
    "\n",
    "# 获取logger实例\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataManager:\n",
    "    \"\"\"数据管理器 - 单例模式\"\"\"\n",
    "    _instance = None\n",
    "    \n",
    "    def __new__(cls, db_config=None):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self, db_config=None):\n",
    "        if not hasattr(self, 'initialized'):\n",
    "            # 初始化组件\n",
    "            self.data_pool = DataPool()\n",
    "            self.data_processor = DataProcessor()\n",
    "            self.data_validator = DataValidator()\n",
    "            \n",
    "            # 数据缓存参数\n",
    "            self.cache_size = config_instance.SYSTEM_CONFIG['DATA_CONFIG']['cache_size']\n",
    "            self.batch_size = 32\n",
    "            self.sequence_length = 14400\n",
    "            \n",
    "            # 线程锁\n",
    "            self.lock = threading.Lock()\n",
    "            \n",
    "            # 新增配置项\n",
    "            self.comparison_dir = os.path.join(config_instance.BASE_DIR, 'comparison')\n",
    "            os.makedirs(self.comparison_dir, exist_ok=True)\n",
    "            self.issue_file = os.path.join(self.comparison_dir, 'issue_number.txt')\n",
    "            \n",
    "            # 标记初始化完成\n",
    "            self.initialized = True\n",
    "            logger.info(\"数据管理器初始化完成\")\n",
    "            \n",
    "            # 在DataManager中增加锁\n",
    "            self.issue_lock = threading.Lock()\n",
    "            \n",
    "            # 获取配置参数\n",
    "            self.normalize_range = config_instance.SYSTEM_CONFIG['DATA_CONFIG']['normalize_range']\n",
    "            \n",
    "            self._init_data_loader()\n",
    "            \n",
    "    def _init_data_loader(self):\n",
    "        \"\"\"初始化数据加载器\"\"\"\n",
    "        self.data_cache = {}\n",
    "        self.batch_size = 32\n",
    "    \n",
    "    def load_training_data(self):\n",
    "        \"\"\"加载训练数据\"\"\"\n",
    "        # 示例数据加载逻辑\n",
    "        return []\n",
    "    \n",
    "    def get_training_batch(self, batch_size=None):\n",
    "        \"\"\"获取训练批次\"\"\"\n",
    "        try:\n",
    "            batch_size = batch_size or self.batch_size\n",
    "            with self.lock:\n",
    "                return self.data_processor.get_training_batch(\n",
    "                    self.data_pool.get_latest_data(),\n",
    "                    batch_size\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取训练批次时出错: {str(e)}\")\n",
    "            return None, None\n",
    "    \n",
    "    def update_data(self):\n",
    "        \"\"\"更新数据\"\"\"\n",
    "        try:\n",
    "            new_data = self._fetch_new_data()\n",
    "            if new_data is not None:\n",
    "                with self.lock:\n",
    "                    self.data_pool.update_data(new_data)\n",
    "                logger.info(f\"数据更新成功，当前数据量: {len(self.data_pool.data)}\")\n",
    "                return True\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"更新数据时出错: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def _get_next_issue(self, current_issue):\n",
    "        \"\"\"计算下一期号\"\"\"\n",
    "        date_str, period = current_issue.split('-')\n",
    "        date = datetime.strptime(date_str, '%Y%m%d')\n",
    "        period = int(period)\n",
    "        \n",
    "        if period == 1440:\n",
    "            next_date = date + timedelta(days=1)\n",
    "            next_period = 1\n",
    "        else:\n",
    "            next_date = date\n",
    "            next_period = period + 1\n",
    "            \n",
    "        return f\"{next_date.strftime('%Y%m%d')}-{next_period:04d}\"\n",
    "\n",
    "    def _fetch_new_data(self):\n",
    "        \"\"\"根据144000+2880的数据需求获取样本\"\"\"\n",
    "        try:\n",
    "            with self.issue_lock:\n",
    "                with open(self.issue_file, 'r+') as f:\n",
    "                    last_issue = f.read().strip()\n",
    "                    \n",
    "                    # 使用配置获取总数\n",
    "                    total = config_instance.SYSTEM_CONFIG['SAMPLE_CONFIG']['total_fetch']()\n",
    "                    \n",
    "                    # 构建精确查询\n",
    "                    query = f\"\"\"\n",
    "                        SELECT date_period, number \n",
    "                        FROM admin_tab \n",
    "                        WHERE date_period {'>' if last_issue else ''}= '{last_issue}'\n",
    "                        ORDER BY date_period \n",
    "                        LIMIT {total}\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    records = db_manager.execute_query(query)\n",
    "                    \n",
    "                    # 验证数据连续性\n",
    "                    if not self._validate_sequence(records):\n",
    "                        raise ValueError(\"数据存在断层\")\n",
    "                    \n",
    "                    # 更新期号文件\n",
    "                    new_last = records[-1]['date_period']\n",
    "                    f.seek(0)\n",
    "                    f.write(new_last)\n",
    "                    \n",
    "                    return self._process_numbers(records)  # 处理五位号码\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"数据获取失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _process_numbers(self, records):\n",
    "        \"\"\"处理五位数字号码\"\"\"\n",
    "        processed = []\n",
    "        for r in records:\n",
    "            # 将\"00236\"转换为[0,0,2,3,6]\n",
    "            numbers = [int(d) for d in r['number'].zfill(5)]\n",
    "            processed.append({\n",
    "                'date_period': r['date_period'],\n",
    "                'numbers': numbers,\n",
    "                'time_features': self.time_feature_extractor.extract_features(r['date_period'])\n",
    "            })\n",
    "        return processed\n",
    "\n",
    "    def validate_data(self, data):\n",
    "        \"\"\"验证数据有效性\"\"\"\n",
    "        return self.data_validator.validate(data)\n",
    "    \n",
    "    def get_data_stats(self):\n",
    "        \"\"\"获取数据统计信息\"\"\"\n",
    "        try:\n",
    "            with self.lock:\n",
    "                return {\n",
    "                    'total_samples': len(self.data_pool.data),\n",
    "                    'cache_size': self.data_pool.get_cache_size(),\n",
    "                    'last_update': self.data_pool.last_update_time\n",
    "                }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取数据统计信息时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def get_data_by_period(self, start_period, end_period):\n",
    "        \"\"\"获取指定期间的数据\"\"\"\n",
    "        try:\n",
    "            query = \"\"\"\n",
    "                SELECT date_period, number \n",
    "                FROM admin_tab \n",
    "                WHERE date_period BETWEEN %s AND %s\n",
    "                ORDER BY date_period\n",
    "            \"\"\"\n",
    "            records = db_manager.execute_query(\n",
    "                query, \n",
    "                params=(start_period, end_period),\n",
    "                use_cache=True\n",
    "            )\n",
    "            \n",
    "            if not records:\n",
    "                return None\n",
    "                \n",
    "            return self.data_processor.process_records(records)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取期间数据时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def clear_data_cache(self):\n",
    "        \"\"\"清理数据缓存\"\"\"\n",
    "        try:\n",
    "            with self.lock:\n",
    "                self.data_pool.clear_cache()\n",
    "                db_manager.clear_cache()\n",
    "            logger.info(\"数据缓存已清理\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"清理数据缓存时出错: {str(e)}\")\n",
    "\n",
    "    def get_training_sample(self):\n",
    "        \"\"\"获取完整训练样本\"\"\"\n",
    "        # 修改为返回单一样本\n",
    "        sample = self.data_pool.get_data('latest')\n",
    "        if self._validate_sample(sample):\n",
    "            return sample\n",
    "        return None\n",
    "\n",
    "    def _validate_sample(self, sample):\n",
    "        \"\"\"验证样本完整性\"\"\"\n",
    "        cfg = config_instance.SYSTEM_CONFIG['SAMPLE_CONFIG']\n",
    "        return (\n",
    "            len(sample['input']) == cfg['input_length'] and \n",
    "            len(sample['target']) == cfg['target_length']\n",
    "        )\n",
    "\n",
    "class DataPool:\n",
    "    \"\"\"数据池 - 负责数据缓存和管理\"\"\"\n",
    "    def __init__(self, max_size=10000):\n",
    "        from collections import OrderedDict\n",
    "        self.data = []\n",
    "        self.cache = OrderedDict()\n",
    "        self.max_size = max_size\n",
    "        self.lock = threading.Lock()\n",
    "        self.last_update_time = None\n",
    "        \n",
    "        # 初始化数据缩放器\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.is_scaler_fitted = False\n",
    "    \n",
    "    def update_data(self, new_data):\n",
    "        \"\"\"更新数据\"\"\"\n",
    "        with self.lock:\n",
    "            self.data.extend(new_data)\n",
    "            self.last_update_time = datetime.now()\n",
    "            \n",
    "            # 如果还没有拟合scaler，进行拟合\n",
    "            if not self.is_scaler_fitted and len(self.data) > 0:\n",
    "                self.scaler.fit(np.array(self.data))\n",
    "                self.is_scaler_fitted = True\n",
    "    \n",
    "    def get_latest_data(self, n=1000):\n",
    "        \"\"\"获取最新的n条数据\"\"\"\n",
    "        with self.lock:\n",
    "            return self.data[-n:] if self.data else []\n",
    "    \n",
    "    def get_cache_size(self):\n",
    "        \"\"\"获取缓存大小\"\"\"\n",
    "        return len(self.cache)\n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"清理数据缓存\"\"\"\n",
    "        with self.lock:\n",
    "            self.cache.clear()\n",
    "\n",
    "    def add_batch(self, batch):\n",
    "        # 添加对齐检查\n",
    "        aligned = self._align_sequences(batch)\n",
    "        self.data.extend(aligned)\n",
    "        \n",
    "    def _align_sequences(self, batch):\n",
    "        max_len = max(len(item['input']) for item in batch)\n",
    "        for item in batch:\n",
    "            item['input'] = np.pad(item['input'], (0, max_len - len(item['input'])))\n",
    "        return batch\n",
    "\n",
    "    def add_data(self, key, data):\n",
    "        if key in self.cache:\n",
    "            self.cache.move_to_end(key)\n",
    "        else:\n",
    "            self.cache[key] = data\n",
    "            if len(self.cache) > self.max_size:\n",
    "                self.cache.popitem(last=False)  # LRU淘汰\n",
    "                \n",
    "    def get_data(self, version='latest'):\n",
    "        if version == 'latest':\n",
    "            return self.cache[next(reversed(self.cache))]\n",
    "        else:\n",
    "            return self.cache.get(version, None)\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"数据处理器 - 负责数据预处理和批次生成\"\"\"\n",
    "    def __init__(self):\n",
    "        self.scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        self.lock = threading.Lock()\n",
    "        # 添加时间特征提取器\n",
    "        self.time_feature_extractor = TimeFeatureExtractor()\n",
    "    \n",
    "    def process_records(self, records):\n",
    "        cleaned = self._remove_nan(records)\n",
    "        normalized = self._normalize(cleaned)\n",
    "        windowed = self._window_sampling(normalized)\n",
    "        return windowed\n",
    "\n",
    "    def _remove_nan(self, data):\n",
    "        return [d for d in data if not np.isnan(d['numbers']).any()]\n",
    "        \n",
    "    def _normalize(self, data):\n",
    "        numbers = np.array([d['numbers'] for d in data])\n",
    "        self.scaler.fit(numbers)\n",
    "        return self.scaler.transform(numbers)\n",
    "        \n",
    "    def _window_sampling(self, data):\n",
    "        cfg = config_instance.SYSTEM_CONFIG['SAMPLE_CONFIG']\n",
    "        return {\n",
    "            'input': data[:cfg['input_length']],\n",
    "            'target': data[cfg['input_length'] : cfg['input_length']+cfg['target_length']]\n",
    "        }\n",
    "\n",
    "    def get_training_batch(self, data, batch_size):\n",
    "        \"\"\"生成训练批次\"\"\"\n",
    "        try:\n",
    "            if not data or len(data) < batch_size:\n",
    "                return None, None\n",
    "            \n",
    "            # 随机选择批次\n",
    "            indices = np.random.choice(len(data), batch_size)\n",
    "            batch_data = np.array([data[i] for i in indices])\n",
    "            \n",
    "            # 分割输入和目标\n",
    "            X = batch_data[:, :-1]  # 所有特征除了最后一个\n",
    "            y = batch_data[:, -1]   # 最后一个特征作为目标\n",
    "            \n",
    "            return X, y\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"生成训练批次时出错: {str(e)}\")\n",
    "            return None, None\n",
    "\n",
    "class DataValidator:\n",
    "    \"\"\"数据验证器 - 负责数据有效性检查\"\"\"\n",
    "    def __init__(self):\n",
    "        self.lock = threading.Lock()\n",
    "    \n",
    "    def validate(self, data):\n",
    "        \"\"\"验证数据有效性\"\"\"\n",
    "        return all([\n",
    "            self._check_nan(data),\n",
    "            self._check_range(data),\n",
    "            self._check_sequence(data)\n",
    "        ])\n",
    "        \n",
    "    def _check_range(self, data):\n",
    "        return np.all((data >= 1) & (data <= 33))\n",
    "        \n",
    "    def _check_sequence(self, data):\n",
    "        diffs = np.diff([d['date_period'] for d in data])\n",
    "        return np.all(diffs == 600)  # 假设期号间隔10分钟\n",
    "\n",
    "    def _check_number(self, number_array):\n",
    "        \"\"\"验证五位数字是否合法\"\"\"\n",
    "        return np.all((number_array >= 0) & (number_array <= 9))\n",
    "        \n",
    "    def _check_sequence_gap(self, diffs):\n",
    "        \"\"\"验证期号间隔是否为1期(10分钟)\"\"\"\n",
    "        return np.all(diffs == 1)  # 假设期号连续递增1\n",
    "\n",
    "class TimeFeatureExtractor:\n",
    "    \"\"\"时间特征提取器\"\"\"\n",
    "    def __init__(self):\n",
    "        self.use_weekday = True\n",
    "        self.use_month = True\n",
    "        self.use_day = True\n",
    "        self.use_period = True\n",
    "    \n",
    "    def extract_features(self, date_period):\n",
    "        \"\"\"提取时间特征\n",
    "        Args:\n",
    "            date_period: 期号字符串 (格式: YYYYMMDD-XXXX)\n",
    "        Returns:\n",
    "            features: 时间特征列表\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 解析日期和期号\n",
    "            date_str, period = date_period.split('-')\n",
    "            date = datetime.strptime(date_str, '%Y%m%d')\n",
    "            period_num = int(period)\n",
    "            \n",
    "            features = []\n",
    "            \n",
    "            # 添加月份特征\n",
    "            if self.use_month:\n",
    "                features.append(date.month / 12)  # 归一化到0-1\n",
    "            \n",
    "            # 添加日期特征    \n",
    "            if self.use_day:\n",
    "                features.append(date.day / 31)  # 归一化到0-1\n",
    "            \n",
    "            # 添加星期特征\n",
    "            if self.use_weekday:\n",
    "                features.append(date.weekday() / 6)  # 归一化到0-1\n",
    "            \n",
    "            # 添加期号特征\n",
    "            if self.use_period:\n",
    "                features.append((period_num - 1) / 1440)  # 归一化到0-1\n",
    "            \n",
    "            return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"提取时间特征时出错: {str(e)}\")\n",
    "            return [0] * 4  # 返回全零特征\n",
    "\n",
    "# 创建全局实例\n",
    "data_manager = DataManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdcebe67-d603-4acf-99c7-d2d56aa35482",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 20:34:36,772 [INFO] __main__ - 资源监控器初始化完成\n"
     ]
    }
   ],
   "source": [
    "#7 资源监控器\\resource_monitor.py\n",
    "import psutil\n",
    "import logging\n",
    "import threading\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "\n",
    "# 获取logger实例\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ResourceMonitor:\n",
    "    \"\"\"资源监控器 - 负责监控CPU、内存、GPU等资源使用情况\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=100, check_interval=5):\n",
    "        self.window_size = window_size\n",
    "        self.check_interval = check_interval\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "        # 监控指标存储\n",
    "        self.metrics = {\n",
    "            'cpu_usage': deque(maxlen=window_size),\n",
    "            'memory_usage': deque(maxlen=window_size),\n",
    "            'disk_usage': deque(maxlen=window_size),\n",
    "            'gpu_usage': None,\n",
    "            'gpu_memory': None\n",
    "        }\n",
    "        \n",
    "        # 警报阈值\n",
    "        self.thresholds = {\n",
    "            'cpu_usage': 90,    # CPU使用率超过90%\n",
    "            'memory_usage': 90,  # 内存使用率超过90%\n",
    "            'disk_usage': 90,    # 磁盘使用率超过90%\n",
    "            'gpu_usage': 90,     # GPU使用率超过90%\n",
    "            'gpu_memory': 90     # GPU内存使用率超过90%\n",
    "        }\n",
    "        \n",
    "        # 监控线程\n",
    "        self._running = False\n",
    "        self._thread = threading.Thread(target=self._monitor_loop)\n",
    "        \n",
    "        # 警报历史\n",
    "        self.alerts = []\n",
    "        \n",
    "        self.memory_usage = 0.0  # 确保有此属性初始化\n",
    "        self.cpu_usage = 0.0\n",
    "        self.gpu_usage = 0.0\n",
    "        \n",
    "        logger.info(\"资源监控器初始化完成\")\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"启动资源监控\"\"\"\n",
    "        if self._thread.is_alive():\n",
    "            logger.warning(\"资源监控器已在运行\")\n",
    "            return\n",
    "            \n",
    "        self._running = True\n",
    "        self._thread.start()\n",
    "        logger.info(\"资源监控器已启动\")\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"停止资源监控\"\"\"\n",
    "        self._running = False\n",
    "        if self._thread:\n",
    "            self._thread.join()\n",
    "        logger.info(\"资源监控器已停止\")\n",
    "    \n",
    "    def _monitor_loop(self):\n",
    "        \"\"\"监控循环\"\"\"\n",
    "        while self._running:\n",
    "            try:\n",
    "                self._collect_metrics()\n",
    "                self._check_alerts()\n",
    "                time.sleep(self.check_interval)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"资源监控循环出错: {str(e)}\")\n",
    "    \n",
    "    def _collect_metrics(self):\n",
    "        \"\"\"收集资源指标\"\"\"\n",
    "        with self.lock:\n",
    "            # CPU使用率\n",
    "            self.cpu_usage = psutil.cpu_percent()\n",
    "            self.metrics['cpu_usage'].append(self.cpu_usage)\n",
    "            \n",
    "            # 内存使用率\n",
    "            mem = psutil.virtual_memory()\n",
    "            self.memory_usage = mem.percent  # 确保有此属性更新\n",
    "            self.metrics['memory_usage'].append(self.memory_usage)\n",
    "            \n",
    "            # 磁盘使用率\n",
    "            disk = psutil.disk_usage('/')\n",
    "            self.metrics['disk_usage'].append(disk.percent)\n",
    "    \n",
    "    def _check_alerts(self):\n",
    "        \"\"\"检查警报\"\"\"\n",
    "        with self.lock:\n",
    "            for metric, values in self.metrics.items():\n",
    "                if not values:\n",
    "                    continue\n",
    "                current = values[-1]\n",
    "                if current > self.thresholds[metric]:\n",
    "                    alert = {\n",
    "                        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        'metric': metric,\n",
    "                        'value': current,\n",
    "                        'threshold': self.thresholds[metric]\n",
    "                    }\n",
    "                    self.alerts.append(alert)\n",
    "                    logger.warning(f\"资源警报: {metric} = {current}% (阈值: {self.thresholds[metric]}%)\")\n",
    "\n",
    "# 创建全局实例\n",
    "resource_monitor = ResourceMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "477ce1a2-e933-4a3b-870c-be6a80c1de7c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#8 性能监控器\\performance_monitor.py\n",
    "import logging\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from collections import deque\n",
    "import threading\n",
    "\n",
    "# 获取logger实例\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"性能监控器 - 负责收集和分析性能指标\"\"\"\n",
    "    def __init__(self, save_dir: str, window_size=100):\n",
    "        self.save_dir = save_dir\n",
    "        self.window_size = window_size\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "        # 性能指标存储\n",
    "        self.metrics = {\n",
    "            'loss': deque(maxlen=window_size),\n",
    "            'accuracy': deque(maxlen=window_size),\n",
    "            'training_time': deque(maxlen=window_size),\n",
    "            'prediction_time': deque(maxlen=window_size),\n",
    "            'memory_usage': deque(maxlen=window_size),\n",
    "            'gpu_usage': deque(maxlen=window_size),  # 新增GPU使用率指标\n",
    "            'batch_time': deque(maxlen=window_size)  # 新增批次处理时间指标\n",
    "        }\n",
    "        \n",
    "        # 警报阈值\n",
    "        self.thresholds = {\n",
    "            'loss_increase': 0.2,        # 损失增加超过20%\n",
    "            'accuracy_drop': 0.1,        # 准确率下降超过10%\n",
    "            'training_time_increase': 0.5,  # 训练时间增加超过50%\n",
    "            'memory_usage': 0.9,         # 内存使用率超过90%\n",
    "            'gpu_usage': 0.95,           # GPU使用率超过95%\n",
    "            'batch_time_increase': 0.3   # 批次时间增加超过30%\n",
    "        }\n",
    "        \n",
    "        # 性能趋势分析\n",
    "        self.trend_window = 10  # 趋势分析窗口\n",
    "        self.trend_threshold = 0.05  # 趋势判定阈值\n",
    "        \n",
    "        # 初始化性能日志文件\n",
    "        self._init_log_file()\n",
    "        \n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        \n",
    "        self._running = False\n",
    "        self._thread = threading.Thread(target=self._monitor_loop)\n",
    "        \n",
    "    def _init_log_file(self):\n",
    "        \"\"\"初始化性能日志文件\"\"\"\n",
    "        try:\n",
    "            self.log_file = os.path.join(\n",
    "                self.save_dir, \n",
    "                f'performance_{datetime.now().strftime(\"%Y%m%d\")}.json'\n",
    "            )\n",
    "            os.makedirs(self.save_dir, exist_ok=True)\n",
    "            \n",
    "            if not os.path.exists(self.log_file):\n",
    "                with open(self.log_file, 'w') as f:\n",
    "                    json.dump([], f)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"初始化性能日志文件失败: {str(e)}\")\n",
    "    \n",
    "    def update_metrics(self, metrics_dict):\n",
    "        \"\"\"更新性能指标\"\"\"\n",
    "        try:\n",
    "            with self.lock:\n",
    "                # 更新指标\n",
    "                for metric_name, value in metrics_dict.items():\n",
    "                    if metric_name in self.metrics:\n",
    "                        self.metrics[metric_name].append(value)\n",
    "                \n",
    "                # 记录到日志文件\n",
    "                self._log_metrics(metrics_dict)\n",
    "                \n",
    "                # 检查警报\n",
    "                self._check_alerts()\n",
    "                \n",
    "                # 分析趋势\n",
    "                self._analyze_trends()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"更新性能指标失败: {str(e)}\")\n",
    "    \n",
    "    def _log_metrics(self, metrics_dict):\n",
    "        \"\"\"记录性能指标到日志文件\"\"\"\n",
    "        try:\n",
    "            log_entry = {\n",
    "                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'metrics': metrics_dict\n",
    "            }\n",
    "            \n",
    "            # 读取现有日志\n",
    "            with open(self.log_file, 'r') as f:\n",
    "                logs = json.load(f)\n",
    "                \n",
    "            # 添加新记录\n",
    "            logs.append(log_entry)\n",
    "            \n",
    "            # 写回文件\n",
    "            with open(self.log_file, 'w') as f:\n",
    "                json.dump(logs, f, indent=4)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"记录性能指标失败: {str(e)}\")\n",
    "    \n",
    "    def _check_alerts(self):\n",
    "        \"\"\"检查性能警报\"\"\"\n",
    "        try:\n",
    "            with self.lock:\n",
    "                # 检查损失值增加\n",
    "                if len(self.metrics['loss']) >= 2:\n",
    "                    loss_increase = (self.metrics['loss'][-1] - self.metrics['loss'][-2]) / self.metrics['loss'][-2]\n",
    "                    if loss_increase > self.thresholds['loss_increase']:\n",
    "                        logger.warning(f\"损失值显著增加: {loss_increase:.2%}\")\n",
    "                \n",
    "                # 检查准确率下降\n",
    "                if len(self.metrics['accuracy']) >= 2:\n",
    "                    acc_drop = (self.metrics['accuracy'][-2] - self.metrics['accuracy'][-1]) / self.metrics['accuracy'][-2]\n",
    "                    if acc_drop > self.thresholds['accuracy_drop']:\n",
    "                        logger.warning(f\"准确率显著下降: {acc_drop:.2%}\")\n",
    "                \n",
    "                # 检查训练时间增加\n",
    "                if len(self.metrics['training_time']) >= 2:\n",
    "                    time_increase = (self.metrics['training_time'][-1] - self.metrics['training_time'][-2]) / self.metrics['training_time'][-2]\n",
    "                    if time_increase > self.thresholds['training_time_increase']:\n",
    "                        logger.warning(f\"训练时间显著增加: {time_increase:.2%}\")\n",
    "                \n",
    "                # 检查内存使用\n",
    "                if self.metrics['memory_usage'] and self.metrics['memory_usage'][-1] > self.thresholds['memory_usage']:\n",
    "                    logger.warning(f\"内存使用率过高: {self.metrics['memory_usage'][-1]:.1%}\")\n",
    "                \n",
    "                # 检查GPU使用\n",
    "                if self.metrics['gpu_usage'] and self.metrics['gpu_usage'][-1] > self.thresholds['gpu_usage']:\n",
    "                    logger.warning(f\"GPU使用率过高: {self.metrics['gpu_usage'][-1]:.1%}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"检查性能警报失败: {str(e)}\")\n",
    "    \n",
    "    def _analyze_trends(self):\n",
    "        \"\"\"分析性能趋势\"\"\"\n",
    "        try:\n",
    "            with self.lock:\n",
    "                for metric_name, values in self.metrics.items():\n",
    "                    if len(values) >= self.trend_window:\n",
    "                        # 计算趋势斜率\n",
    "                        x = np.arange(self.trend_window)\n",
    "                        y = np.array(list(values)[-self.trend_window:])\n",
    "                        slope = np.polyfit(x, y, 1)[0]\n",
    "                        \n",
    "                        # 判断趋势\n",
    "                        if abs(slope) > self.trend_threshold:\n",
    "                            trend = \"上升\" if slope > 0 else \"下降\"\n",
    "                            logger.info(f\"{metric_name}指标呈{trend}趋势\")\n",
    "                            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"分析性能趋势失败: {str(e)}\")\n",
    "    \n",
    "    def get_metric_history(self, metric_name, window=None):\n",
    "        \"\"\"获取指定指标的历史数据\n",
    "        Args:\n",
    "            metric_name: 指标名称\n",
    "            window: 获取最近window个数据点，None表示获取全部\n",
    "        Returns:\n",
    "            list: 指标历史数据\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self.lock:\n",
    "                if metric_name not in self.metrics:\n",
    "                    logger.warning(f\"未找到指标: {metric_name}\")\n",
    "                    return []\n",
    "                    \n",
    "                values = list(self.metrics[metric_name])\n",
    "                if window is not None:\n",
    "                    values = values[-window:]\n",
    "                return values\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取指标历史数据失败: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def set_threshold(self, metric_name, value):\n",
    "        \"\"\"设置警报阈值\n",
    "        Args:\n",
    "            metric_name: 指标名称\n",
    "            value: 阈值\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self.lock:\n",
    "                if metric_name not in self.thresholds:\n",
    "                    logger.warning(f\"未找到阈值配置: {metric_name}\")\n",
    "                    return\n",
    "                    \n",
    "                self.thresholds[metric_name] = value\n",
    "                logger.info(f\"已更新{metric_name}的警报阈值为: {value}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"设置警报阈值失败: {str(e)}\")\n",
    "    \n",
    "    def export_metrics(self, start_time=None, end_time=None):\n",
    "        \"\"\"导出指定时间范围的性能指标\n",
    "        Args:\n",
    "            start_time: 开始时间，datetime对象\n",
    "            end_time: 结束时间，datetime对象\n",
    "        Returns:\n",
    "            dict: 导出的性能指标\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.log_file, 'r') as f:\n",
    "                logs = json.load(f)\n",
    "                \n",
    "            # 过滤时间范围\n",
    "            if start_time or end_time:\n",
    "                filtered_logs = []\n",
    "                for log in logs:\n",
    "                    log_time = datetime.strptime(log['timestamp'], '%Y-%m-%d %H:%M:%S')\n",
    "                    if start_time and log_time < start_time:\n",
    "                        continue\n",
    "                    if end_time and log_time > end_time:\n",
    "                        continue\n",
    "                    filtered_logs.append(log)\n",
    "                logs = filtered_logs\n",
    "                \n",
    "            return logs\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"导出性能指标失败: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"获取性能总结\"\"\"\n",
    "        try:\n",
    "            with self.lock:\n",
    "                summary = {}\n",
    "                for metric_name, values in self.metrics.items():\n",
    "                    if values:\n",
    "                        summary[metric_name] = {\n",
    "                            'current': values[-1],\n",
    "                            'mean': np.mean(values),\n",
    "                            'std': np.std(values),\n",
    "                            'min': np.min(values),\n",
    "                            'max': np.max(values)\n",
    "                        }\n",
    "                \n",
    "                # 添加趋势分析\n",
    "                trends = {}\n",
    "                for metric_name, values in self.metrics.items():\n",
    "                    if len(values) >= self.trend_window:\n",
    "                        x = np.arange(self.trend_window)\n",
    "                        y = np.array(list(values)[-self.trend_window:])\n",
    "                        slope = np.polyfit(x, y, 1)[0]\n",
    "                        \n",
    "                        if abs(slope) > self.trend_threshold:\n",
    "                            trend = \"上升\" if slope > 0 else \"下降\"\n",
    "                        else:\n",
    "                            trend = \"稳定\"\n",
    "                            \n",
    "                        trends[metric_name] = {\n",
    "                            'trend': trend,\n",
    "                            'slope': slope\n",
    "                        }\n",
    "                \n",
    "                summary['trends'] = trends\n",
    "                return summary\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取性能总结失败: {str(e)}\")\n",
    "            return {}\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"重置性能监控器\"\"\"\n",
    "        try:\n",
    "            with self.lock:\n",
    "                for metric_name in self.metrics:\n",
    "                    self.metrics[metric_name].clear()\n",
    "                logger.info(\"性能监控器已重置\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"重置性能监控器失败: {str(e)}\")\n",
    "    \n",
    "    def get_alerts_history(self):\n",
    "        \"\"\"获取警报历史\"\"\"\n",
    "        try:\n",
    "            with open(self.log_file, 'r') as f:\n",
    "                logs = json.load(f)\n",
    "            \n",
    "            alerts = []\n",
    "            for log in logs:\n",
    "                if 'alerts' in log:\n",
    "                    alerts.extend(log['alerts'])\n",
    "            return alerts\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取警报历史失败: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def get_performance_report(self, start_time=None, end_time=None):\n",
    "        \"\"\"生成性能报告\n",
    "        Args:\n",
    "            start_time: 开始时间，datetime对象\n",
    "            end_time: 结束时间，datetime对象\n",
    "        Returns:\n",
    "            dict: 性能报告\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 获取指定时间范围的指标\n",
    "            metrics = self.export_metrics(start_time, end_time)\n",
    "            \n",
    "            # 获取警报历史\n",
    "            alerts = self.get_alerts_history()\n",
    "            \n",
    "            # 获取当前性能总结\n",
    "            summary = self.get_summary()\n",
    "            \n",
    "            report = {\n",
    "                'time_range': {\n",
    "                    'start': start_time.strftime('%Y-%m-%d %H:%M:%S') if start_time else None,\n",
    "                    'end': end_time.strftime('%Y-%m-%d %H:%M:%S') if end_time else None\n",
    "                },\n",
    "                'metrics': metrics,\n",
    "                'alerts': alerts,\n",
    "                'summary': summary,\n",
    "                'generated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "            \n",
    "            return report\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"生成性能报告失败: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "    def analyze_performance_trend(self, match_history):\n",
    "        \"\"\"分析性能趋势\"\"\"\n",
    "        try:\n",
    "            # 1. 计算移动平均\n",
    "            window_size = 100\n",
    "            moving_avg = np.convolve(match_history, np.ones(window_size)/window_size, mode='valid')\n",
    "            \n",
    "            # 2. 判断趋势\n",
    "            if len(moving_avg) >= 2:\n",
    "                trend = moving_avg[-1] - moving_avg[-2]\n",
    "                \n",
    "                if trend > 0.1:\n",
    "                    return \"IMPROVING\"\n",
    "                elif trend < -0.1:\n",
    "                    return \"DEGRADING\"\n",
    "                else:\n",
    "                    return \"STABLE\"\n",
    "                    \n",
    "            return \"INSUFFICIENT_DATA\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"分析性能趋势时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"启动监控线程\"\"\"\n",
    "        self._running = True\n",
    "        self._thread.start()\n",
    "        logger.info(\"性能监控器已启动\")\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"停止监控线程\"\"\"\n",
    "        self._running = False\n",
    "        self._thread.join()\n",
    "        logger.info(\"性能监控器已停止\")\n",
    "    \n",
    "    def _monitor_loop(self):\n",
    "        \"\"\"监控主循环\"\"\"\n",
    "        while self._running:\n",
    "            # 这里添加实际的监控逻辑\n",
    "            time.sleep(1)  # 每秒采集一次数据\n",
    "\n",
    "# 创建全局实例\n",
    "performance_monitor = PerformanceMonitor(save_dir='logs/performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97c6419e-b74f-4e18-8bc3-350308cad779",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 20:34:36,841 [INFO] __main__ - 系统管理器初始化完成\n"
     ]
    }
   ],
   "source": [
    "#9 系统检查\\system_manager.py\n",
    "import os\n",
    "import psutil\n",
    "import logging\n",
    "import time\n",
    "import gc\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "# 获取logger实例\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SystemManager:\n",
    "    \"\"\"系统管理器 - 单例模式\"\"\"\n",
    "    _instance = None\n",
    "    \n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self):\n",
    "        if not hasattr(self, 'initialized'):\n",
    "            # 初始化监控器\n",
    "            self.memory_monitor = MemoryMonitor()\n",
    "            self.system_monitor = SystemMonitor()\n",
    "            self.system_cleaner = SystemCleaner()\n",
    "            \n",
    "            # 初始化阈值\n",
    "            self.memory_warning_threshold = 0.85  # 85%内存使用率警告\n",
    "            self.memory_critical_threshold = 0.95  # 95%内存使用率危险\n",
    "            self.cpu_warning_threshold = 0.85  # 85% CPU使用率警告\n",
    "            \n",
    "            # 初始化状态\n",
    "            self.last_cleanup_time = time.time()\n",
    "            self.cleanup_interval = 300  # 5分钟执行一次清理\n",
    "            self.initialized = True\n",
    "            self.compatibility_checked = False\n",
    "            \n",
    "            logger.info(\"系统管理器初始化完成\")\n",
    "    \n",
    "    def check_system_health(self):\n",
    "        \"\"\"检查系统健康状态\"\"\"\n",
    "        try:\n",
    "            # 检查内存使用\n",
    "            memory_info = self.memory_monitor.check_memory()\n",
    "            if not memory_info['healthy']:\n",
    "                self.handle_memory_warning(memory_info)\n",
    "            \n",
    "            # 检查系统状态\n",
    "            system_status = self.system_monitor.check_system_status()\n",
    "            if not system_status['healthy']:\n",
    "                self.handle_system_warning(system_status)\n",
    "            \n",
    "            # 定期清理\n",
    "            self._perform_periodic_cleanup()\n",
    "            \n",
    "            return memory_info['healthy'] and system_status['healthy']\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"检查系统健康状态时出错: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def handle_memory_warning(self, memory_info):\n",
    "        \"\"\"处理内存警告\"\"\"\n",
    "        try:\n",
    "            if memory_info['usage_percent'] > self.memory_critical_threshold:\n",
    "                logger.critical(\"内存使用率超过临界值，执行紧急清理\")\n",
    "                self.system_cleaner._emergency_cleanup()\n",
    "            elif memory_info['usage_percent'] > self.memory_warning_threshold:\n",
    "                logger.warning(\"内存使用率较高，执行常规清理\")\n",
    "                self.system_cleaner._regular_cleanup()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"处理内存警告时出错: {str(e)}\")\n",
    "    \n",
    "    def handle_system_warning(self, status):\n",
    "        \"\"\"处理系统警告\"\"\"\n",
    "        try:\n",
    "            if status.get('cpu_warning'):\n",
    "                logger.warning(f\"CPU使用率过高: {status['cpu_percent']}%\")\n",
    "            if status.get('gpu_warning'):\n",
    "                logger.warning(f\"GPU使用率过高: {status['gpu_usage']}%\")\n",
    "            if status.get('disk_warning'):\n",
    "                logger.warning(f\"磁盘使用率过高: {status['disk_percent']}%\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"处理系统警告时出错: {str(e)}\")\n",
    "    \n",
    "    def _perform_periodic_cleanup(self):\n",
    "        \"\"\"执行定期清理\"\"\"\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_cleanup_time > self.cleanup_interval:\n",
    "            try:\n",
    "                self.system_cleaner.check_and_cleanup()\n",
    "                self.last_cleanup_time = current_time\n",
    "            except Exception as e:\n",
    "                logger.error(f\"执行定期清理时出错: {str(e)}\")\n",
    "    \n",
    "    def get_system_metrics(self):\n",
    "        \"\"\"获取系统指标\"\"\"\n",
    "        try:\n",
    "            return {\n",
    "                'memory': self.memory_monitor.get_memory_usage(),\n",
    "                'cpu': psutil.cpu_percent(),\n",
    "                'disk': psutil.disk_usage('/').percent,\n",
    "                'gpu': self._get_gpu_metrics()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取系统指标时出错: {str(e)}\")\n",
    "            return {}\n",
    "    \n",
    "    def _get_gpu_metrics(self):\n",
    "        \"\"\"获取GPU指标\"\"\"\n",
    "        try:\n",
    "            if not tf.config.list_physical_devices('GPU'):\n",
    "                return None\n",
    "                \n",
    "            result = subprocess.check_output(\n",
    "                ['nvidia-smi', '--query-gpu=memory.used,memory.total,temperature.gpu', \n",
    "                 '--format=csv,nounits,noheader'],\n",
    "                encoding='utf-8'\n",
    "            )\n",
    "            used, total, temp = map(int, result.strip().split(','))\n",
    "            return {\n",
    "                'memory_used': used,\n",
    "                'memory_total': total,\n",
    "                'temperature': temp,\n",
    "                'utilization': used / total * 100\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取GPU指标时出错: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def check_system_compatibility(self):\n",
    "        # 实现系统检查逻辑\n",
    "        pass\n",
    "\n",
    "    def check_dependencies(self):\n",
    "        \"\"\"系统启动时自动调用\"\"\"\n",
    "        from ..notebooks.Untitled import check_requirements\n",
    "        need_install = check_requirements(requirements)\n",
    "        if need_install:\n",
    "            self.install_dependencies(need_install)\n",
    "\n",
    "    def install_dependencies(self, packages):\n",
    "        \"\"\"受控安装方法\"\"\"\n",
    "        # 记录安装日志\n",
    "        logger.info(f\"自动安装依赖: {packages}\")\n",
    "        # 调用安装逻辑\n",
    "        # ...\n",
    "\n",
    "class MemoryMonitor:\n",
    "    def check_memory(self):\n",
    "        return {'healthy': True}  # 示例实现\n",
    "\n",
    "class SystemMonitor:\n",
    "    def check_system_status(self):\n",
    "        return {'healthy': True}  # 示例实现\n",
    "\n",
    "class SystemCleaner:\n",
    "    def _emergency_cleanup(self):\n",
    "        gc.collect()\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "# 创建全局实例\n",
    "sys_manager = SystemManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37657e4f-da24-4b38-85be-6ef814e2372e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#10 模型配置管理\\model_config.py\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, Any, Optional\n",
    "import tensorflow as tf\n",
    "from cell1 import config_instance  # 修改后的导入\n",
    "\n",
    "# 获取logger实例\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ModelConfig:\n",
    "    \"\"\"模型配置管理类\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        初始化模型配置\n",
    "        \n",
    "        Args:\n",
    "            config_path: 配置文件路径,如果为None则使用默认配置\n",
    "        \"\"\"\n",
    "        self.config_path = config_path\n",
    "        self.config = self._load_config()\n",
    "        self.sequence_length = config_instance.SYSTEM_CONFIG['SAMPLE_CONFIG']['input_length']\n",
    "        \n",
    "    def _load_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"加载配置\"\"\"\n",
    "        try:\n",
    "            if self.config_path and os.path.exists(self.config_path):\n",
    "                with open(self.config_path, 'r', encoding='utf-8') as f:\n",
    "                    config = json.load(f)\n",
    "                logger.info(f\"从{self.config_path}加载配置\")\n",
    "                return config\n",
    "            \n",
    "            # 使用默认配置\n",
    "            return self._get_default_config()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"加载配置失败: {str(e)}\")\n",
    "            return self._get_default_config()\n",
    "    \n",
    "    def _get_default_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取默认配置\"\"\"\n",
    "        sample_cfg = config_instance.SYSTEM_CONFIG['SAMPLE_CONFIG']\n",
    "        return {\n",
    "            # 1. 基础配置\n",
    "            'base_config': {\n",
    "                'sequence_length': sample_cfg['input_length'],\n",
    "                'feature_dim': 5,\n",
    "                'prediction_range': sample_cfg['target_length'],\n",
    "                'batch_size': 32,\n",
    "                'epochs': 100\n",
    "            },\n",
    "            \n",
    "            # 2. 优化器配置\n",
    "            'optimizer_config': {\n",
    "                'optimizer_type': 'adam',\n",
    "                'learning_rate': 0.001,\n",
    "                'beta_1': 0.9,\n",
    "                'beta_2': 0.999,\n",
    "                'epsilon': 1e-7,\n",
    "                'weight_decay': 0.0001\n",
    "            },\n",
    "            \n",
    "            # 3. 损失函数配置\n",
    "            'loss_config': {\n",
    "                'loss_type': 'enhanced_match_loss',\n",
    "                'huber_delta': 1.0,\n",
    "                'focal_gamma': 2.0,\n",
    "                'label_smoothing': 0.1\n",
    "            },\n",
    "            \n",
    "            # 4. 模型架构配置\n",
    "            'architecture_config': {\n",
    "                # LSTM + GRU + 注意力模型\n",
    "                'model_1': {\n",
    "                    'lstm_units': 128,\n",
    "                    'lstm_dropout': 0.3,\n",
    "                    'gru_units': 64,\n",
    "                    'attention_heads': 4\n",
    "                },\n",
    "                \n",
    "                # 双向LSTM + Transformer模型\n",
    "                'model_2': {\n",
    "                    'bilstm_units': 128,\n",
    "                    'transformer_blocks': 2,\n",
    "                    'transformer_heads': 4\n",
    "                },\n",
    "                \n",
    "                # TCN + LSTM模型\n",
    "                'model_3': {\n",
    "                    'tcn_filters': 64,\n",
    "                    'tcn_kernel_size': 3,\n",
    "                    'lstm_units': 128\n",
    "                },\n",
    "                \n",
    "                # Transformer + 残差模型\n",
    "                'model_4': {\n",
    "                    'num_layers': 3,\n",
    "                    'num_heads': 4,\n",
    "                    'd_model': 128\n",
    "                },\n",
    "                \n",
    "                # 双向GRU + 注意力模型\n",
    "                'model_5': {\n",
    "                    'gru_units': 128,\n",
    "                    'attention_dim': 64\n",
    "                },\n",
    "                \n",
    "                # LSTM + CNN + 注意力模型\n",
    "                'model_6': {\n",
    "                    'lstm_units': 128,\n",
    "                    'cnn_filters': [64, 128],\n",
    "                    'attention_units': 64\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            # 5. 训练配置\n",
    "            'training_config': {\n",
    "                'early_stopping_patience': 20,\n",
    "                'reduce_lr_patience': 10,\n",
    "                'min_delta': 1e-4,\n",
    "                'validation_split': 0.2,\n",
    "                'shuffle': True\n",
    "            },\n",
    "            \n",
    "            # 6. 集成配置\n",
    "            'ensemble_config': {\n",
    "                'voting_method': 'weighted',\n",
    "                'min_weight': 0.1,\n",
    "                'max_weight': 0.9,\n",
    "                'weight_update_freq': 100\n",
    "            },\n",
    "            \n",
    "            # 7. 监控配置\n",
    "            'monitor_config': {\n",
    "                'performance_window': 500,\n",
    "                'alert_threshold': 0.2,\n",
    "                'recovery_patience': 10\n",
    "            },\n",
    "            \n",
    "            # 8. 保存配置\n",
    "            'save_config': {\n",
    "                'save_freq': 100,\n",
    "                'max_to_keep': 5,\n",
    "                'save_format': 'tf'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_model_config(self, model_index: int) -> Dict[str, Any]:\n",
    "        \"\"\"获取指定模型的配置\"\"\"\n",
    "        try:\n",
    "            return self.config['architecture_config'][f'model_{model_index}']\n",
    "        except KeyError:\n",
    "            logger.error(f\"未找到模型{model_index}的配置\")\n",
    "            return {}\n",
    "    \n",
    "    def get_optimizer_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取优化器配置\"\"\"\n",
    "        return self.config['optimizer_config']\n",
    "    \n",
    "    def get_loss_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取损失函数配置\"\"\"\n",
    "        return self.config['loss_config']\n",
    "    \n",
    "    def get_training_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取训练配置\"\"\"\n",
    "        return self.config['training_config']\n",
    "    \n",
    "    def update_config(self, new_config: Dict[str, Any]) -> None:\n",
    "        \"\"\"更新配置\"\"\"\n",
    "        try:\n",
    "            self.config.update(new_config)\n",
    "            self._save_config()\n",
    "            logger.info(\"配置更新成功\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"更新配置失败: {str(e)}\")\n",
    "    \n",
    "    def _save_config(self) -> None:\n",
    "        \"\"\"保存配置到文件\"\"\"\n",
    "        if self.config_path:\n",
    "            try:\n",
    "                with open(self.config_path, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(self.config, f, indent=4)\n",
    "                logger.info(f\"配置已保存到{self.config_path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"保存配置失败: {str(e)}\")\n",
    "    \n",
    "    def validate_config(self) -> bool:\n",
    "        \"\"\"验证配置有效性\"\"\"\n",
    "        try:\n",
    "            # 验证基础配置\n",
    "            base_config = self.config['base_config']\n",
    "            assert base_config['sequence_length'] > 0\n",
    "            assert base_config['feature_dim'] > 0\n",
    "            assert base_config['batch_size'] > 0\n",
    "            \n",
    "            # 验证优化器配置\n",
    "            optimizer_config = self.config['optimizer_config']\n",
    "            assert optimizer_config['learning_rate'] > 0\n",
    "            assert 0 < optimizer_config['beta_1'] < 1\n",
    "            assert 0 < optimizer_config['beta_2'] < 1\n",
    "            \n",
    "            # 验证模型架构配置\n",
    "            for model_name, model_config in self.config['architecture_config'].items():\n",
    "                assert all(value > 0 for value in model_config.values() if isinstance(value, (int, float)))\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except AssertionError as e:\n",
    "            logger.error(f\"配置验证失败: {str(e)}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"配置验证出错: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def save_best_params(self):\n",
    "        # 保存当前最佳参数组合\n",
    "        with open('best_params.json', 'w') as f:\n",
    "            json.dump(self.best_params, f)\n",
    "\n",
    "# 创建全局实例\n",
    "model_config = ModelConfig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1eb1b39-2cb7-45d9-8b18-22871b7affdd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 20:34:37,201 [WARNING] py.warnings - D:\\ruanjiananzhuang\\anaconda\\envs\\ace\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "[I 2025-02-14 20:34:37,659] Using an existing study with name 'model_optim_v1' instead of creating a new one.\n"
     ]
    }
   ],
   "source": [
    "#11 集成模型类\\model_ensemble.py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU, Bidirectional, Conv1D\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add\n",
    "import os\n",
    "import json\n",
    "import threading\n",
    "from cell1 import config_instance\n",
    "from cell13 import model_optimizer\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import torch\n",
    "\n",
    "# 获取logger实例\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ModelEnsemble:\n",
    "    \"\"\"模型集成类\"\"\"\n",
    "    \n",
    "    def __init__(self, sequence_length=14400, feature_dim=5):\n",
    "        \"\"\"\n",
    "        初始化模型集成\n",
    "        Args:\n",
    "            sequence_length: 输入序列长度\n",
    "            feature_dim: 特征维度\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.feature_dim = feature_dim\n",
    "        self.prediction_range = 2880  # 预测范围\n",
    "        self.models = []  # 模型列表\n",
    "        self.weights = np.ones(6) / 6  # 初始权重平均分配\n",
    "        \n",
    "        # 模型性能追踪\n",
    "        self.performance_history = {i: [] for i in range(6)}\n",
    "        \n",
    "        # 新增训练同步机制\n",
    "        self.training_lock = threading.Lock()\n",
    "        self.finished_models = 0\n",
    "        self.total_models = 6\n",
    "        \n",
    "        # 初始化模型\n",
    "        self._build_models()\n",
    "        logger.info(\"模型集成初始化完成\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def enhanced_match_loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        增强型匹配损失函数 - 结合匹配度评判和方向性引导\n",
    "        \n",
    "        Args:\n",
    "            y_true: 真实值 (batch_size, 2880, 5) - 2880期目标值\n",
    "            y_pred: 预测值 (batch_size, 5) - 预测的一组五位数\n",
    "            \n",
    "        Returns:\n",
    "            total_loss: 综合损失值,包含匹配损失和方向性损失\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 1. 预处理\n",
    "            y_pred_expanded = tf.expand_dims(y_pred, axis=1)  # (batch_size, 1, 5)\n",
    "            y_pred_rounded = tf.round(y_pred_expanded)\n",
    "            \n",
    "            # 2. 计算匹配情况\n",
    "            matches = tf.cast(tf.equal(y_true, y_pred_rounded), tf.float32)  # (batch_size, 2880, 5)\n",
    "            match_counts = tf.reduce_sum(matches, axis=-1)  # (batch_size, 2880)\n",
    "            \n",
    "            # 3. 找出最佳匹配的目标值\n",
    "            best_match_indices = tf.argmax(match_counts, axis=1)  # (batch_size,)\n",
    "            best_targets = tf.gather(y_true, best_match_indices, batch_dims=1)  # (batch_size, 5)\n",
    "            best_match_counts = tf.reduce_max(match_counts, axis=1)  # (batch_size,)\n",
    "            \n",
    "            # 4. 计算基础匹配损失\n",
    "            base_loss = 5.0 - best_match_counts  # (batch_size,)\n",
    "            \n",
    "            # 5. 计算方向性损失\n",
    "            # 计算预测值与最佳匹配目标的差异\n",
    "            value_diff = tf.squeeze(best_targets - y_pred, axis=1)  # (batch_size, 5)\n",
    "            \n",
    "            # 创建方向性掩码(只对未匹配的位置计算方向性损失)\n",
    "            direction_mask = tf.cast(\n",
    "                tf.not_equal(y_pred_rounded, tf.expand_dims(best_targets, axis=1)),\n",
    "                tf.float32\n",
    "            )  # (batch_size, 1, 5)\n",
    "            \n",
    "            # 使用sigmoid函数平滑方向性梯度\n",
    "            direction_factor = tf.sigmoid(value_diff * 2.0) * 2.0 - 1.0  # 范围(-1, 1)\n",
    "            \n",
    "            # 计算方向性损失(差异越大,损失越大)\n",
    "            direction_loss = tf.reduce_mean(\n",
    "                direction_mask * direction_factor * tf.abs(value_diff),\n",
    "                axis=-1\n",
    "            )  # (batch_size,)\n",
    "            \n",
    "            # 6. 完全匹配时损失为0\n",
    "            perfect_match = tf.cast(tf.equal(best_match_counts, 5.0), tf.float32)\n",
    "            \n",
    "            # 7. 组合损失(动态权重)\n",
    "            # 匹配数越少,方向性损失权重越大\n",
    "            direction_weight = tf.exp(-best_match_counts / 5.0) * 0.5  # 随匹配数增加呈指数衰减\n",
    "            total_loss = base_loss * (1.0 - perfect_match) + direction_weight * direction_loss\n",
    "            \n",
    "            # 8. 添加调试信息\n",
    "            tf.debugging.assert_all_finite(\n",
    "                total_loss,\n",
    "                \"Loss computation resulted in invalid values\"\n",
    "            )\n",
    "            \n",
    "            return total_loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"计算损失时出错: {str(e)}\")\n",
    "            return 5.0 * tf.ones_like(y_pred[:, 0])\n",
    "        \n",
    "    def _build_models(self):\n",
    "        \"\"\"构建所有模型\"\"\"\n",
    "        try:\n",
    "            model_params = {\n",
    "                'model_1': {\n",
    "                    'conv_filters': [32, 64, 128],\n",
    "                    'lstm_units': 128,\n",
    "                    'attention_heads': 8\n",
    "                },\n",
    "                'model_2': {\n",
    "                    'conv_filters': [64, 128],\n",
    "                    'lstm_units': 256,\n",
    "                    'dropout': 0.2\n",
    "                },\n",
    "                'model_3': {\n",
    "                    'gru_units': 128,\n",
    "                    'dense_units': [256, 128],\n",
    "                    'learning_rate': 0.001\n",
    "                },\n",
    "                'model_4': {\n",
    "                    'num_heads': 8,\n",
    "                    'key_dim': 64,\n",
    "                    'ff_dim': 256\n",
    "                },\n",
    "                'model_5': {\n",
    "                    'lstm_units': 128,\n",
    "                    'attention_heads': 4,\n",
    "                    'dropout': 0.1\n",
    "                },\n",
    "                'model_6': {\n",
    "                    'conv_filters': [32, 64],\n",
    "                    'gru_units': 128,\n",
    "                    'dense_units': [128, 64]\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            for i in range(6):\n",
    "                logger.info(f\"构建模型 {i+1}/6...\")\n",
    "                model = self._build_model(i+1, model_params[f'model_{i+1}'])\n",
    "                self.compile_model(model)  # 使用新的compile_model方法\n",
    "                self.models.append(model)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建模型时出错: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def _build_model(self, model_num, params):\n",
    "        \"\"\"构建增强版单个模型\"\"\"\n",
    "        try:\n",
    "            inputs = tf.keras.Input(shape=(self.sequence_length, self.feature_dim))\n",
    "            \n",
    "            # 1. 基础特征提取\n",
    "            x = self._build_basic_features(inputs)\n",
    "            \n",
    "            # 2. 高级特征分析\n",
    "            advanced_features = self._build_advanced_features(inputs)\n",
    "            \n",
    "            # 3. 特征融合\n",
    "            x = tf.keras.layers.Concatenate()([x, advanced_features])\n",
    "            \n",
    "            # 4. 深度特征提取\n",
    "            x = self._build_deep_features(x, params)\n",
    "            \n",
    "            # 5. 预测头\n",
    "            outputs = self._build_prediction_head(x)\n",
    "            \n",
    "            return Model(inputs=inputs, outputs=outputs)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建模型 {model_num} 时出错: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _add_positional_encoding(self, x):\n",
    "        \"\"\"添加位置编码\"\"\"\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        d_model = tf.shape(x)[-1]\n",
    "        \n",
    "        # 生成位置编码矩阵\n",
    "        position = tf.range(seq_len, dtype=tf.float32)[:, tf.newaxis]\n",
    "        div_term = tf.exp(tf.range(0, d_model, 2, dtype=tf.float32) * -(tf.math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe = tf.zeros((seq_len, d_model))\n",
    "        pe = tf.tensor_scatter_nd_update(\n",
    "            pe,\n",
    "            tf.stack([tf.range(seq_len), tf.range(0, d_model, 2)], axis=1),\n",
    "            tf.sin(position * div_term)\n",
    "        )\n",
    "        pe = tf.tensor_scatter_nd_update(\n",
    "            pe,\n",
    "            tf.stack([tf.range(seq_len), tf.range(1, d_model, 2)], axis=1),\n",
    "            tf.cos(position * div_term)\n",
    "        )\n",
    "        \n",
    "        return x + pe[tf.newaxis, :, :]\n",
    "        \n",
    "    def _build_multi_scale_features(self, x):\n",
    "        \"\"\"多尺度特征提取\"\"\"\n",
    "        # 不同kernel_size的并行卷积\n",
    "        conv1 = Conv1D(32, kernel_size=3, padding='same')(x)\n",
    "        conv2 = Conv1D(32, kernel_size=5, padding='same')(x)\n",
    "        conv3 = Conv1D(32, kernel_size=7, padding='same')(x)\n",
    "        \n",
    "        # 空洞卷积捕获更大感受野\n",
    "        dconv1 = Conv1D(32, kernel_size=3, dilation_rate=2, padding='same')(x)\n",
    "        dconv2 = Conv1D(32, kernel_size=3, dilation_rate=4, padding='same')(x)\n",
    "        \n",
    "        # 特征融合\n",
    "        x = tf.keras.layers.Concatenate()([conv1, conv2, conv3, dconv1, dconv2])\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        return tf.keras.layers.Activation('relu')(x)\n",
    "        \n",
    "    def _build_multi_task_head(self, x):\n",
    "        \"\"\"多任务输出头\"\"\"\n",
    "        # 主任务: 预测下一个数字\n",
    "        main_output = Dense(5, activation='linear', name='main_output')(x)\n",
    "        \n",
    "        # 辅助任务1: 预测趋势\n",
    "        trend = Dense(1, activation='tanh', name='trend')(x)\n",
    "        \n",
    "        # 辅助任务2: 预测波动性\n",
    "        volatility = Dense(1, activation='sigmoid', name='volatility')(x)\n",
    "        \n",
    "        return {\n",
    "            'main_output': main_output,\n",
    "            'trend': trend,\n",
    "            'volatility': volatility\n",
    "        }\n",
    "        \n",
    "    def _build_bilstm_residual(self, x, params):\n",
    "        \"\"\"双向LSTM + 残差连接\"\"\"\n",
    "        # 主路径\n",
    "        main = Bidirectional(LSTM(params['lstm_units'], return_sequences=True))(x)\n",
    "        \n",
    "        # 残差路径\n",
    "        residual = Conv1D(params['lstm_units']*2, kernel_size=1)(x)  # 匹配通道数\n",
    "        \n",
    "        # 残差连接\n",
    "        x = Add()([main, residual])\n",
    "        return LayerNormalization()(x)\n",
    "        \n",
    "    def _build_temporal_conv_lstm(self, x, params):\n",
    "        \"\"\"时空卷积 + LSTM\"\"\"\n",
    "        # 时序卷积模块\n",
    "        x = Conv1D(64, kernel_size=3, padding='same')(x)\n",
    "        x = tf.keras.layers.PReLU()(x)\n",
    "        x = tf.keras.layers.SpatialDropout1D(0.1)(x)\n",
    "        \n",
    "        # 因果卷积\n",
    "        x = Conv1D(64, kernel_size=3, padding='causal', dilation_rate=2)(x)\n",
    "        x = tf.keras.layers.PReLU()(x)\n",
    "        \n",
    "        return LSTM(params['lstm_units'], return_sequences=True)(x)\n",
    "        \n",
    "    def _build_gru_attention_skip(self, x, params):\n",
    "        \"\"\"GRU + 自注意力 + 跳跃连接\"\"\"\n",
    "        # GRU层\n",
    "        gru_out = GRU(params['gru_units'], return_sequences=True)(x)\n",
    "        \n",
    "        # 自注意力\n",
    "        att = MultiHeadAttention(num_heads=4, key_dim=16)(gru_out, gru_out)\n",
    "        \n",
    "        # 跳跃连接\n",
    "        x = Add()([att, x])\n",
    "        return LayerNormalization()(x)\n",
    "        \n",
    "    def _adjust_sequence_length(self, x):\n",
    "        \"\"\"调整序列长度到预测范围\"\"\"\n",
    "        # 获取当前序列长度\n",
    "        current_length = x.shape[1]\n",
    "        \n",
    "        if current_length > self.prediction_range:\n",
    "            # 如果当前长度大于目标长度，使用池化层减少长度\n",
    "            pool_size = current_length // self.prediction_range\n",
    "            x = tf.keras.layers.AveragePooling1D(pool_size=pool_size)(x)\n",
    "        elif current_length < self.prediction_range:\n",
    "            # 如果当前长度小于目标长度，使用上采样增加长度\n",
    "            x = tf.keras.layers.UpSampling1D(size=self.prediction_range // current_length)(x)\n",
    "        \n",
    "        # 确保最终长度精确匹配\n",
    "        x = tf.keras.layers.Conv1D(filters=32, kernel_size=1)(x)  # 1x1 卷积调整\n",
    "        x = tf.keras.layers.Reshape((-1, 32))(x)  # 重塑确保维度正确\n",
    "        x = tf.keras.layers.Dense(32)(x)  # 保持特征维度\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def _build_lstm_gru_attention(self, x, params):\n",
    "        \"\"\"构建LSTM+GRU+注意力模型\"\"\"\n",
    "        x = LSTM(params['lstm_units'], return_sequences=True)(x)\n",
    "        x = GRU(params['gru_units'], return_sequences=True)(x)\n",
    "        x = MultiHeadAttention(\n",
    "            num_heads=4,\n",
    "            key_dim=16,\n",
    "            value_dim=16\n",
    "        )(x, x)\n",
    "        return x\n",
    "        \n",
    "    def _build_bilstm(self, x, params):\n",
    "        \"\"\"构建双向LSTM模型\"\"\"\n",
    "        return Bidirectional(LSTM(params['lstm_units'], return_sequences=True))(x)\n",
    "        \n",
    "    def _build_cnn_lstm(self, x, params):\n",
    "        \"\"\"构建CNN+LSTM模型\"\"\"\n",
    "        x = Conv1D(filters=16, kernel_size=3, padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "        return LSTM(params['lstm_units'], return_sequences=True)(x)\n",
    "        \n",
    "    def _build_transformer(self, x, params):\n",
    "        \"\"\"构建Transformer模型\"\"\"\n",
    "        x = MultiHeadAttention(num_heads=params['num_heads'], \n",
    "                             key_dim=params['key_dim'])(x, x)\n",
    "        return LayerNormalization()(x)\n",
    "        \n",
    "    def _build_gru_attention(self, x, params):\n",
    "        \"\"\"构建GRU+注意力模型\"\"\"\n",
    "        x = GRU(params['gru_units'], return_sequences=True)(x)\n",
    "        return MultiHeadAttention(num_heads=4, key_dim=16)(x, x)\n",
    "        \n",
    "    def _build_lstm_cnn(self, x, params):\n",
    "        \"\"\"构建LSTM+CNN模型\"\"\"\n",
    "        x = LSTM(params['lstm_units'], return_sequences=True)(x)\n",
    "        x = Conv1D(filters=16, kernel_size=3, padding='same')(x)\n",
    "        return tf.keras.layers.BatchNormalization()(x)\n",
    "        \n",
    "    def update_weights(self, performance_metrics):\n",
    "        \"\"\"更新模型权重\"\"\"\n",
    "        try:\n",
    "            if not performance_metrics:\n",
    "                return\n",
    "                \n",
    "            # 根据性能指标计算新权重\n",
    "            scores = np.array([metrics['score'] for metrics in performance_metrics])\n",
    "            new_weights = scores / np.sum(scores)\n",
    "            \n",
    "            # 平滑更新\n",
    "            alpha = 0.3\n",
    "            self.weights = alpha * new_weights + (1 - alpha) * self.weights\n",
    "            \n",
    "            logger.info(f\"更新模型权重: {self.weights}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"更新模型权重时出错: {str(e)}\")\n",
    "            \n",
    "    def get_ensemble_prediction(self, X):\n",
    "        \"\"\"获取集成预测结果\"\"\"\n",
    "        try:\n",
    "            predictions = []\n",
    "            for i, model in enumerate(self.models):\n",
    "                pred = model.predict(X)  # shape: (batch_size, prediction_range, 5)\n",
    "                predictions.append(pred * self.weights[i])\n",
    "                \n",
    "            # 加权平均\n",
    "            ensemble_pred = np.sum(predictions, axis=0)  # shape: (batch_size, prediction_range, 5)\n",
    "            return ensemble_pred\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取集成预测时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _build_advanced_features(self, x):\n",
    "        \"\"\"构建高级特征分析\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # 1. 冷热号分析\n",
    "        hot_cold = self._analyze_hot_cold_numbers(x)\n",
    "        features.append(hot_cold)\n",
    "        \n",
    "        # 2. 号码频率统计\n",
    "        freq = self._analyze_frequency(x)\n",
    "        features.append(freq)\n",
    "        \n",
    "        # 3. 和值分析\n",
    "        sum_features = self._analyze_sum_value(x)\n",
    "        features.append(sum_features)\n",
    "        \n",
    "        # 4. 数字特征分析\n",
    "        digit_features = self._analyze_digit_patterns(x)\n",
    "        features.append(digit_features)\n",
    "        \n",
    "        # 5. 形态分析\n",
    "        pattern_features = self._analyze_number_patterns(x)\n",
    "        features.append(pattern_features)\n",
    "        \n",
    "        # 6. 012路分析\n",
    "        route_features = self._analyze_012_routes(x)\n",
    "        features.append(route_features)\n",
    "        \n",
    "        return tf.keras.layers.Concatenate()(features)\n",
    "\n",
    "    def _analyze_hot_cold_numbers(self, x, window_sizes=[100, 500, 1000]):\n",
    "        \"\"\"分析冷热号\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for window in window_sizes:\n",
    "            # 最近window期的频率\n",
    "            recent = x[:, -window:]\n",
    "            freq = tf.reduce_sum(tf.one_hot(tf.cast(recent, tf.int32), 10), axis=1)\n",
    "            features.append(freq)\n",
    "        \n",
    "        return tf.keras.layers.Concatenate()(features)\n",
    "\n",
    "    def _analyze_digit_patterns(self, x):\n",
    "        \"\"\"分析数字特征\"\"\"\n",
    "        # 1. 奇偶比\n",
    "        odd_even = tf.reduce_sum(tf.cast(x % 2 == 1, tf.float32), axis=-1, keepdims=True)\n",
    "        \n",
    "        # 2. 大小比 (5-9为大)\n",
    "        big_small = tf.reduce_sum(tf.cast(x >= 5, tf.float32), axis=-1, keepdims=True)\n",
    "        \n",
    "        # 3. 质合比\n",
    "        prime_numbers = tf.constant([2, 3, 5, 7])\n",
    "        is_prime = tf.reduce_sum(tf.cast(\n",
    "            tf.equal(x[..., None], prime_numbers), tf.float32\n",
    "        ), axis=-1)\n",
    "        prime_composite = tf.reduce_sum(is_prime, axis=-1, keepdims=True)\n",
    "        \n",
    "        # 4. 跨度\n",
    "        span = tf.reduce_max(x, axis=-1) - tf.reduce_min(x, axis=-1)\n",
    "        \n",
    "        return tf.concat([odd_even, big_small, prime_composite, span[..., None]], axis=-1)\n",
    "\n",
    "    def _analyze_number_patterns(self, x):\n",
    "        \"\"\"分析号码形态\"\"\"\n",
    "        # 1. 连号分析\n",
    "        consecutive = tf.reduce_sum(tf.cast(\n",
    "            x[:, 1:] == x[:, :-1] + 1, tf.float32\n",
    "        ), axis=-1, keepdims=True)\n",
    "        \n",
    "        # 2. 重复号分析\n",
    "        unique_counts = tf.reduce_sum(tf.one_hot(tf.cast(x, tf.int32), 10), axis=-2)\n",
    "        repeats = tf.reduce_sum(tf.cast(unique_counts > 1, tf.float32), axis=-1, keepdims=True)\n",
    "        \n",
    "        # 3. 形态识别\n",
    "        patterns = self._identify_number_patterns(x)\n",
    "        \n",
    "        return tf.concat([consecutive, repeats, patterns], axis=-1)\n",
    "\n",
    "    def _identify_number_patterns(self, x):\n",
    "        \"\"\"识别特定号码形态\"\"\"\n",
    "        # 1. 豹子号(AAAAA)\n",
    "        baozi = tf.reduce_all(x == x[..., :1], axis=-1, keepdims=True)\n",
    "        \n",
    "        # 2. 组5(AAAAB)\n",
    "        zu5 = tf.logical_or(\n",
    "            tf.reduce_sum(tf.cast(x == x[..., :1], tf.float32), axis=-1) == 4,\n",
    "            tf.reduce_sum(tf.cast(x == x[..., 1:2], tf.float32), axis=-1) == 4\n",
    "        )\n",
    "        \n",
    "        # 3. 组10(AAABB)\n",
    "        sorted_x = tf.sort(x, axis=-1)\n",
    "        zu10 = tf.logical_and(\n",
    "            tf.reduce_sum(tf.cast(sorted_x[..., :2] == sorted_x[..., :1], tf.float32), axis=-1) == 2,\n",
    "            tf.reduce_sum(tf.cast(sorted_x[..., 2:] == sorted_x[..., 2:3], tf.float32), axis=-1) == 2\n",
    "        )\n",
    "        \n",
    "        # 4. 顺子(连续5个数)\n",
    "        shunzi = tf.reduce_all(sorted_x[..., 1:] == sorted_x[..., :-1] + 1, axis=-1)\n",
    "        \n",
    "        return tf.cast(tf.stack([baozi, zu5, zu10, shunzi], axis=-1), tf.float32)\n",
    "\n",
    "    def _analyze_sum_value(self, x):\n",
    "        \"\"\"分析和值特征\"\"\"\n",
    "        # 1. 计算和值\n",
    "        sum_value = tf.reduce_sum(x, axis=-1, keepdims=True)\n",
    "        \n",
    "        # 2. 和值分布区间\n",
    "        sum_ranges = [\n",
    "            (0, 10), (11, 20), (21, 30), (31, 40), (41, 45)\n",
    "        ]\n",
    "        sum_dist = []\n",
    "        for low, high in sum_ranges:\n",
    "            in_range = tf.logical_and(\n",
    "                sum_value >= low,\n",
    "                sum_value <= high\n",
    "            )\n",
    "            sum_dist.append(tf.cast(in_range, tf.float32))\n",
    "        \n",
    "        # 3. 和值特征\n",
    "        sum_features = tf.concat([sum_value, tf.concat(sum_dist, axis=-1)], axis=-1)\n",
    "        \n",
    "        return sum_features\n",
    "\n",
    "    def _analyze_012_routes(self, x):\n",
    "        \"\"\"分析012路特征\"\"\"\n",
    "        # 1. 计算每个数字的路数\n",
    "        routes = tf.math.floormod(x, 3)  # 对3取余\n",
    "        \n",
    "        # 2. 统计每个位置的路数分布\n",
    "        route_distributions = []\n",
    "        for i in range(5):  # 五个位置\n",
    "            digit_routes = routes[..., i:i+1]\n",
    "            # 统计0,1,2路的数量\n",
    "            route_counts = []\n",
    "            for r in range(3):\n",
    "                count = tf.reduce_sum(\n",
    "                    tf.cast(digit_routes == r, tf.float32),\n",
    "                    axis=-1, keepdims=True\n",
    "                )\n",
    "                route_counts.append(count)\n",
    "            route_distributions.append(tf.concat(route_counts, axis=-1))\n",
    "        \n",
    "        # 3. 计算整体012路比例\n",
    "        total_route_dist = tf.reduce_mean(tf.concat(route_distributions, axis=-1), axis=-1, keepdims=True)\n",
    "        \n",
    "        # 4. 分析路数组合特征\n",
    "        route_patterns = self._analyze_route_patterns(routes)\n",
    "        \n",
    "        # 5. 计算相邻位置的路数关系\n",
    "        route_transitions = []\n",
    "        for i in range(4):\n",
    "            transition = tf.cast(\n",
    "                routes[..., i:i+1] == routes[..., i+1:i+2],\n",
    "                tf.float32\n",
    "            )\n",
    "            route_transitions.append(transition)\n",
    "        \n",
    "        # 6. 特征组合\n",
    "        features = [\n",
    "            *route_distributions,  # 每位路数分布\n",
    "            total_route_dist,     # 整体路数比例\n",
    "            route_patterns,       # 路数组合特征\n",
    "            *route_transitions    # 相邻位置路数关系\n",
    "        ]\n",
    "        \n",
    "        return tf.concat(features, axis=-1)\n",
    "\n",
    "    def _analyze_route_patterns(self, routes):\n",
    "        \"\"\"分析路数组合模式\"\"\"\n",
    "        # 1. 全0路\n",
    "        all_zero = tf.reduce_all(routes == 0, axis=-1, keepdims=True)\n",
    "        \n",
    "        # 2. 全1路\n",
    "        all_one = tf.reduce_all(routes == 1, axis=-1, keepdims=True)\n",
    "        \n",
    "        # 3. 全2路\n",
    "        all_two = tf.reduce_all(routes == 2, axis=-1, keepdims=True)\n",
    "        \n",
    "        # 4. 012路是否均匀分布(各有至少一个)\n",
    "        has_zero = tf.reduce_any(routes == 0, axis=-1, keepdims=True)\n",
    "        has_one = tf.reduce_any(routes == 1, axis=-1, keepdims=True)\n",
    "        has_two = tf.reduce_any(routes == 2, axis=-1, keepdims=True)\n",
    "        balanced = tf.logical_and(\n",
    "            tf.logical_and(has_zero, has_one),\n",
    "            has_two\n",
    "        )\n",
    "        \n",
    "        # 5. 主路特征(出现最多的路数)\n",
    "        route_counts = []\n",
    "        for r in range(3):\n",
    "            count = tf.reduce_sum(\n",
    "                tf.cast(routes == r, tf.float32),\n",
    "                axis=-1, keepdims=True\n",
    "            )\n",
    "            route_counts.append(count)\n",
    "        main_route = tf.argmax(tf.concat(route_counts, axis=-1), axis=-1)\n",
    "        \n",
    "        return tf.concat([\n",
    "            tf.cast(all_zero, tf.float32),\n",
    "            tf.cast(all_one, tf.float32),\n",
    "            tf.cast(all_two, tf.float32),\n",
    "            tf.cast(balanced, tf.float32),\n",
    "            tf.cast(main_route, tf.float32)[..., tf.newaxis]\n",
    "        ], axis=-1)\n",
    "\n",
    "    def _build_advanced_digit_features(self, x):\n",
    "        \"\"\"构建高级数字特征\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # 1. 每位数字的独立特征\n",
    "        for i in range(5):\n",
    "            digit = x[..., i:i+1]  # 提取第i位数字\n",
    "            \n",
    "            # 数字频率统计\n",
    "            freq = tf.keras.layers.Lambda(\n",
    "                lambda x: tf.cast(tf.histogram_fixed_width(x, [0, 9], nbins=10), tf.float32)\n",
    "            )(digit)\n",
    "            \n",
    "            # 数字转换模式\n",
    "            transitions = self._build_digit_transitions(digit)\n",
    "            \n",
    "            features.extend([freq, transitions])\n",
    "        \n",
    "        # 2. 数字组合特征\n",
    "        for i in range(5):\n",
    "            for j in range(i+1, 5):\n",
    "                # 两位数字组合\n",
    "                pair = tf.stack([x[..., i], x[..., j]], axis=-1)\n",
    "                pair_features = self._build_pair_features(pair)\n",
    "                features.append(pair_features)\n",
    "        \n",
    "        # 3. 完整号码特征\n",
    "        full_number = tf.reshape(x, (-1, self.sequence_length))  # 将5位数字合并为一个完整号码\n",
    "        number_features = self._build_number_features(full_number)\n",
    "        features.append(number_features)\n",
    "        \n",
    "        return tf.keras.layers.Concatenate()(features)\n",
    "\n",
    "    def _extract_periodic_pattern(self, x, period):\n",
    "        \"\"\"提取周期性模式\"\"\"\n",
    "        # 重塑以匹配周期\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        length = tf.shape(x)[1]\n",
    "        n_periods = length // period\n",
    "        \n",
    "        # 重塑为(batch, n_periods, period, features)\n",
    "        x_reshaped = tf.reshape(x[:, :n_periods*period], \n",
    "                               (batch_size, n_periods, period, -1))\n",
    "        \n",
    "        # 计算周期内模式\n",
    "        pattern = tf.reduce_mean(x_reshaped, axis=1)  # 平均周期模式\n",
    "        variance = tf.math.reduce_variance(x_reshaped, axis=1)  # 周期变异性\n",
    "        \n",
    "        return tf.concat([pattern, variance], axis=-1)\n",
    "\n",
    "    def _build_statistical_features(self, x):\n",
    "        \"\"\"构建统计特征\"\"\"\n",
    "        # 1. 移动统计\n",
    "        windows = [60, 360, 720]  # 1小时、6小时、12小时\n",
    "        stats = []\n",
    "        \n",
    "        for window in windows:\n",
    "            # 移动平均\n",
    "            ma = tf.keras.layers.AveragePooling1D(\n",
    "                pool_size=window, strides=1, padding='same')(x)\n",
    "            # 移动标准差\n",
    "            std = tf.math.reduce_std(\n",
    "                tf.stack([x, ma], axis=-1), axis=-1)\n",
    "            # 移动极差\n",
    "            pooled_max = tf.keras.layers.MaxPooling1D(\n",
    "                pool_size=window, strides=1, padding='same')(x)\n",
    "            pooled_min = -tf.keras.layers.MaxPooling1D(\n",
    "                pool_size=window, strides=1, padding='same')(-x)\n",
    "            range_stat = pooled_max - pooled_min\n",
    "            \n",
    "            stats.extend([ma, std, range_stat])\n",
    "        \n",
    "        # 2. 概率分布特征\n",
    "        probs = self._build_probability_features(x)\n",
    "        \n",
    "        return tf.concat(stats + [probs], axis=-1)\n",
    "\n",
    "    def _build_probability_features(self, x):\n",
    "        \"\"\"构建概率分布特征\"\"\"\n",
    "        # 1. 条件概率矩阵\n",
    "        cond_matrix = tf.zeros((10, 10, 5))  # 每位数字的转移概率\n",
    "        \n",
    "        # 2. 计算历史转移概率\n",
    "        for i in range(5):\n",
    "            current = tf.cast(x[..., i], tf.int32)\n",
    "            next_digit = tf.roll(current, shift=-1, axis=0)\n",
    "            \n",
    "            # 更新转移矩阵\n",
    "            for j in range(10):\n",
    "                for k in range(10):\n",
    "                    mask_current = tf.cast(current == j, tf.float32)\n",
    "                    mask_next = tf.cast(next_digit == k, tf.float32)\n",
    "                    prob = tf.reduce_mean(mask_current * mask_next)\n",
    "                    cond_matrix = tf.tensor_scatter_nd_update(\n",
    "                        cond_matrix,\n",
    "                        [[j, k, i]],\n",
    "                        [prob]\n",
    "                    )\n",
    "        \n",
    "        return cond_matrix\n",
    "\n",
    "    def _build_periodic_pattern_model(self, x, params):\n",
    "        \"\"\"周期性模式捕获模型\"\"\"\n",
    "        # 1. 多尺度周期性分析\n",
    "        periods = [12, 24, 60, 120, 360]\n",
    "        period_features = []\n",
    "        \n",
    "        for period in periods:\n",
    "            # 周期性卷积\n",
    "            conv = Conv1D(32, kernel_size=period, strides=1, padding='same')(x)\n",
    "            # 周期性池化\n",
    "            pool = tf.keras.layers.MaxPooling1D(pool_size=period)(conv)\n",
    "            period_features.append(pool)\n",
    "        \n",
    "        # 合并周期特征\n",
    "        x = tf.concat(period_features, axis=-1)\n",
    "        \n",
    "        # 2. 时序注意力\n",
    "        x = self._build_temporal_attention(x, params)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def _build_probability_head(self, x):\n",
    "        \"\"\"概率输出头\"\"\"\n",
    "        # 1. 每位数字的概率分布\n",
    "        digit_probs = []\n",
    "        for i in range(5):\n",
    "            # 输出每位数字0-9的概率\n",
    "            digit_prob = Dense(10, activation='softmax', name=f'digit_{i}')(x)\n",
    "            digit_probs.append(digit_prob)\n",
    "        \n",
    "        # 2. 组合概率\n",
    "        combination_prob = Dense(1, activation='sigmoid', name='combination')(x)\n",
    "        \n",
    "        # 3. 置信度\n",
    "        confidence = Dense(1, activation='sigmoid', name='confidence')(x)\n",
    "        \n",
    "        return {\n",
    "            'digits': digit_probs,\n",
    "            'combination': combination_prob,\n",
    "            'confidence': confidence\n",
    "        }\n",
    "\n",
    "    def compile_model(self, model):\n",
    "        \"\"\"编译模型\"\"\"\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss=self.enhanced_match_loss,  # 使用增强型匹配损失函数\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "    def _build_digit_correlation_model(self, x, params):\n",
    "        \"\"\"数字关联分析模型\"\"\"\n",
    "        # 1. 相邻数字关系\n",
    "        adjacent_patterns = Conv1D(64, kernel_size=2, strides=1, padding='same')(x)\n",
    "        \n",
    "        # 2. 数字组合模式\n",
    "        combination_patterns = []\n",
    "        for window in [3, 5, 7]:\n",
    "            pattern = Conv1D(32, kernel_size=window, padding='same')(x)\n",
    "            combination_patterns.append(pattern)\n",
    "        \n",
    "        # 3. 数字重复模式\n",
    "        repeat_patterns = self._build_repeat_patterns(x)\n",
    "        \n",
    "        # 合并所有模式\n",
    "        x = tf.keras.layers.Concatenate()([\n",
    "            adjacent_patterns,\n",
    "            *combination_patterns,\n",
    "            repeat_patterns\n",
    "        ])\n",
    "        \n",
    "        # LSTM层捕获长期关联\n",
    "        x = LSTM(128, return_sequences=True)(x)\n",
    "        return x\n",
    "\n",
    "    def _build_trend_prediction_model(self, x, params):\n",
    "        \"\"\"趋势预测模型\"\"\"\n",
    "        # 1. 移动平均特征\n",
    "        ma_features = []\n",
    "        for window in [12, 24, 48, 96]:\n",
    "            ma = tf.keras.layers.AveragePooling1D(\n",
    "                pool_size=window, \n",
    "                strides=1, \n",
    "                padding='same'\n",
    "            )(x)\n",
    "            ma_features.append(ma)\n",
    "        \n",
    "        # 2. 趋势特征\n",
    "        trend = self._build_trend_features(x)\n",
    "        \n",
    "        # 3. 组合特征\n",
    "        x = tf.keras.layers.Concatenate()([*ma_features, trend])\n",
    "        \n",
    "        # 4. 双向GRU捕获双向趋势\n",
    "        x = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "        return x\n",
    "\n",
    "    def _build_combination_pattern_model(self, x, params):\n",
    "        \"\"\"组合模式识别模型\"\"\"\n",
    "        # 1. 局部组合模式\n",
    "        local_patterns = self._build_local_patterns(x)\n",
    "        \n",
    "        # 2. 全局组合模式\n",
    "        global_patterns = self._build_global_patterns(x)\n",
    "        \n",
    "        # 3. 注意力机制关注重要组合\n",
    "        x = MultiHeadAttention(\n",
    "            num_heads=8,\n",
    "            key_dim=32\n",
    "        )(tf.concat([local_patterns, global_patterns], axis=-1))\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def _build_probability_model(self, x, params):\n",
    "        \"\"\"概率分布学习模型\"\"\"\n",
    "        # 1. 历史概率分布\n",
    "        hist_probs = self._build_historical_probabilities(x)\n",
    "        \n",
    "        # 2. 条件概率特征\n",
    "        cond_probs = self._build_conditional_probabilities(x)\n",
    "        \n",
    "        # 3. 组合概率模型\n",
    "        x = tf.keras.layers.Concatenate()([hist_probs, cond_probs])\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def _build_digit_transition_model(self, x, params):\n",
    "        \"\"\"数字转换预测模型\"\"\"\n",
    "        # 1. 数字转换矩阵\n",
    "        transition_matrix = self._build_transition_matrix(x)\n",
    "        \n",
    "        # 2. 状态转换LSTM\n",
    "        x = LSTM(128, return_sequences=True)(x)\n",
    "        \n",
    "        # 3. 注意力加权\n",
    "        x = MultiHeadAttention(\n",
    "            num_heads=4,\n",
    "            key_dim=32\n",
    "        )(x, transition_matrix)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def _build_temporal_attention(self, x, params):\n",
    "        \"\"\"构建时序注意力层\"\"\"\n",
    "        # 多头自注意力\n",
    "        attention_output = MultiHeadAttention(\n",
    "            num_heads=8,\n",
    "            key_dim=64,\n",
    "            value_dim=64,\n",
    "            dropout=0.1\n",
    "        )(x, x)\n",
    "        \n",
    "        # 残差连接和层归一化\n",
    "        x = Add()([x, attention_output])\n",
    "        x = LayerNormalization()(x)\n",
    "        \n",
    "        # 前馈网络\n",
    "        ffn = Dense(256, activation='relu')(x)\n",
    "        ffn = Dense(x.shape[-1])(ffn)\n",
    "        \n",
    "        # 再次残差连接和层归一化\n",
    "        x = Add()([x, ffn])\n",
    "        x = LayerNormalization()(x)\n",
    "        \n",
    "        return x \n",
    "\n",
    "    def _build_combination_predictor(self, x, params):\n",
    "        \"\"\"组合模式预测器\"\"\"\n",
    "        # 1. 提取数字组合特征\n",
    "        combinations = []\n",
    "        for i in range(5):\n",
    "            for j in range(i+1, 5):\n",
    "                # 两位数字组合的频率分析\n",
    "                pair = tf.stack([x[..., i], x[..., j]], axis=-1)\n",
    "                pair_conv = Conv1D(32, kernel_size=2, padding='same')(pair)\n",
    "                combinations.append(pair_conv)\n",
    "        \n",
    "        # 2. 组合模式分析\n",
    "        x = tf.keras.layers.Concatenate()(combinations)\n",
    "        x = LSTM(128, return_sequences=True)(x)\n",
    "        x = MultiHeadAttention(num_heads=8, key_dim=32)(x, x)\n",
    "        return x\n",
    "\n",
    "    def _build_probability_predictor(self, x, params):\n",
    "        \"\"\"概率分布预测器\"\"\"\n",
    "        # 1. 计算历史概率分布\n",
    "        digit_probs = []\n",
    "        for i in range(5):\n",
    "            # 每位数字的概率分布\n",
    "            digit = tf.cast(x[..., i], tf.int32)\n",
    "            probs = tf.keras.layers.Lambda(\n",
    "                lambda x: tf.cast(tf.histogram_fixed_width(x, [0, 9], nbins=10), tf.float32)\n",
    "            )(digit)\n",
    "            digit_probs.append(probs)\n",
    "        \n",
    "        # 2. 条件概率计算\n",
    "        cond_probs = self._build_conditional_probabilities(x)\n",
    "        \n",
    "        # 3. 概率模型\n",
    "        x = tf.keras.layers.Concatenate()(digit_probs + [cond_probs])\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        return x\n",
    "\n",
    "    def _build_periodic_predictor(self, x, params):\n",
    "        \"\"\"周期模式预测器\"\"\"\n",
    "        # 1. 多周期分析\n",
    "        periods = [60, 120, 360, 720, 1440]  # 1小时到24小时的周期\n",
    "        period_features = []\n",
    "        \n",
    "        for period in periods:\n",
    "            # 提取周期特征\n",
    "            pattern = self._extract_periodic_pattern(x, period)\n",
    "            # 周期性注意力\n",
    "            attention = MultiHeadAttention(\n",
    "                num_heads=4, \n",
    "                key_dim=32\n",
    "            )(pattern, pattern)\n",
    "            period_features.append(attention)\n",
    "        \n",
    "        # 2. 周期特征融合\n",
    "        x = tf.keras.layers.Concatenate()(period_features)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        return x\n",
    "\n",
    "    def _build_trend_predictor(self, x, params):\n",
    "        \"\"\"趋势预测器\"\"\"\n",
    "        # 1. 多尺度趋势分析\n",
    "        trends = []\n",
    "        windows = [60, 360, 720, 1440]  # 不同时间尺度\n",
    "        \n",
    "        for window in windows:\n",
    "            # 移动平均\n",
    "            ma = tf.keras.layers.AveragePooling1D(\n",
    "                pool_size=window, strides=1, padding='same')(x)\n",
    "            # 趋势方向\n",
    "            trend = tf.sign(x - ma)\n",
    "            trends.append(trend)\n",
    "        \n",
    "        # 2. 趋势特征融合\n",
    "        x = tf.keras.layers.Concatenate()(trends)\n",
    "        x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "        return x\n",
    "\n",
    "    def _build_statistical_predictor(self, x, params):\n",
    "        \"\"\"统计模式预测器\"\"\"\n",
    "        # 1. 统计特征提取\n",
    "        stats = []\n",
    "        \n",
    "        # 均值特征\n",
    "        mean = tf.reduce_mean(x, axis=1, keepdims=True)\n",
    "        # 标准差特征\n",
    "        std = tf.math.reduce_std(x, axis=1, keepdims=True)\n",
    "        # 峰度\n",
    "        kurtosis = tf.reduce_mean(tf.pow(x - mean, 4), axis=1, keepdims=True) / tf.pow(std, 4)\n",
    "        # 偏度\n",
    "        skewness = tf.reduce_mean(tf.pow(x - mean, 3), axis=1, keepdims=True) / tf.pow(std, 3)\n",
    "        \n",
    "        stats.extend([mean, std, kurtosis, skewness])\n",
    "        \n",
    "        # 2. 统计模型\n",
    "        x = tf.keras.layers.Concatenate()(stats)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        return x\n",
    "\n",
    "    def _build_ensemble_predictor(self, x, params):\n",
    "        \"\"\"集成预测器\"\"\"\n",
    "        # 1. 多模型特征\n",
    "        features = []\n",
    "        \n",
    "        # 组合模式特征\n",
    "        comb_features = self._build_combination_predictor(x, params)\n",
    "        # 概率特征\n",
    "        prob_features = self._build_probability_predictor(x, params)\n",
    "        # 周期特征\n",
    "        period_features = self._build_periodic_predictor(x, params)\n",
    "        # 趋势特征\n",
    "        trend_features = self._build_trend_predictor(x, params)\n",
    "        # 统计特征\n",
    "        stat_features = self._build_statistical_predictor(x, params)\n",
    "        \n",
    "        features.extend([\n",
    "            comb_features, prob_features, period_features,\n",
    "            trend_features, stat_features\n",
    "        ])\n",
    "        \n",
    "        # 2. 特征融合\n",
    "        x = tf.keras.layers.Concatenate()(features)\n",
    "        \n",
    "        # 3. 注意力加权\n",
    "        x = MultiHeadAttention(\n",
    "            num_heads=8,\n",
    "            key_dim=64,\n",
    "            value_dim=64\n",
    "        )(x, x)\n",
    "        \n",
    "        # 4. 最终预测层\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        \n",
    "        return x \n",
    "\n",
    "    def _build_number_features(self, numbers):\n",
    "        \"\"\"提取完整号码特征\"\"\"\n",
    "        # 1. 号码间隔分析\n",
    "        intervals = self._analyze_intervals(numbers)\n",
    "        \n",
    "        # 2. 号码重复模式\n",
    "        repeats = self._analyze_repeats(numbers)\n",
    "        \n",
    "        # 3. 号码差值特征\n",
    "        diffs = self._analyze_differences(numbers)\n",
    "        \n",
    "        # 4. 号码分布特征\n",
    "        distributions = self._analyze_distributions(numbers)\n",
    "        \n",
    "        return tf.keras.layers.Concatenate()([\n",
    "            intervals, repeats, diffs, distributions\n",
    "        ])\n",
    "\n",
    "    def _analyze_intervals(self, numbers):\n",
    "        \"\"\"分析号码出现间隔\"\"\"\n",
    "        def get_intervals(sequence):\n",
    "            # 计算每个号码的出现间隔\n",
    "            intervals_dict = {}\n",
    "            for i in range(len(sequence)):\n",
    "                num = sequence[i]\n",
    "                if num not in intervals_dict:\n",
    "                    intervals_dict[num] = []\n",
    "                else:\n",
    "                    last_pos = max(idx for idx, val in enumerate(sequence[:i]) if val == num)\n",
    "                    intervals_dict[num].append(i - last_pos)\n",
    "            return intervals_dict\n",
    "        \n",
    "        intervals = tf.keras.layers.Lambda(get_intervals)(numbers)\n",
    "        return Dense(64, activation='relu')(intervals) \n",
    "\n",
    "    def _build_prediction_head(self, x):\n",
    "        \"\"\"构建预测输出头\"\"\"\n",
    "        # 1. 每位数字的概率分布\n",
    "        digit_predictions = []\n",
    "        for i in range(5):\n",
    "            digit_pred = Dense(10, activation='softmax', name=f'digit_{i}')(x)\n",
    "            digit_predictions.append(digit_pred)\n",
    "        \n",
    "        # 2. 完整号码匹配概率\n",
    "        match_prob = Dense(self.prediction_range, activation='sigmoid', name='match_prob')(x)\n",
    "        \n",
    "        # 3. 预测置信度\n",
    "        confidence = Dense(1, activation='sigmoid', name='confidence')(x)\n",
    "        \n",
    "        return {\n",
    "            'digits': digit_predictions,\n",
    "            'match_prob': match_prob,\n",
    "            'confidence': confidence\n",
    "        } \n",
    "\n",
    "    def _build_pattern_features(self, x):\n",
    "        \"\"\"构建形态特征分析\"\"\"\n",
    "        # 1. 当前形态识别\n",
    "        current_patterns = self._identify_patterns(x)\n",
    "        \n",
    "        # 2. 形态遗漏值分析\n",
    "        pattern_gaps = self._analyze_pattern_gaps(x)\n",
    "        \n",
    "        # 3. 形态转换规律\n",
    "        pattern_transitions = self._analyze_pattern_transitions(x)\n",
    "        \n",
    "        return tf.keras.layers.Concatenate()([\n",
    "            current_patterns,\n",
    "            pattern_gaps,\n",
    "            pattern_transitions\n",
    "        ])\n",
    "\n",
    "    def _identify_patterns(self, x):\n",
    "        \"\"\"识别号码形态\"\"\"\n",
    "        patterns = []\n",
    "        \n",
    "        # 1. 豹子号(AAAAA)\n",
    "        baozi = tf.reduce_all(x == x[..., :1], axis=-1, keepdims=True)\n",
    "        patterns.append(tf.cast(baozi, tf.float32))\n",
    "        \n",
    "        # 2. 组5(AAAAB)\n",
    "        sorted_x = tf.sort(x, axis=-1)\n",
    "        zu5 = tf.logical_and(\n",
    "            tf.reduce_sum(tf.cast(sorted_x[..., :4] == sorted_x[..., :1], tf.float32), axis=-1) == 4,\n",
    "            sorted_x[..., 4] != sorted_x[..., 0]\n",
    "        )\n",
    "        patterns.append(tf.cast(zu5, tf.float32)[..., tf.newaxis])\n",
    "        \n",
    "        # 3. 组10(AAABB)\n",
    "        zu10 = tf.logical_and(\n",
    "            tf.reduce_sum(tf.cast(sorted_x[..., :3] == sorted_x[..., :1], tf.float32), axis=-1) == 3,\n",
    "            tf.reduce_sum(tf.cast(sorted_x[..., 3:] == sorted_x[..., 3:4], tf.float32), axis=-1) == 2\n",
    "        )\n",
    "        patterns.append(tf.cast(zu10, tf.float32)[..., tf.newaxis])\n",
    "        \n",
    "        # 4. 组20(AAABC)\n",
    "        zu20 = tf.logical_and(\n",
    "            tf.reduce_sum(tf.cast(sorted_x[..., :3] == sorted_x[..., :1], tf.float32), axis=-1) == 3,\n",
    "            sorted_x[..., 3] != sorted_x[..., 4]\n",
    "        )\n",
    "        patterns.append(tf.cast(zu20, tf.float32)[..., tf.newaxis])\n",
    "        \n",
    "        # 5. 组30(AABBC)\n",
    "        zu30 = tf.logical_and(\n",
    "            tf.reduce_sum(tf.cast(sorted_x[..., :2] == sorted_x[..., :1], tf.float32), axis=-1) == 2,\n",
    "            tf.reduce_sum(tf.cast(sorted_x[..., 2:4] == sorted_x[..., 2:3], tf.float32), axis=-1) == 2\n",
    "        )\n",
    "        patterns.append(tf.cast(zu30, tf.float32)[..., tf.newaxis])\n",
    "        \n",
    "        # 6. 组60(AABCD)\n",
    "        zu60 = tf.logical_and(\n",
    "            tf.reduce_sum(tf.cast(sorted_x[..., :2] == sorted_x[..., :1], tf.float32), axis=-1) == 2,\n",
    "            tf.reduce_all(sorted_x[..., 2:] != sorted_x[..., 1:4], axis=-1)\n",
    "        )\n",
    "        patterns.append(tf.cast(zu60, tf.float32)[..., tf.newaxis])\n",
    "        \n",
    "        # 7. 组120(ABCDE)\n",
    "        zu120 = tf.reduce_all(sorted_x[..., 1:] > sorted_x[..., :-1], axis=-1, keepdims=True)\n",
    "        patterns.append(tf.cast(zu120, tf.float32))\n",
    "        \n",
    "        return tf.concat(patterns, axis=-1)\n",
    "\n",
    "    def _analyze_pattern_gaps(self, x):\n",
    "        \"\"\"分析形态遗漏值\"\"\"\n",
    "        # 1. 初始化遗漏值计数器\n",
    "        gap_counters = tf.zeros_like(x[..., :7])  # 7种形态\n",
    "        \n",
    "        # 2. 计算每种形态的遗漏值\n",
    "        def update_gaps(sequence):\n",
    "            gaps = []\n",
    "            for i in range(7):  # 7种形态\n",
    "                last_pos = -1\n",
    "                current_gap = 0\n",
    "                pattern_positions = tf.where(sequence[:, i])\n",
    "                \n",
    "                if tf.size(pattern_positions) > 0:\n",
    "                    last_pos = tf.reduce_max(pattern_positions)\n",
    "                    current_gap = tf.shape(sequence)[0] - 1 - last_pos\n",
    "                \n",
    "                gaps.append(current_gap)\n",
    "            return tf.stack(gaps)\n",
    "        \n",
    "        # 3. 应用遗漏值计算\n",
    "        patterns = self._identify_patterns(x)\n",
    "        gaps = tf.keras.layers.Lambda(update_gaps)(patterns)\n",
    "        \n",
    "        # 4. 遗漏值特征\n",
    "        gap_features = []\n",
    "        \n",
    "        # 当前遗漏值\n",
    "        gap_features.append(gaps)\n",
    "        \n",
    "        # 历史最大遗漏值\n",
    "        max_gaps = tf.reduce_max(gaps, axis=0, keepdims=True)\n",
    "        gap_features.append(max_gaps)\n",
    "        \n",
    "        # 历史平均遗漏值\n",
    "        mean_gaps = tf.reduce_mean(gaps, axis=0, keepdims=True)\n",
    "        gap_features.append(mean_gaps)\n",
    "        \n",
    "        # 遗漏值分布\n",
    "        gap_dist = tf.keras.layers.Lambda(\n",
    "            lambda x: tf.cast(tf.histogram_fixed_width(x, [0, 1000], nbins=50), tf.float32)\n",
    "        )(gaps)\n",
    "        gap_features.append(gap_dist)\n",
    "        \n",
    "        return tf.concat(gap_features, axis=-1)\n",
    "\n",
    "    def _analyze_pattern_transitions(self, x):\n",
    "        \"\"\"分析形态转换规律\"\"\"\n",
    "        # 1. 识别所有形态\n",
    "        patterns = self._identify_patterns(x)\n",
    "        \n",
    "        # 2. 计算形态转换矩阵\n",
    "        def get_transition_matrix(sequence):\n",
    "            matrix = tf.zeros((7, 7))  # 7x7转换矩阵\n",
    "            for i in range(len(sequence)-1):\n",
    "                current = tf.argmax(sequence[i])\n",
    "                next_pattern = tf.argmax(sequence[i+1])\n",
    "                matrix = tf.tensor_scatter_nd_update(\n",
    "                    matrix,\n",
    "                    [[current, next_pattern]],\n",
    "                    [1.0]\n",
    "                )\n",
    "            return matrix\n",
    "        \n",
    "        transition_matrix = tf.keras.layers.Lambda(get_transition_matrix)(patterns)\n",
    "        \n",
    "        # 3. 提取转换特征\n",
    "        transitions = []\n",
    "        \n",
    "        # 转换概率\n",
    "        prob_matrix = transition_matrix / (tf.reduce_sum(transition_matrix, axis=-1, keepdims=True) + 1e-7)\n",
    "        transitions.append(tf.reshape(prob_matrix, [-1]))\n",
    "        \n",
    "        # 最常见转换路径\n",
    "        common_transitions = tf.reduce_max(prob_matrix, axis=-1)\n",
    "        transitions.append(common_transitions)\n",
    "        \n",
    "        # 形态稳定性(自我转换概率)\n",
    "        stability = tf.linalg.diag_part(prob_matrix)\n",
    "        transitions.append(stability)\n",
    "        \n",
    "        return tf.concat(transitions, axis=-1) \n",
    "\n",
    "    def parallel_training(self, data):\n",
    "        \"\"\"补全并行训练逻辑\"\"\"\n",
    "        try:\n",
    "            # 根据当前线程数调整\n",
    "            with ThreadPoolExecutor(max_workers=config_instance.SYSTEM_CONFIG['max_threads']) as executor:\n",
    "                # 动态分配任务\n",
    "                chunk_size = len(data) // self.threads\n",
    "                futures = []\n",
    "                for i in range(self.threads):\n",
    "                    chunk = data[i*chunk_size : (i+1)*chunk_size]\n",
    "                    futures.append(executor.submit(self._train_chunk, chunk))\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"训练失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _forward_pass(self, inputs):\n",
    "        \"\"\"实现多模型并行前向传播\"\"\"\n",
    "        # 使用线程池并行执行\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(model, inputs) for model in self.models]\n",
    "        return [f.result() for f in futures]\n",
    "\n",
    "    def _calculate_loss(self, preds, targets):\n",
    "        \"\"\"计算集成损失\"\"\"\n",
    "        # 加权平均各模型损失\n",
    "        losses = [self.enhanced_match_loss(p, targets) for p in preds]\n",
    "        return sum(w*l for w, l in zip(self.weights, losses))\n",
    "\n",
    "    def _backward_pass(self, loss):\n",
    "        \"\"\"实现多模型并行反向传播\"\"\"\n",
    "        # 使用线程池并行执行\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(self._backward_single_model, loss) for _ in range(len(self.models))]\n",
    "        for f in futures:\n",
    "            f.result()\n",
    "\n",
    "    def _update_parameters(self):\n",
    "        \"\"\"实现多模型并行参数更新\"\"\"\n",
    "        # 使用线程池并行执行\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(self._update_single_model) for _ in range(len(self.models))]\n",
    "        for f in futures:\n",
    "            f.result()\n",
    "\n",
    "    def _init_training(self, data):\n",
    "        \"\"\"实现训练初始化逻辑\"\"\"\n",
    "        # 实现训练初始化逻辑\n",
    "        pass  # 临时实现，需要根据实际需求实现\n",
    "\n",
    "    def _backward_single_model(self, loss):\n",
    "        \"\"\"实现单个模型反向传播逻辑\"\"\"\n",
    "        # 实现单个模型反向传播逻辑\n",
    "        pass  # 临时实现，需要根据实际需求实现\n",
    "\n",
    "    def _update_single_model(self):\n",
    "        \"\"\"实现单个模型参数更新逻辑\"\"\"\n",
    "        # 实现单个模型参数更新逻辑\n",
    "        pass  # 临时实现，需要根据实际需求实现\n",
    "\n",
    "    def _save_final_issue(self, final_issue):\n",
    "        \"\"\"保存最终期号\"\"\"\n",
    "        # 由最后一个完成训练的模型执行\n",
    "        if self.finished_models == self.total_models:\n",
    "            with open('D:\\JupyterWork\\comparison\\issue_number.txt', 'a') as f:\n",
    "                f.write(f\"\\n{final_issue}\")\n",
    "\n",
    "    def save_model(self, model_idx):\n",
    "        model = self.models[model_idx]\n",
    "        model.save(os.path.join(config_instance.MODEL_DIR, f'model_{model_idx}')) \n",
    "\n",
    "    def save_ensemble_info(self):\n",
    "        with open('ensemble_info.json', 'w') as f:\n",
    "            json.dump({\n",
    "                'weights': self.weights.tolist(),\n",
    "                'performance': self.performance_history\n",
    "            }, f) \n",
    "\n",
    "    def load_pretrained_models(self, model_paths):\n",
    "        for path in model_paths:\n",
    "            # 添加device参数适配CPU\n",
    "            model = torch.load(path, map_location=torch.device('cpu'))  \n",
    "            self.models.append(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a2d2a33-0b58-47f5-8b89-296b30f90a96",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#12 数据优化策略\\data_optimizer.py\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataOptimizer:\n",
    "    def _evaluate_distribution(self, X):\n",
    "        \"\"\"评估数据分布情况\"\"\"\n",
    "        try:\n",
    "            # 1. 检查数据偏度\n",
    "            skewness = np.abs(np.mean([np.abs(stats.skew(X[:, i])) for i in range(X.shape[1])]))\n",
    "            skewness_score = 1 / (1 + skewness)  # 转换为0-1分数\n",
    "            \n",
    "            # 2. 检查数据峰度\n",
    "            kurtosis = np.abs(np.mean([np.abs(stats.kurtosis(X[:, i])) for i in range(X.shape[1])]))\n",
    "            kurtosis_score = 1 / (1 + kurtosis)  # 转换为0-1分数\n",
    "            \n",
    "            # 3. 检查异常值比例\n",
    "            z_scores = np.abs(stats.zscore(X))\n",
    "            outlier_ratio = np.mean(z_scores > 3)  # 3个标准差以外视为异常值\n",
    "            outlier_score = 1 - outlier_ratio\n",
    "            \n",
    "            # 计算加权平均分数\n",
    "            distribution_score = (\n",
    "                0.4 * skewness_score +\n",
    "                0.3 * kurtosis_score +\n",
    "                0.3 * outlier_score\n",
    "            )\n",
    "            \n",
    "            return distribution_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"评估数据分布时出错: {str(e)}\")\n",
    "            return 0.0\n",
    "\n",
    "    def _evaluate_correlation(self, X):\n",
    "        \"\"\"评估特征相关性\"\"\"\n",
    "        try:\n",
    "            # 1. 计算特征间相关系数矩阵\n",
    "            corr_matrix = np.corrcoef(X.T)\n",
    "            \n",
    "            # 2. 计算特征间的平均相关性\n",
    "            # 去除对角线上的1\n",
    "            mask = ~np.eye(corr_matrix.shape[0], dtype=bool)\n",
    "            avg_correlation = np.mean(np.abs(corr_matrix[mask]))\n",
    "            \n",
    "            # 3. 计算相关性得分\n",
    "            correlation_score = 1 - avg_correlation  # 越小越好\n",
    "            \n",
    "            return correlation_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"评估特征相关性时出错: {str(e)}\")\n",
    "            return 0.0\n",
    "\n",
    "    def _evaluate_time_series(self, X):\n",
    "        \"\"\"评估时间序列特性\"\"\"\n",
    "        try:\n",
    "            # 1. 检查平稳性\n",
    "            stationarity_scores = []\n",
    "            for i in range(X.shape[1]):\n",
    "                # 使用ADF测试检查平稳性\n",
    "                adf_result = adfuller(X[:, i])[1]  # 获取p值\n",
    "                stationarity_scores.append(1 - min(adf_result, 1))  # 转换为0-1分数\n",
    "            \n",
    "            stationarity_score = np.mean(stationarity_scores)\n",
    "            \n",
    "            # 2. 检查自相关性\n",
    "            autocorr_scores = []\n",
    "            for i in range(X.shape[1]):\n",
    "                # 计算滞后1期的自相关系数\n",
    "                autocorr = np.corrcoef(X[1:, i], X[:-1, i])[0, 1]\n",
    "                autocorr_scores.append(abs(autocorr))\n",
    "            \n",
    "            autocorr_score = np.mean(autocorr_scores)\n",
    "            \n",
    "            # 3. 检查趋势性\n",
    "            trend_scores = []\n",
    "            for i in range(X.shape[1]):\n",
    "                # 使用简单线性回归检测趋势\n",
    "                slope = np.polyfit(np.arange(len(X)), X[:, i], 1)[0]\n",
    "                trend_scores.append(abs(slope))\n",
    "            \n",
    "            trend_score = 1 / (1 + np.mean(trend_scores))  # 转换为0-1分数\n",
    "            \n",
    "            # 计算加权平均分数\n",
    "            time_series_score = (\n",
    "                0.4 * stationarity_score +\n",
    "                0.3 * autocorr_score +\n",
    "                0.3 * trend_score\n",
    "            )\n",
    "            \n",
    "            return time_series_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"评估时间序列特性时出错: {str(e)}\")\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd8898a1-43c3-4845-b16b-4987106ac224",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-14 20:34:40,590] Using an existing study with name 'model_optim_v1' instead of creating a new one.\n"
     ]
    }
   ],
   "source": [
    "#13 模型参数优化\\model_optimizer.py\n",
    "import numpy as np\n",
    "from bayes_opt import BayesianOptimization  # 修正包名导入方式\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.util import load_logs\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "import bayes_opt\n",
    "import optuna\n",
    "import importlib\n",
    "from bayes_opt.logger import Events  # 正确位置\n",
    "\n",
    "# 获取logger实例\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 修改版本检查方式\n",
    "try:\n",
    "    from bayes_opt import __version__ as bo_version\n",
    "except ImportError:\n",
    "    bo_version = \"1.2.0\"  # 默认兼容旧版本\n",
    "\n",
    "if version.parse(bo_version) >= version.parse(\"1.3\"):\n",
    "    from bayes_opt import Events\n",
    "else:\n",
    "    from bayes_opt import Events  # 统一导入方式\n",
    "\n",
    "# 直接导入新版\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# 性能优化配置\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'  # 启用OneDNN优化\n",
    "os.environ['OMP_NUM_THREADS'] = '4'        # 设置OpenMP线程数\n",
    "\n",
    "class ModelOptimizer:\n",
    "    def __init__(self, model_ensemble, data_processor):\n",
    "        self.model_ensemble = model_ensemble\n",
    "        self.data_processor = data_processor\n",
    "        self.best_params = None\n",
    "        self.optimization_history = []\n",
    "        \n",
    "        # 定义参数范围\n",
    "        self.param_ranges = {\n",
    "            # LSTM相关参数\n",
    "            'lstm_params': {\n",
    "                'units': (64, 512),\n",
    "                'layers': (1, 4),\n",
    "                'dropout': (0.1, 0.5),\n",
    "                'recurrent_dropout': (0.0, 0.3)\n",
    "            },\n",
    "            \n",
    "            # CNN相关参数\n",
    "            'cnn_params': {\n",
    "                'filters': (32, 256),\n",
    "                'kernel_size': (2, 5),\n",
    "                'pool_size': (2, 4),\n",
    "                'conv_layers': (1, 3)\n",
    "            },\n",
    "            \n",
    "            # Transformer相关参数\n",
    "            'transformer_params': {\n",
    "                'heads': (4, 16),\n",
    "                'd_model': (128, 512),\n",
    "                'dff': (256, 1024),\n",
    "                'num_layers': (2, 6)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 定义验证规则\n",
    "        self.validation_rules = {\n",
    "            'units': lambda x: isinstance(x, int) and x > 0,\n",
    "            'layers': lambda x: isinstance(x, int) and x > 0,\n",
    "            'dropout': lambda x: 0 <= x <= 1,\n",
    "            'heads': lambda x: isinstance(x, int) and x > 0\n",
    "        }\n",
    "    \n",
    "    def optimize(self, n_iter=50):\n",
    "        \"\"\"运行优化过程\"\"\"\n",
    "        try:\n",
    "            optimizer = BayesianOptimization(\n",
    "                f=self.objective_function,\n",
    "                pbounds=self._flatten_param_ranges(),\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            # 注册回调函数\n",
    "            optimizer.subscribe(\n",
    "                event=Events.OPTIMIZATION_STEP,\n",
    "                subscriber=self._optimization_callback\n",
    "            )\n",
    "            \n",
    "            optimizer.maximize(\n",
    "                init_points=5,\n",
    "                n_iter=n_iter\n",
    "            )\n",
    "            \n",
    "            self.best_params = self._process_params(optimizer.max['params'])\n",
    "            self._save_best_params()\n",
    "            \n",
    "            return self.best_params\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"运行优化过程时出错: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def objective_function(self, **params):\n",
    "        \"\"\"优化目标函数\"\"\"\n",
    "        try:\n",
    "            # 验证参数\n",
    "            is_valid, message = self.validate_params(params)\n",
    "            if not is_valid:\n",
    "                logger.warning(message)\n",
    "                return float('-inf')\n",
    "            \n",
    "            # 更新模型架构\n",
    "            self._update_model_architecture(params)\n",
    "            \n",
    "            # 评估性能\n",
    "            score = self._evaluate_model_performance()\n",
    "            \n",
    "            # 记录历史\n",
    "            self.optimization_history.append({\n",
    "                'params': params,\n",
    "                'score': score\n",
    "            })\n",
    "            \n",
    "            return score\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"优化目标函数执行出错: {str(e)}\")\n",
    "            return float('-inf')\n",
    "    \n",
    "    def validate_params(self, params):\n",
    "        \"\"\"验证参数有效性\"\"\"\n",
    "        try:\n",
    "            for param_name, value in params.items():\n",
    "                for rule_name, rule_func in self.validation_rules.items():\n",
    "                    if rule_name in param_name and not rule_func(value):\n",
    "                        return False, f\"{param_name}参数验证失败\"\n",
    "            return True, \"参数验证通过\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return False, f\"参数验证出错: {str(e)}\"\n",
    "    \n",
    "    def _update_model_architecture(self, params):\n",
    "        \"\"\"更新模型架构\"\"\"\n",
    "        try:\n",
    "            nested_params = self._process_params(params)\n",
    "            self.model_ensemble.update_model_architecture(nested_params)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"更新模型架构时出错: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _evaluate_model_performance(self):\n",
    "        \"\"\"评估模型性能\"\"\"\n",
    "        try:\n",
    "            X_val, y_val = self.data_processor.get_validation_data()\n",
    "            predictions = self.model_ensemble.get_ensemble_prediction(X_val)\n",
    "            \n",
    "            # 使用匹配率作为评估指标\n",
    "            matches = np.any(np.round(predictions) == y_val, axis=1)\n",
    "            accuracy = np.mean(matches)\n",
    "            \n",
    "            return accuracy\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"评估模型性能时出错: {str(e)}\")\n",
    "            return float('-inf')\n",
    "    \n",
    "    def _save_best_params(self):\n",
    "        \"\"\"保存最佳参数\"\"\"\n",
    "        try:\n",
    "            save_path = os.path.join(BASE_SAVE_DIR, 'best_model_params.json')\n",
    "            with open(save_path, 'w') as f:\n",
    "                json.dump(self.best_params, f, indent=4)\n",
    "            logger.info(f\"最佳模型参数已保存到: {save_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"保存最佳参数时出错: {str(e)}\")\n",
    "\n",
    "    def _flatten_param_ranges(self):\n",
    "        \"\"\"将嵌套参数范围展平\"\"\"\n",
    "        flat_ranges = {}\n",
    "        for model_type, params in self.param_ranges.items():\n",
    "            for param_name, param_range in params.items():\n",
    "                flat_name = f\"{model_type}__{param_name}\"\n",
    "                flat_ranges[flat_name] = param_range\n",
    "        return flat_ranges\n",
    "    \n",
    "    def _process_params(self, flat_params):\n",
    "        \"\"\"将展平的参数重构为嵌套结构\"\"\"\n",
    "        nested_params = {}\n",
    "        for flat_name, value in flat_params.items():\n",
    "            model_type, param_name = flat_name.split('__')\n",
    "            if model_type not in nested_params:\n",
    "                nested_params[model_type] = {}\n",
    "            nested_params[model_type][param_name] = value\n",
    "        return nested_params\n",
    "\n",
    "    def _optimization_callback(self, event, instance):\n",
    "        \"\"\"优化进度回调\"\"\"\n",
    "        try:\n",
    "            iteration = len(instance.res)\n",
    "            current_best = instance.max['target']\n",
    "            current_params = instance.max['params']\n",
    "            \n",
    "            # 记录优化进度\n",
    "            logger.info(\n",
    "                f\"优化进度: 第{iteration}次迭代\\n\"\n",
    "                f\"当前最佳分数: {current_best:.4f}\\n\"\n",
    "                f\"参数: {json.dumps(current_params, indent=2)}\"\n",
    "            )\n",
    "            \n",
    "            # 保存阶段性结果\n",
    "            if iteration % 10 == 0:  # 每10次迭代保存一次\n",
    "                self._save_checkpoint(iteration, current_best, current_params)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"回调函数执行出错: {str(e)}\")\n",
    "\n",
    "    def _save_checkpoint(self, iteration, score, params):\n",
    "        \"\"\"保存优化检查点\"\"\"\n",
    "        try:\n",
    "            checkpoint = {\n",
    "                'iteration': iteration,\n",
    "                'best_score': score,\n",
    "                'best_params': params,\n",
    "                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "            \n",
    "            save_path = os.path.join(\n",
    "                BASE_SAVE_DIR, \n",
    "                f'model_opt_checkpoint_{iteration}.json'\n",
    "            )\n",
    "            \n",
    "            with open(save_path, 'w') as f:\n",
    "                json.dump(checkpoint, f, indent=4)\n",
    "            \n",
    "            logger.info(f\"保存优化检查点: {save_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"保存检查点时出错: {str(e)}\")\n",
    "\n",
    "    def optimize_model_params(self, training_direction):\n",
    "        \"\"\"根据训练方向优化模型参数\"\"\"\n",
    "        try:\n",
    "            if isinstance(training_direction, dict):\n",
    "                # 1. 学习率调整\n",
    "                if training_direction['learning_rate'] == 'INCREASE':\n",
    "                    self.current_lr *= 1.5\n",
    "                elif training_direction['learning_rate'] == 'DECREASE':\n",
    "                    self.current_lr *= 0.7\n",
    "                \n",
    "                # 2. 批次大小调整\n",
    "                if training_direction['batch_size'] == 'DECREASE':\n",
    "                    self.batch_size = max(16, self.batch_size // 2)\n",
    "                \n",
    "                # 3. 模型复杂度调整\n",
    "                if training_direction['model_complexity'] == 'INCREASE':\n",
    "                    self.increase_model_complexity()\n",
    "                \n",
    "                # 4. 正则化调整\n",
    "                if training_direction.get('regularization') == 'INCREASE':\n",
    "                    self.increase_regularization()\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"优化模型参数时出错: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def on_train_end(self):\n",
    "        new_params = self.bayesian_optimizer.suggest()\n",
    "        self.model_ensemble.update_params(new_params)  # 更新集成模型参数\n",
    "        self._save_optimization_record()  # 保存优化轨迹\n",
    "\n",
    "    def adjust_after_sample(self, model, sample, current_params):\n",
    "        # 使用sample['input']，但data_manager未生成该字段\n",
    "        # 需要与data_manager的数据格式对齐\n",
    "        # 基于样本梯度调整\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(sample['input'])\n",
    "            loss = tf.keras.losses.MSE(sample['target'], predictions)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        # 生成参数调整建议\n",
    "        adjusted_params = {\n",
    "            'learning_rate': self._adjust_learning_rate(grads, current_params),\n",
    "            'batch_size': current_params['batch_size']  # 保持原批次大小\n",
    "        }\n",
    "        return adjusted_params\n",
    "\n",
    "# 创建全局实例\n",
    "model_optimizer = ModelOptimizer(model_ensemble=None, data_processor=None)\n",
    "\n",
    "# 使用Optuna替代\n",
    "study = optuna.create_study(\n",
    "    study_name=\"model_optim_v1\",\n",
    "    storage=\"sqlite:///optuna.db\",\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "# 修改后的导入部分\n",
    "try:\n",
    "    from bayes_opt import BayesianOptimization\n",
    "    from bayes_opt.logger import JSONLogger\n",
    "    from bayes_opt.util import load_logs\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        \"请安装最新版贝叶斯优化库：\\n\"\n",
    "        \"pip install bayesian-optimization\\n\"\n",
    "        \"或使用conda：\\n\"\n",
    "        \"conda install -c conda-forge bayesian-optimization\"\n",
    "    ) from e "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "409ca799-3302-4564-a6f5-28292f2e70af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 20:34:40,627 [INFO] __main__ - 动态优化器初始化完成\n"
     ]
    }
   ],
   "source": [
    "#14 动态调整策略\\dynamic_optimizer.py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import Events\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# 获取logger实例\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DynamicOptimizer:\n",
    "    \"\"\"动态参数优化器类\"\"\"\n",
    "    \n",
    "    def __init__(self, model_ensemble, performance_monitor):\n",
    "        \"\"\"\n",
    "        初始化动态优化器\n",
    "        Args:\n",
    "            model_ensemble: 模型集成实例\n",
    "            performance_monitor: 性能监控器实例\n",
    "        \"\"\"\n",
    "        self.model_ensemble = model_ensemble\n",
    "        self.performance_monitor = performance_monitor\n",
    "        self.optimization_history = []\n",
    "        \n",
    "        # 定义参数范围\n",
    "        self.param_ranges = {\n",
    "            'learning_rate': (1e-5, 1e-2),\n",
    "            'batch_size': (16, 128),\n",
    "            'dropout_rate': (0.1, 0.5),\n",
    "            'weight_decay': (1e-6, 1e-3)\n",
    "        }\n",
    "        \n",
    "        # 定义调整阈值\n",
    "        self.adjustment_thresholds = {\n",
    "            'performance_drop': 0.1,  # 性能下降阈值\n",
    "            'loss_spike': 0.5,       # 损失突增阈值\n",
    "            'gradient_norm': 10.0     # 梯度范数阈值\n",
    "        }\n",
    "        \n",
    "        logger.info(\"动态优化器初始化完成\")\n",
    "\n",
    "    def optimize(self, current_metrics):\n",
    "        \"\"\"\n",
    "        根据当前指标优化参数\n",
    "        Args:\n",
    "            current_metrics: 当前性能指标\n",
    "        Returns:\n",
    "            dict: 优化后的参数\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 检查是否需要调整\n",
    "            if not self._needs_adjustment(current_metrics):\n",
    "                return None\n",
    "                \n",
    "            # 获取优化建议\n",
    "            suggestions = self._get_optimization_suggestions(current_metrics)\n",
    "            \n",
    "            # 应用参数调整\n",
    "            new_params = self._apply_adjustments(suggestions)\n",
    "            \n",
    "            # 记录优化历史\n",
    "            self.optimization_history.append({\n",
    "                'metrics': current_metrics,\n",
    "                'suggestions': suggestions,\n",
    "                'new_params': new_params\n",
    "            })\n",
    "            \n",
    "            return new_params\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"参数优化过程出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _needs_adjustment(self, metrics):\n",
    "        \"\"\"\n",
    "        判断是否需要调整参数\n",
    "        Args:\n",
    "            metrics: 当前性能指标\n",
    "        Returns:\n",
    "            bool: 是否需要调整\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 检查性能下降\n",
    "            if metrics['performance_change'] < -self.adjustment_thresholds['performance_drop']:\n",
    "                return True\n",
    "                \n",
    "            # 检查损失突增\n",
    "            if metrics['loss_change'] > self.adjustment_thresholds['loss_spike']:\n",
    "                return True\n",
    "                \n",
    "            # 检查梯度不稳定\n",
    "            if metrics['gradient_norm'] > self.adjustment_thresholds['gradient_norm']:\n",
    "                return True\n",
    "                \n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"检查参数调整需求时出错: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _get_optimization_suggestions(self, metrics):\n",
    "        \"\"\"\n",
    "        获取参数优化建议\n",
    "        Args:\n",
    "            metrics: 当前性能指标\n",
    "        Returns:\n",
    "            dict: 参数调整建议\n",
    "        \"\"\"\n",
    "        try:\n",
    "            suggestions = {}\n",
    "            \n",
    "            # 根据性能变化调整学习率\n",
    "            if metrics['performance_change'] < 0:\n",
    "                suggestions['learning_rate'] = self._adjust_learning_rate(\n",
    "                    metrics['current_lr'],\n",
    "                    metrics['performance_change']\n",
    "                )\n",
    "            \n",
    "            # 根据内存使用调整批次大小\n",
    "            if metrics['memory_usage'] > 0.9:  # 内存使用超过90%\n",
    "                suggestions['batch_size'] = self._adjust_batch_size(\n",
    "                    metrics['current_batch_size']\n",
    "                )\n",
    "            \n",
    "            # 根据过拟合风险调整dropout\n",
    "            if metrics['validation_loss'] > metrics['training_loss'] * 1.2:\n",
    "                suggestions['dropout_rate'] = self._adjust_dropout_rate(\n",
    "                    metrics['current_dropout']\n",
    "                )\n",
    "            \n",
    "            return suggestions\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"生成优化建议时出错: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "    def _apply_adjustments(self, suggestions):\n",
    "        \"\"\"\n",
    "        应用参数调整\n",
    "        Args:\n",
    "            suggestions: 参数调整建议\n",
    "        Returns:\n",
    "            dict: 调整后的参数\n",
    "        \"\"\"\n",
    "        try:\n",
    "            new_params = {}\n",
    "            \n",
    "            for param_name, new_value in suggestions.items():\n",
    "                # 验证参数范围\n",
    "                if param_name in self.param_ranges:\n",
    "                    min_val, max_val = self.param_ranges[param_name]\n",
    "                    new_value = np.clip(new_value, min_val, max_val)\n",
    "                    new_params[param_name] = new_value\n",
    "                    \n",
    "            return new_params\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"应用参数调整时出错: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "    def _adjust_learning_rate(self, current_lr, performance_change):\n",
    "        \"\"\"调整学习率\"\"\"\n",
    "        if performance_change < -0.2:  # 性能显著下降\n",
    "            return current_lr * 0.5\n",
    "        elif performance_change < -0.1:  # 性能轻微下降\n",
    "            return current_lr * 0.8\n",
    "        return current_lr\n",
    "\n",
    "    def _adjust_batch_size(self, current_batch_size):\n",
    "        \"\"\"调整批次大小\"\"\"\n",
    "        return max(16, current_batch_size // 2)  # 减小批次大小，但不小于16\n",
    "\n",
    "    def _adjust_dropout_rate(self, current_dropout):\n",
    "        \"\"\"调整dropout率\"\"\"\n",
    "        return min(0.5, current_dropout + 0.1)  # 增加dropout，但不超过0.5\n",
    "\n",
    "    def optimize_parameters(self):\n",
    "        self.bayesian_optimization()  # 贝叶斯优化初始化基础参数\n",
    "        logger.info(\"初始参数设置完成：%s\", self.best_params)\n",
    "\n",
    "    def save_best_params(self):\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"dynamic_params_{timestamp}.json\"\n",
    "        with open(os.path.join(config_manager.MODEL_DIR, filename), 'w') as f:\n",
    "            json.dump(self.best_params, f)\n",
    "\n",
    "# 创建全局实例\n",
    "dynamic_optimizer = DynamicOptimizer(model_ensemble=None, performance_monitor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f42b319-84a3-419a-94dd-1a774bfee846",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#15 集成优化控制\\ensemble_optimizer.py\n",
    "import numpy as np\n",
    "from bayes_opt.logger import Events\n",
    "from bayes_opt import BayesianOptimization\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# 获取logger实例\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EnsembleOptimizer:\n",
    "    \"\"\"集成学习超参数优化器\"\"\"\n",
    "    def __init__(self, model_ensemble, data_processor):\n",
    "        self.model_ensemble = model_ensemble\n",
    "        self.data_processor = data_processor\n",
    "        self.best_params = None\n",
    "        self.optimization_history = []\n",
    "        \n",
    "        # 定义参数范围\n",
    "        self.param_ranges = {\n",
    "            'weight_params': {\n",
    "                'initial_weights': (0.1, 0.3),\n",
    "                'min_weight': (0.05, 0.2),\n",
    "                'max_weight': (0.8, 0.95),\n",
    "                'weight_smoothing': (0.1, 0.5)\n",
    "            },\n",
    "            'diversity_params': {\n",
    "                'correlation_threshold': (0.5, 0.9),\n",
    "                'diversity_weight': (0.1, 0.5),\n",
    "                'agreement_threshold': (0.7, 0.9)\n",
    "            },\n",
    "            'adaptation_params': {\n",
    "                'adaptation_rate': (0.1, 0.5),\n",
    "                'performance_window': (100, 1000),\n",
    "                'min_improvement': (0.001, 0.01)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _flatten_param_ranges(self):\n",
    "        \"\"\"将嵌套的参数范围展平为一维字典\"\"\"\n",
    "        flat_ranges = {}\n",
    "        for category, params in self.param_ranges.items():\n",
    "            for param_name, param_range in params.items():\n",
    "                flat_name = f\"{category}__{param_name}\"\n",
    "                flat_ranges[flat_name] = param_range\n",
    "        return flat_ranges\n",
    "    \n",
    "    def _process_params(self, flat_params):\n",
    "        \"\"\"将一维参数重构为嵌套结构\"\"\"\n",
    "        nested_params = {}\n",
    "        for flat_name, value in flat_params.items():\n",
    "            category, param_name = flat_name.split('__')\n",
    "            if category not in nested_params:\n",
    "                nested_params[category] = {}\n",
    "            nested_params[category][param_name] = value\n",
    "        return nested_params\n",
    "\n",
    "    def objective_function(self, **params):\n",
    "        \"\"\"优化目标函数\"\"\"\n",
    "        try:\n",
    "            # 重构参数\n",
    "            nested_params = self._process_params(params)\n",
    "            \n",
    "            # 更新集成参数\n",
    "            self._update_ensemble_params(nested_params)\n",
    "            \n",
    "            # 获取验证数据\n",
    "            X_val, y_val = self.data_processor.get_validation_data()\n",
    "            if X_val is None or y_val is None:\n",
    "                return float('-inf')\n",
    "            \n",
    "            # 评估集成性能\n",
    "            ensemble_score = self._evaluate_ensemble(X_val, y_val)\n",
    "            diversity_score = self._calculate_diversity()\n",
    "            \n",
    "            # 综合得分 (考虑性能和多样性)\n",
    "            final_score = (0.7 * ensemble_score + \n",
    "                         0.3 * diversity_score)\n",
    "            \n",
    "            # 记录历史\n",
    "            self.optimization_history.append({\n",
    "                'params': nested_params,\n",
    "                'ensemble_score': ensemble_score,\n",
    "                'diversity_score': diversity_score,\n",
    "                'final_score': final_score\n",
    "            })\n",
    "            \n",
    "            return final_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"优化目标函数执行出错: {str(e)}\")\n",
    "            return float('-inf')\n",
    "    \n",
    "    def _evaluate_ensemble(self, X, y):\n",
    "        \"\"\"评估集成模型性能\"\"\"\n",
    "        try:\n",
    "            predictions = self.model_ensemble.predict(X)\n",
    "            matches = np.any(np.round(predictions) == y, axis=1)\n",
    "            return np.mean(matches)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"评估集成性能时出错: {str(e)}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def _calculate_diversity(self):\n",
    "        \"\"\"计算模型多样性得分\"\"\"\n",
    "        try:\n",
    "            # 获取所有模型的预测\n",
    "            predictions = []\n",
    "            X_val, _ = self.data_processor.get_validation_data()\n",
    "            \n",
    "            for model in self.model_ensemble.models:\n",
    "                pred = model.predict(X_val)\n",
    "                predictions.append(pred)\n",
    "            \n",
    "            # 计算模型间的互信息\n",
    "            diversity_scores = []\n",
    "            n_models = len(predictions)\n",
    "            \n",
    "            for i in range(n_models):\n",
    "                for j in range(i + 1, n_models):\n",
    "                    mi_score = mutual_info_score(\n",
    "                        predictions[i].ravel(),\n",
    "                        predictions[j].ravel()\n",
    "                    )\n",
    "                    diversity_scores.append(1 - mi_score)  # 转换为多样性分数\n",
    "            \n",
    "            return np.mean(diversity_scores)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"计算多样性得分时出错: {str(e)}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def optimize(self, n_iter=50):\n",
    "        \"\"\"运行优化过程\"\"\"\n",
    "        try:\n",
    "            optimizer = BayesianOptimization(\n",
    "                f=self.objective_function,\n",
    "                pbounds=self._flatten_param_ranges(),\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            # 定义优化回调\n",
    "            def optimization_callback(res):\n",
    "                progress = self._check_optimization_progress()\n",
    "                if progress and not progress['is_improving'] and progress['is_stable']:\n",
    "                    logger.info(\"优化进入稳定阶段，但仍将继续探索\")\n",
    "            \n",
    "            optimizer.subscribe(Events.OPTIMIZATION_STEP, optimization_callback)\n",
    "            optimizer.maximize(\n",
    "                init_points=5,\n",
    "                n_iter=n_iter\n",
    "            )\n",
    "            \n",
    "            self.best_params = self._process_params(optimizer.max['params'])\n",
    "            self._save_best_params()\n",
    "            \n",
    "            # 最终优化结果总结\n",
    "            final_progress = self._check_optimization_progress()\n",
    "            if final_progress:\n",
    "                logger.info(\"优化完成总结:\")\n",
    "                logger.info(f\"最终得分: {final_progress['current_score']:.4f}\")\n",
    "                logger.info(f\"最佳得分: {final_progress['best_score']:.4f}\")\n",
    "                logger.info(f\"优化稳定性: {final_progress['std_score']:.4f}\")\n",
    "            \n",
    "            return self.best_params\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"运行优化过程时出错: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _check_optimization_progress(self):\n",
    "        \"\"\"检查优化进度和效果\"\"\"\n",
    "        try:\n",
    "            if len(self.optimization_history) < 5:\n",
    "                return\n",
    "            \n",
    "            # 获取最近5次和历史最佳得分\n",
    "            recent_scores = [h['final_score'] for h in self.optimization_history[-5:]]\n",
    "            best_score = max(h['final_score'] for h in self.optimization_history)\n",
    "            current_score = recent_scores[-1]\n",
    "            \n",
    "            # 计算最近5次的统计信息\n",
    "            mean_score = np.mean(recent_scores)\n",
    "            std_score = np.std(recent_scores)\n",
    "            \n",
    "            # 判断优化趋势\n",
    "            is_improving = current_score > mean_score\n",
    "            is_stable = std_score < 0.1  # 根据实际情况调整阈值\n",
    "            \n",
    "            # 记录详细的优化进展\n",
    "            logger.info(\"-\" * 50)\n",
    "            logger.info(\"优化进度检查:\")\n",
    "            logger.info(f\"当前得分: {current_score:.4f}\")\n",
    "            logger.info(f\"历史最佳: {best_score:.4f}\")\n",
    "            logger.info(f\"最近5次得分: {[f'{s:.4f}' for s in recent_scores]}\")\n",
    "            logger.info(f\"最近5次平均: {mean_score:.4f} (标准差: {std_score:.4f})\")\n",
    "            logger.info(f\"优化趋势: {'改善中' if is_improving else '停滞'}\")\n",
    "            logger.info(f\"稳定性: {'稳定' if is_stable else '波动'}\")\n",
    "            logger.info(\"-\" * 50)\n",
    "            \n",
    "            return {\n",
    "                'is_improving': is_improving,\n",
    "                'is_stable': is_stable,\n",
    "                'current_score': current_score,\n",
    "                'best_score': best_score,\n",
    "                'mean_score': mean_score,\n",
    "                'std_score': std_score\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"检查优化进度时出错: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _save_best_params(self):\n",
    "        \"\"\"保存最佳参数\"\"\"\n",
    "        try:\n",
    "            save_path = os.path.join(BASE_SAVE_DIR, 'best_ensemble_params.json')\n",
    "            with open(save_path, 'w') as f:\n",
    "                json.dump(self.best_params, f, indent=4)\n",
    "            logger.info(f\"最佳集成参数已保存到: {save_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"保存最佳参数时出错: {str(e)}\")\n",
    "\n",
    "    def _update_ensemble_params(self, params):\n",
    "        \"\"\"更新集成模型参数\"\"\"\n",
    "        try:\n",
    "            # 更新权重参数\n",
    "            weight_params = params.get('weight_params', {})\n",
    "            self.model_ensemble.update_weights(weight_params)\n",
    "            \n",
    "            # 更新多样性参数\n",
    "            diversity_params = params.get('diversity_params', {})\n",
    "            self.model_ensemble.update_diversity_settings(diversity_params)\n",
    "            \n",
    "            # 更新自适应参数\n",
    "            adaptation_params = params.get('adaptation_params', {})\n",
    "            self.model_ensemble.update_adaptation_settings(adaptation_params)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"更新集成参数时出错: {str(e)}\")\n",
    "\n",
    "    def adjust_ensemble_strategy(self, match_distribution):\n",
    "        \"\"\"调整集成策略\"\"\"\n",
    "        try:\n",
    "            total_samples = sum(match_distribution.values())\n",
    "            \n",
    "            # 1. 分析集成效果\n",
    "            high_match_ratio = (match_distribution[4] + match_distribution[5]) / total_samples\n",
    "            low_match_ratio = (match_distribution[0] + match_distribution[1]) / total_samples\n",
    "            \n",
    "            # 2. 根据分布调整集成策略\n",
    "            if high_match_ratio < 0.1:  # 高匹配率太低\n",
    "                # 增加模型多样性\n",
    "                self.increase_model_diversity()\n",
    "                # 调整模型权重\n",
    "                self.adjust_model_weights()\n",
    "                \n",
    "            elif low_match_ratio > 0.5:  # 低匹配率太高\n",
    "                # 强化表现好的模型\n",
    "                self.strengthen_best_models()\n",
    "                # 重新训练表现差的模型\n",
    "                self.retrain_weak_models()\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"调整集成策略时出错: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "# 创建全局实例\n",
    "ensemble_optimizer = EnsembleOptimizer(model_ensemble=None, data_processor=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dd78381-bc2b-48c9-b104-39520f63ef14",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 20:34:40,721 [INFO] __main__ - 状态管理器初始化完成\n"
     ]
    }
   ],
   "source": [
    "#16 状态管理\\state_manager.py\n",
    "import os\n",
    "import signal\n",
    "import logging\n",
    "import threading\n",
    "from typing import Optional, Any, Dict\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from bayes_opt.logger import Events  # 拆分导入\n",
    "from bayes_opt.util import load_logs\n",
    "\n",
    "# 获取logger实例\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class StateManager:\n",
    "    \"\"\"全局状态管理器 - 单例模式\"\"\"\n",
    "    _instance = None\n",
    "    \n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self):\n",
    "        if not hasattr(self, 'initialized'):\n",
    "            # 训练相关状态\n",
    "            self.trainer = None  # 全局trainer实例\n",
    "            self.training_state = 'idle'  # 训练状态: idle/training/paused/stopped\n",
    "            self.current_epoch = 0\n",
    "            self.current_batch = 0\n",
    "            self.best_performance = float('inf')\n",
    "            \n",
    "            # 显示相关状态\n",
    "            self.display_running = True  # 显示线程运行标志\n",
    "            self.display_thread = None  # 显示线程实例\n",
    "            self.log_buffer = deque(maxlen=100)  # 日志缓冲区\n",
    "            self.show_print = False  # 控制是否显示打印信息\n",
    "            \n",
    "            # 性能监控状态\n",
    "            self.performance_metrics = {\n",
    "                'loss': deque(maxlen=1000),\n",
    "                'accuracy': deque(maxlen=1000),\n",
    "                'learning_rate': 0.001\n",
    "            }\n",
    "            \n",
    "            # 资源监控状态\n",
    "            self.resource_metrics = {\n",
    "                'memory_usage': 0,\n",
    "                'cpu_usage': 0,\n",
    "                'gpu_usage': 0\n",
    "            }\n",
    "            \n",
    "            # 注册信号处理器\n",
    "            self._register_signal_handlers()\n",
    "            \n",
    "            self.initialized = True\n",
    "            logger.info(\"状态管理器初始化完成\")\n",
    "    \n",
    "    def _register_signal_handlers(self):\n",
    "        \"\"\"注册信号处理器\"\"\"\n",
    "        signal.signal(signal.SIGINT, self._signal_handler)   # Ctrl+C\n",
    "        signal.signal(signal.SIGTERM, self._signal_handler)  # 终止信号\n",
    "    \n",
    "    def _signal_handler(self, signum, frame):\n",
    "        \"\"\"信号处理器\"\"\"\n",
    "        logger.info(f\"接收到信号: {signum}, 开始保存进度...\")\n",
    "        self.save_all_progress()\n",
    "        import sys\n",
    "        sys.exit(0)\n",
    "    \n",
    "    def save_all_progress(self):\n",
    "        \"\"\"保存所有进度和参数\"\"\"\n",
    "        if self.trainer:\n",
    "            try:\n",
    "                # 保存训练进度\n",
    "                self.trainer.save_training_progress()\n",
    "                # 保存模型参数\n",
    "                self.trainer.save_model_weights()\n",
    "                # 保存性能指标\n",
    "                self.save_performance_metrics()\n",
    "                logger.info(\"所有进度和参数已保存\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"保存进度时出错: {str(e)}\")\n",
    "    \n",
    "    def update_training_state(self, new_state: str):\n",
    "        \"\"\"更新训练状态\"\"\"\n",
    "        valid_states = {'idle', 'training', 'paused', 'stopped'}\n",
    "        if new_state not in valid_states:\n",
    "            logger.error(f\"无效的训练状态: {new_state}\")\n",
    "            return\n",
    "            \n",
    "        old_state = self.training_state\n",
    "        self.training_state = new_state\n",
    "        logger.info(f\"训练状态从 {old_state} 变更为 {new_state}\")\n",
    "    \n",
    "    def update_performance_metrics(self, metrics: Dict[str, float]):\n",
    "        \"\"\"更新性能指标\"\"\"\n",
    "        try:\n",
    "            for key, value in metrics.items():\n",
    "                if key in self.performance_metrics:\n",
    "                    self.performance_metrics[key].append(value)\n",
    "                    \n",
    "            # 更新最佳性能\n",
    "            if 'loss' in metrics and metrics['loss'] < self.best_performance:\n",
    "                self.best_performance = metrics['loss']\n",
    "                logger.info(f\"更新最佳性能: {self.best_performance:.4f}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"更新性能指标时出错: {str(e)}\")\n",
    "    \n",
    "    def update_resource_metrics(self, metrics: Dict[str, float]):\n",
    "        \"\"\"更新资源使用指标\"\"\"\n",
    "        try:\n",
    "            self.resource_metrics.update(metrics)\n",
    "            # 检查资源使用是否超过警戒线\n",
    "            if metrics.get('memory_usage', 0) > 90:\n",
    "                logger.warning(\"内存使用率超过90%!\")\n",
    "            if metrics.get('gpu_usage', 0) > 90:\n",
    "                logger.warning(\"GPU使用率超过90%!\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"更新资源指标时出错: {str(e)}\")\n",
    "    \n",
    "    def set_trainer(self, trainer: Any):\n",
    "        \"\"\"设置trainer实例\"\"\"\n",
    "        self.trainer = trainer\n",
    "    \n",
    "    def set_display_thread(self, thread: threading.Thread):\n",
    "        \"\"\"设置显示线程\"\"\"\n",
    "        self.display_thread = thread\n",
    "    \n",
    "    def stop_display(self):\n",
    "        \"\"\"停止显示线程\"\"\"\n",
    "        self.display_running = False\n",
    "        if self.display_thread and self.display_thread.is_alive():\n",
    "            self.display_thread.join()\n",
    "            logger.info(\"显示线程已停止\")\n",
    "    \n",
    "    def save_performance_metrics(self):\n",
    "        \"\"\"保存性能指标到文件\"\"\"\n",
    "        try:\n",
    "            metrics_file = os.path.join('logs', f'metrics_{datetime.now():%Y%m%d_%H%M%S}.json')\n",
    "            import json\n",
    "            with open(metrics_file, 'w') as f:\n",
    "                # 将deque转换为list后保存\n",
    "                metrics_to_save = {\n",
    "                    k: list(v) if isinstance(v, deque) else v \n",
    "                    for k, v in self.performance_metrics.items()\n",
    "                }\n",
    "                json.dump(metrics_to_save, f, indent=4)\n",
    "            logger.info(f\"性能指标已保存到: {metrics_file}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"保存性能指标时出错: {str(e)}\")\n",
    "\n",
    "    def save_training_state(self):\n",
    "        \"\"\"保存训练状态\"\"\"\n",
    "        torch.save({\n",
    "            'model_states': [m.get_weights() for m in model_ensemble.models],\n",
    "            'optimizer_states': training_optimizer.get_states()\n",
    "        }, 'training_state.pt')\n",
    "\n",
    "    def restore_training_state(self):\n",
    "        \"\"\"恢复训练状态\"\"\"\n",
    "        if os.path.exists('training_state.pt'):\n",
    "            state = torch.load('training_state.pt')\n",
    "            # 恢复模型参数\n",
    "            for model, weights in zip(model_ensemble.models, state['model_states']):\n",
    "                model.set_weights(weights)\n",
    "            # 恢复优化器状态\n",
    "            training_optimizer.set_states(state['optimizer_states'])\n",
    "\n",
    "# 创建全局实例\n",
    "state_manager = StateManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16d791b2-0215-496d-8ca2-0bc8cdd7441d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 20:34:40,793 [INFO] __main__ - 训练优化器初始化完成\n"
     ]
    }
   ],
   "source": [
    "#17 训练策略优化器\\training_optimizer.py\n",
    "import numpy as np\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import Events\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import psutil\n",
    "\n",
    "# 获取logger实例\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TrainingOptimizer:\n",
    "    \"\"\"训练策略优化器类\"\"\"\n",
    "    \n",
    "    def __init__(self, model_ensemble, data_processor):\n",
    "        \"\"\"\n",
    "        初始化训练优化器\n",
    "        Args:\n",
    "            model_ensemble: 模型集成实例\n",
    "            data_processor: 数据处理器实例\n",
    "        \"\"\"\n",
    "        self.model_ensemble = model_ensemble\n",
    "        self.data_processor = data_processor\n",
    "        self.best_params = None\n",
    "        self.optimization_history = []\n",
    "        self.best_match_history = []  # 记录最佳匹配数历史\n",
    "        self.match_distribution = {i: 0 for i in range(6)}  # 记录各匹配数的分布\n",
    "        \n",
    "        # 定义参数范围\n",
    "        self.param_ranges = {\n",
    "            # 1. 优化器参数\n",
    "            'optimizer_params': {\n",
    "                'learning_rate': (1e-5, 1e-2),\n",
    "                'beta_1': (0.8, 0.999),\n",
    "                'beta_2': (0.8, 0.999),\n",
    "                'epsilon': (1e-8, 1e-6)\n",
    "            },\n",
    "            \n",
    "            # 2. 学习率调度参数\n",
    "            'lr_schedule_params': {\n",
    "                'decay_rate': (0.9, 0.99),\n",
    "                'decay_steps': (100, 1000),\n",
    "                'warmup_steps': (0, 100),\n",
    "                'min_lr': (1e-6, 1e-4)\n",
    "            },\n",
    "            \n",
    "            # 3. 训练控制参数\n",
    "            'training_control': {\n",
    "                'batch_size': (16, 128),\n",
    "                'epochs_per_iteration': (1, 10),\n",
    "                'validation_frequency': (1, 10),\n",
    "                'early_stopping_patience': (10, 50)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 定义离散参数\n",
    "        self.discrete_params = {\n",
    "            'optimizer_type': ['adam', 'adamw', 'radam'],\n",
    "            'scheduler_type': ['exponential', 'cosine', 'step']\n",
    "        }\n",
    "        \n",
    "        logger.info(\"训练优化器初始化完成\")\n",
    "\n",
    "    def optimize(self, n_iter=50):\n",
    "        \"\"\"\n",
    "        运行优化过程\n",
    "        Args:\n",
    "            n_iter: 优化迭代次数\n",
    "        Returns:\n",
    "            dict: 最佳参数\n",
    "        \"\"\"\n",
    "        try:\n",
    "            optimizer = BayesianOptimization(\n",
    "                f=self._objective_function,\n",
    "                pbounds=self._flatten_param_ranges(),\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            # 定义优化回调\n",
    "            def optimization_callback(res):\n",
    "                progress = self._check_optimization_progress()\n",
    "                if progress and not progress['is_improving'] and progress['is_stable']:\n",
    "                    logger.info(\"优化进入稳定阶段，但仍将继续探索\")\n",
    "            \n",
    "            optimizer.subscribe(Events.OPTIMIZATION_STEP, optimization_callback)\n",
    "            \n",
    "            optimizer.maximize(\n",
    "                init_points=5,\n",
    "                n_iter=n_iter\n",
    "            )\n",
    "            \n",
    "            self.best_params = self._process_params(optimizer.max['params'])\n",
    "            self._save_best_params()\n",
    "            \n",
    "            # 最终优化结果总结\n",
    "            final_progress = self._check_optimization_progress()\n",
    "            if final_progress:\n",
    "                logger.info(\"优化完成总结:\")\n",
    "                logger.info(f\"最终得分: {final_progress['current_score']:.4f}\")\n",
    "                logger.info(f\"最佳得分: {final_progress['best_score']:.4f}\")\n",
    "                logger.info(f\"优化稳定性: {final_progress['std_score']:.4f}\")\n",
    "            \n",
    "            return self.best_params\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"运行优化过程时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _objective_function(self, **params):\n",
    "        \"\"\"优化目标函数\"\"\"\n",
    "        try:\n",
    "            # 1. 验证参数\n",
    "            is_valid, message = self._validate_params(params)\n",
    "            if not is_valid:\n",
    "                logger.warning(f\"参数验证失败: {message}\")\n",
    "                return float('-inf')\n",
    "            \n",
    "            # 2. 更新训练参数\n",
    "            self._update_training_params(params)\n",
    "            \n",
    "            # 3. 获取评估数据\n",
    "            X, y = self.data_processor.get_validation_data()\n",
    "            if X is None or y is None:\n",
    "                return float('-inf')\n",
    "            \n",
    "            # 4. 评估性能\n",
    "            score = self._evaluate_performance(X, y)\n",
    "            \n",
    "            # 5. 记录历史\n",
    "            self.optimization_history.append({\n",
    "                'params': params,\n",
    "                'score': score\n",
    "            })\n",
    "            \n",
    "            return score\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"目标函数执行出错: {str(e)}\")\n",
    "            return float('-inf')\n",
    "\n",
    "    def _update_training_params(self, params):\n",
    "        \"\"\"更新训练参数\"\"\"\n",
    "        try:\n",
    "            nested_params = self._process_params(params)\n",
    "            \n",
    "            # 1. 更新优化器参数\n",
    "            optimizer_params = nested_params['optimizer_params']\n",
    "            for model in self.model_ensemble.models:\n",
    "                model.optimizer.learning_rate = optimizer_params['learning_rate']\n",
    "                if hasattr(model.optimizer, 'beta_1'):\n",
    "                    model.optimizer.beta_1 = optimizer_params['beta_1']\n",
    "                if hasattr(model.optimizer, 'beta_2'):\n",
    "                    model.optimizer.beta_2 = optimizer_params['beta_2']\n",
    "            \n",
    "            # 2. 更新学习率调度\n",
    "            lr_params = nested_params['lr_schedule_params']\n",
    "            self.model_ensemble.update_learning_rate_schedule(\n",
    "                decay_rate=lr_params['decay_rate'],\n",
    "                decay_steps=int(lr_params['decay_steps']),\n",
    "                warmup_steps=int(lr_params['warmup_steps']),\n",
    "                min_lr=lr_params['min_lr']\n",
    "            )\n",
    "            \n",
    "            # 3. 更新训练控制参数\n",
    "            training_params = nested_params['training_control']\n",
    "            self.model_ensemble.batch_size = int(training_params['batch_size'])\n",
    "            self.model_ensemble.epochs_per_iteration = int(training_params['epochs_per_iteration'])\n",
    "            self.model_ensemble.validation_frequency = int(training_params['validation_frequency'])\n",
    "            \n",
    "            logger.info(\"训练参数已更新\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"更新训练参数时出错: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _evaluate_performance(self, X, y):\n",
    "        \"\"\"评估性能\"\"\"\n",
    "        try:\n",
    "            # 1. 获取集成预测\n",
    "            predictions = self.model_ensemble.predict(X)\n",
    "            \n",
    "            # 2. 计算匹配率\n",
    "            matches = np.any(np.round(predictions) == y, axis=1)\n",
    "            accuracy = np.mean(matches)\n",
    "            \n",
    "            # 3. 计算其他指标\n",
    "            mse = np.mean((predictions - y) ** 2)\n",
    "            mae = np.mean(np.abs(predictions - y))\n",
    "            \n",
    "            # 4. 计算综合得分\n",
    "            score = (0.6 * accuracy + \n",
    "                    0.2 * (1 / (1 + mse)) + \n",
    "                    0.2 * (1 / (1 + mae)))\n",
    "            \n",
    "            logger.debug(f\"性能评估 - 准确率: {accuracy:.4f}, MSE: {mse:.4f}, MAE: {mae:.4f}\")\n",
    "            return score\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"评估性能时出错: {str(e)}\")\n",
    "            return float('-inf')\n",
    "\n",
    "    def _save_best_params(self):\n",
    "        \"\"\"保存最佳参数\"\"\"\n",
    "        try:\n",
    "            save_path = os.path.join('config', 'best_training_params.json')\n",
    "            os.makedirs('config', exist_ok=True)\n",
    "            \n",
    "            with open(save_path, 'w') as f:\n",
    "                json.dump(self.best_params, f, indent=4)\n",
    "            logger.info(f\"最佳训练参数已保存到: {save_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"保存最佳参数时出错: {str(e)}\")\n",
    "\n",
    "    def _validate_params(self, params):\n",
    "        \"\"\"\n",
    "        验证参数有效性\n",
    "        Args:\n",
    "            params: 待验证的参数\n",
    "        Returns:\n",
    "            tuple: (是否有效, 错误信息)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            nested_params = self._process_params(params)\n",
    "            \n",
    "            # 验证优化器参数\n",
    "            optimizer_params = nested_params['optimizer_params']\n",
    "            assert 0 < optimizer_params['learning_rate'] < 1\n",
    "            assert 0 < optimizer_params['beta_1'] < 1\n",
    "            assert 0 < optimizer_params['beta_2'] < 1\n",
    "            \n",
    "            # 验证学习率调度参数\n",
    "            lr_params = nested_params['lr_schedule_params']\n",
    "            assert 0 < lr_params['decay_rate'] < 1\n",
    "            assert lr_params['decay_steps'] > 0\n",
    "            \n",
    "            # 验证训练控制参数\n",
    "            training_params = nested_params['training_control']\n",
    "            assert training_params['batch_size'] > 0\n",
    "            assert training_params['epochs_per_iteration'] > 0\n",
    "            \n",
    "            return True, \"参数验证通过\"\n",
    "            \n",
    "        except AssertionError as e:\n",
    "            return False, f\"参数验证失败: {str(e)}\"\n",
    "        except Exception as e:\n",
    "            return False, f\"参数验证出错: {str(e)}\"\n",
    "\n",
    "    def _flatten_param_ranges(self):\n",
    "        \"\"\"\n",
    "        将嵌套的参数范围展平为一维字典\n",
    "        Returns:\n",
    "            dict: 展平后的参数范围\n",
    "        \"\"\"\n",
    "        flat_ranges = {}\n",
    "        for category, params in self.param_ranges.items():\n",
    "            for param_name, param_range in params.items():\n",
    "                flat_ranges[f\"{category}__{param_name}\"] = param_range\n",
    "        return flat_ranges\n",
    "\n",
    "    def _process_params(self, flat_params):\n",
    "        \"\"\"\n",
    "        将一维参数字典重构为嵌套结构\n",
    "        Args:\n",
    "            flat_params: 展平的参数字典\n",
    "        Returns:\n",
    "            dict: 嵌套的参数字典\n",
    "        \"\"\"\n",
    "        nested_params = {\n",
    "            'optimizer_params': {},\n",
    "            'lr_schedule_params': {},\n",
    "            'training_control': {}\n",
    "        }\n",
    "        \n",
    "        for flat_name, value in flat_params.items():\n",
    "            category, param_name = flat_name.split('__')\n",
    "            nested_params[category][param_name] = value\n",
    "            \n",
    "        return nested_params\n",
    "\n",
    "    def _check_optimization_progress(self):\n",
    "        \"\"\"检查优化进度和效果\"\"\"\n",
    "        try:\n",
    "            if len(self.optimization_history) < 5:\n",
    "                return\n",
    "            \n",
    "            # 获取最近5次和历史最佳得分\n",
    "            recent_scores = [h['score'] for h in self.optimization_history[-5:]]\n",
    "            best_score = max(h['score'] for h in self.optimization_history)\n",
    "            current_score = recent_scores[-1]\n",
    "            \n",
    "            # 计算最近5次的统计信息\n",
    "            mean_score = np.mean(recent_scores)\n",
    "            std_score = np.std(recent_scores)\n",
    "            \n",
    "            # 判断优化趋势\n",
    "            is_improving = current_score > mean_score\n",
    "            is_stable = std_score < 0.1  # 根据实际情况调整阈值\n",
    "            \n",
    "            # 记录详细的优化进展\n",
    "            logger.info(\"-\" * 50)\n",
    "            logger.info(\"优化进度检查:\")\n",
    "            logger.info(f\"当前得分: {current_score:.4f}\")\n",
    "            logger.info(f\"历史最佳: {best_score:.4f}\")\n",
    "            logger.info(f\"最近5次得分: {[f'{s:.4f}' for s in recent_scores]}\")\n",
    "            logger.info(f\"最近5次平均: {mean_score:.4f} (标准差: {std_score:.4f})\")\n",
    "            logger.info(f\"优化趋势: {'改善中' if is_improving else '停滞'}\")\n",
    "            logger.info(f\"稳定性: {'稳定' if is_stable else '波动'}\")\n",
    "            logger.info(\"-\" * 50)\n",
    "            \n",
    "            return {\n",
    "                'is_improving': is_improving,\n",
    "                'is_stable': is_stable,\n",
    "                'current_score': current_score,\n",
    "                'best_score': best_score,\n",
    "                'mean_score': mean_score,\n",
    "                'std_score': std_score\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"检查优化进度时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def analyze_training_direction(self, match_counts, current_params):\n",
    "        \"\"\"分析训练方向\"\"\"\n",
    "        try:\n",
    "            # 1. 分析匹配分布\n",
    "            for count in match_counts:\n",
    "                self.match_distribution[count] += 1\n",
    "            \n",
    "            # 2. 判断当前状态\n",
    "            if self.match_distribution[5] > 0:\n",
    "                return \"OPTIMAL\"  # 已达到最优\n",
    "                \n",
    "            avg_match = sum(k * v for k, v in self.match_distribution.items()) / sum(self.match_distribution.values())\n",
    "            \n",
    "            # 3. 根据匹配分布给出调整建议\n",
    "            if avg_match < 2:  # 大部分预测匹配数低于2\n",
    "                return {\n",
    "                    'learning_rate': 'INCREASE',  # 增大学习率\n",
    "                    'batch_size': 'DECREASE',     # 减小批次大小\n",
    "                    'model_complexity': 'INCREASE' # 增加模型复杂度\n",
    "                }\n",
    "            elif avg_match > 3:  # 大部分预测匹配数高于3\n",
    "                return {\n",
    "                    'learning_rate': 'DECREASE',   # 减小学习率\n",
    "                    'regularization': 'INCREASE',  # 增加正则化\n",
    "                    'ensemble_diversity': 'INCREASE' # 增加集成多样性\n",
    "                }\n",
    "            \n",
    "            return \"CONTINUE\"  # 保持当前方向\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"分析训练方向时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def adjust_training_strategy(self):\n",
    "        # 根据当前批次表现调整\n",
    "        if self.performance_monitor.get_recent_accuracy() < 0.5:\n",
    "            self._adjust_learning_rate()\n",
    "\n",
    "    def optimize_training_flow(self):\n",
    "        \"\"\"补全训练流程优化\"\"\"\n",
    "        # 新增功能\n",
    "        self._dynamic_batch_adjust()  # 动态批次调整\n",
    "        self._enable_mixed_precision()  # 混合精度训练\n",
    "        self._setup_checkpoints()  # 检查点配置\n",
    "\n",
    "    def _dynamic_batch_adjust(self):\n",
    "        \"\"\"根据内存使用动态调整批次大小\"\"\"\n",
    "        mem_usage = memory_manager.get_memory_info()\n",
    "        if mem_usage['percent'] > 80:\n",
    "            new_size = max(8, self.batch_size // 2)\n",
    "            logger.info(f\"批次大小从{self.batch_size}调整为{new_size}\")\n",
    "            self.batch_size = new_size\n",
    "\n",
    "    def _enable_mixed_precision(self):\n",
    "        # 当前仅设置标志位\n",
    "        self.mixed_precision = True  # 需添加具体实现\n",
    "\n",
    "    def _dynamic_resource_adjust(self):\n",
    "        \"\"\"根据硬件资源动态调整参数\"\"\"\n",
    "        # 获取实时资源数据\n",
    "        mem_info = memory_manager.get_memory_info()\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        \n",
    "        # 内存调整策略\n",
    "        if mem_info['percent'] > 75:\n",
    "            new_batch = max(4, self.batch_size // 2)\n",
    "            logger.info(f\"内存使用{mem_info['percent']}% → 批次从{self.batch_size}调整为{new_batch}\")\n",
    "            self.batch_size = new_batch\n",
    "        \n",
    "        # CPU线程调整策略\n",
    "        if cpu_usage < 60:\n",
    "            self.threads = min(12, self.threads + 2)  # 最大12线程\n",
    "        else:\n",
    "            self.threads = max(4, self.threads - 2)\n",
    "        \n",
    "        # GPU显存优化\n",
    "        if tf.config.list_physical_devices('GPU'):\n",
    "            gpu_mem = tf.config.experimental.get_memory_info('GPU:0')\n",
    "            used_percent = gpu_mem['current'] / gpu_mem['total']\n",
    "            if used_percent > 0.8:\n",
    "                tf.config.experimental.set_memory_growth(True)\n",
    "\n",
    "# 创建全局实例\n",
    "training_optimizer = TrainingOptimizer(model_ensemble=None, data_processor=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a545e793-2ea9-4d4a-a16f-69517291adf4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 20:34:40,808 [INFO] __main__ - 配置系统测试通过\n"
     ]
    }
   ],
   "source": [
    "#18 配置系统测试用例\\test_config.py\n",
    "from cell1 import config_instance  # 从cell1导入配置实例\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# 添加配置测试用例\n",
    "def test_config_system():\n",
    "    \"\"\"测试配置系统完整性\"\"\"\n",
    "    required_keys = ['host', 'port', 'user', 'password']\n",
    "    assert all(key in config_instance.DB_CONFIG for key in required_keys), \"数据库配置缺失必要参数\"\n",
    "    logger.info(\"配置系统测试通过\")\n",
    "\n",
    "# 原有代码保持不变\n",
    "class ConfigValidator:\n",
    "    def __init__(self):\n",
    "        self.validation_rules = {\n",
    "            'learning_rate': lambda x: 0 < x < 1,\n",
    "            'batch_size': lambda x: x > 0 and x & (x-1) == 0  # 验证是否为2的幂\n",
    "        }\n",
    "    \n",
    "    def validate(self, config: dict) -> bool:\n",
    "        valid = True\n",
    "        for key, rule in self.validation_rules.items():\n",
    "            if key in config:\n",
    "                if not rule(config[key]):\n",
    "                    logger.warning(f\"参数 {key} 的值 {config[key]} 无效\")\n",
    "                    valid = False\n",
    "        return valid\n",
    "\n",
    "# 在模块加载时自动运行测试\n",
    "if __name__ == \"__main__\":\n",
    "    test_config_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baddf552-6305-4320-a785-21534fd7ef48",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 20:34:40,838 [INFO] cell6 - 数据管理器初始化完成\n",
      "2025-02-14 20:34:40,841 [INFO] cell7 - 资源监控器初始化完成\n",
      "2025-02-14 20:34:40,847 [INFO] cell14 - 动态优化器初始化完成\n",
      "2025-02-14 20:34:40,849 [INFO] cell16 - 状态管理器初始化完成\n",
      "2025-02-14 20:34:40,852 [INFO] __main__ - 正在初始化系统...\n",
      "2025-02-14 20:34:40,853 [INFO] __main__ - 启动系统监控...\n",
      "2025-02-14 20:34:40,855 [INFO] cell7 - 资源监控器初始化完成\n",
      "2025-02-14 20:34:40,863 [INFO] cell7 - 资源监控器已启动\n",
      "2025-02-14 20:34:40,864 [INFO] cell8 - 性能监控器已启动\n",
      "2025-02-14 20:34:40,865 [INFO] __main__ - 正在加载数据...\n",
      "2025-02-14 20:34:40,866 [INFO] __main__ - 正在初始化模型...\n",
      "2025-02-14 20:34:40,867 [INFO] cell11 - 构建模型 1/6...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ruanjiananzhuang\\anaconda\\envs\\ace\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 20:34:41,122 [WARNING] tensorflow - From D:\\ruanjiananzhuang\\anaconda\\envs\\ace\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "2025-02-14 20:34:41,137 [ERROR] cell11 - 构建模型 1 时出错: 'ModelEnsemble' object has no attribute '_build_basic_features'\n",
      "2025-02-14 20:34:41,138 [ERROR] cell11 - 构建模型时出错: 'ModelEnsemble' object has no attribute '_build_basic_features'\n",
      "2025-02-14 20:34:41,139 [ERROR] __main__ - 运行出错: 'ModelEnsemble' object has no attribute '_build_basic_features'\n",
      "2025-02-14 20:34:41,140 [INFO] __main__ - 清理资源...\n",
      "2025-02-14 20:34:45,864 [INFO] cell7 - 资源监控器已停止\n",
      "2025-02-14 20:34:45,866 [INFO] cell8 - 性能监控器已停止\n",
      "2025-02-14 20:34:45,867 [INFO] cell5 - 已关闭所有数据库连接\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ModelEnsemble' object has no attribute '_build_basic_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 136\u001b[0m\n\u001b[0;32m    133\u001b[0m         db_manager\u001b[38;5;241m.\u001b[39mclose_all()\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 136\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[1;32mIn[19], line 81\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# 4. 创建模型和优化器\u001b[39;00m\n\u001b[0;32m     80\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m正在初始化模型...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 81\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModelEnsemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m ModelOptimizer(model)\n\u001b[0;32m     83\u001b[0m dynamic_optimizer \u001b[38;5;241m=\u001b[39m DynamicOptimizer()\n",
      "File \u001b[1;32mD:\\JupyterWork\\notebooks\\cell11.py:44\u001b[0m, in \u001b[0;36mModelEnsemble.__init__\u001b[1;34m(self, sequence_length, feature_dim)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# 初始化模型\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m模型集成初始化完成\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\JupyterWork\\notebooks\\cell11.py:153\u001b[0m, in \u001b[0;36mModelEnsemble._build_models\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m):\n\u001b[0;32m    152\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m构建模型 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/6...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 153\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile_model(model)  \u001b[38;5;66;03m# 使用新的compile_model方法\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "File \u001b[1;32mD:\\JupyterWork\\notebooks\\cell11.py:167\u001b[0m, in \u001b[0;36mModelEnsemble._build_model\u001b[1;34m(self, model_num, params)\u001b[0m\n\u001b[0;32m    164\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_length, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_dim))\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# 1. 基础特征提取\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_basic_features\u001b[49m(inputs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# 2. 高级特征分析\u001b[39;00m\n\u001b[0;32m    170\u001b[0m advanced_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_advanced_features(inputs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ModelEnsemble' object has no attribute '_build_basic_features'"
     ]
    }
   ],
   "source": [
    "# 主程序入口\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# 导入自定义模块\n",
    "from cell1 import ConfigManager\n",
    "from cell5 import DatabaseManager\n",
    "from cell6 import DataManager\n",
    "from cell7 import ResourceMonitor\n",
    "from cell8 import PerformanceMonitor\n",
    "from cell11 import ModelEnsemble\n",
    "from cell13 import ModelOptimizer\n",
    "from cell14 import DynamicOptimizer\n",
    "from cell16 import StateManager\n",
    "\n",
    "def setup_monitoring():\n",
    "    \"\"\"初始化监控系统\"\"\"\n",
    "    # 从配置获取保存目录\n",
    "    config = ConfigManager()\n",
    "    save_dir = os.path.join(config.BASE_DIR, 'performance_records')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    resource_monitor = ResourceMonitor()\n",
    "    performance_monitor = PerformanceMonitor(save_dir=save_dir)\n",
    "    \n",
    "    # 启动监控\n",
    "    resource_monitor.start()\n",
    "    performance_monitor.start()\n",
    "    \n",
    "    return resource_monitor, performance_monitor\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"配置日志系统\"\"\"\n",
    "    log_dir = os.path.join('JupyterWork', 'logs')\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    log_file = os.path.join(log_dir, f'system_{datetime.now().strftime(\"%Y%m%d\")}.log')\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(name)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "def initialize_system():\n",
    "    \"\"\"初始化系统组件\"\"\"\n",
    "    config = ConfigManager()\n",
    "    db_manager = DatabaseManager()\n",
    "    data_manager = DataManager()\n",
    "    state_manager = StateManager()\n",
    "    \n",
    "    return config, db_manager, data_manager, state_manager\n",
    "\n",
    "def main():\n",
    "    # 设置日志\n",
    "    logger = setup_logging()\n",
    "    \n",
    "    try:\n",
    "        # 1. 初始化系统\n",
    "        logger.info(\"正在初始化系统...\")\n",
    "        config, db_manager, data_manager, state_manager = initialize_system()\n",
    "        \n",
    "        # 2. 启动监控\n",
    "        logger.info(\"启动系统监控...\")\n",
    "        resource_monitor, perf_monitor = setup_monitoring()\n",
    "        \n",
    "        # 3. 加载数据\n",
    "        logger.info(\"正在加载数据...\")\n",
    "        data_manager._init_data_loader()\n",
    "        train_data = data_manager.load_training_data()\n",
    "        \n",
    "        # 4. 创建模型和优化器\n",
    "        logger.info(\"正在初始化模型...\")\n",
    "        model = ModelEnsemble()\n",
    "        optimizer = ModelOptimizer(model)\n",
    "        dynamic_optimizer = DynamicOptimizer()\n",
    "        \n",
    "        # 5. 开始训练循环\n",
    "        logger.info(\"开始训练...\")\n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        def optimization_callback(study, trial):\n",
    "            nonlocal best_loss\n",
    "            current_loss = trial.value\n",
    "            if current_loss < best_loss:\n",
    "                best_loss = current_loss\n",
    "                logger.info(f\"发现更好的模型，loss: {best_loss}\")\n",
    "                model.save_checkpoint()\n",
    "        \n",
    "        optimizer.optimize(\n",
    "            train_data,\n",
    "            callbacks=[optimization_callback]\n",
    "        )\n",
    "        \n",
    "        # 6. 保存结果\n",
    "        logger.info(\"保存结果...\")\n",
    "        model.save_weights()\n",
    "        \n",
    "        # 7. 记录性能指标\n",
    "        perf_data = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'best_loss': float(best_loss),\n",
    "            'training_time': perf_monitor.get_total_time(),\n",
    "            'resource_usage': resource_monitor.get_summary()\n",
    "        }\n",
    "        \n",
    "        performance_file = os.path.join('logs', 'performance', \n",
    "                                      f'performance_{datetime.now().strftime(\"%Y%m%d\")}.json')\n",
    "        os.makedirs(os.path.dirname(performance_file), exist_ok=True)\n",
    "        \n",
    "        with open(performance_file, 'w') as f:\n",
    "            json.dump(perf_data, f, indent=4)\n",
    "        \n",
    "        logger.info(\"训练完成！\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"运行出错: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        # 清理资源\n",
    "        logger.info(\"清理资源...\")\n",
    "        if 'resource_monitor' in locals():\n",
    "            resource_monitor.stop()\n",
    "        if 'perf_monitor' in locals():\n",
    "            perf_monitor.stop()\n",
    "        db_manager.close_all()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (ace)",
   "language": "python",
   "name": "ace"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
