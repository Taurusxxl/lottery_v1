{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6df2e0d6-ef49-43ac-ae15-b95190004353",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "配置验证：\n",
      "数据库配置: {'host': 'localhost', 'port': 5432, 'user': 'ace_user'}\n",
      "训练配置: {'max_epochs': 50, 'batch_size': 32}\n"
     ]
    }
   ],
   "source": [
    "# 配置管理器\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, Any\n",
    "\n",
    "class ConfigManager:\n",
    "    \"\"\"配置管理器\"\"\"\n",
    "    _instance = None\n",
    "    \n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self):\n",
    "        if not hasattr(self, 'initialized'):\n",
    "            # 基础路径配置\n",
    "            self.BASE_DIR = 'D:\\\\JupyterWork'\n",
    "            self.LOG_DIR = os.path.join(self.BASE_DIR, 'logs')\n",
    "            self.MODEL_DIR = os.path.join(self.BASE_DIR, 'models')\n",
    "            self.DATA_DIR = os.path.join(self.BASE_DIR, 'data')\n",
    "            self.CHECKPOINT_DIR = os.path.join(self.BASE_DIR, 'checkpoints')\n",
    "            \n",
    "            # 创建必要的目录\n",
    "            for dir_path in [self.LOG_DIR, self.MODEL_DIR, self.DATA_DIR, self.CHECKPOINT_DIR]:\n",
    "                os.makedirs(dir_path, exist_ok=True)\n",
    "            \n",
    "            # 数据库配置\n",
    "            self.DB_CONFIG: dict = {\n",
    "                'host': 'localhost',\n",
    "                'port': 5432,\n",
    "                'user': 'ace_user'\n",
    "            }\n",
    "            \n",
    "            # 训练配置\n",
    "            self.TRAINING_CONFIG: dict = {\n",
    "                'max_epochs': 50,\n",
    "                'batch_size': 32\n",
    "            }\n",
    "            \n",
    "            # 系统配置\n",
    "            self.SYSTEM_CONFIG = {\n",
    "                'memory_limit': 8000,  # MB\n",
    "                'gpu_memory_limit': 4000,  # MB\n",
    "                'cleanup_interval': 300,  # seconds\n",
    "                'log_retention_days': 7,\n",
    "                'check_interval': 60,  # 检查新期号的间隔\n",
    "                'AUTO_TUNING': {\n",
    "                    'enable_per_sample': True,  # 启用逐样本调整\n",
    "                    'adjustment_steps': 5,      # 每个样本最大调整次数\n",
    "                    'learning_rate_range': (1e-5, 1e-2)  # 学习率调整范围\n",
    "                },\n",
    "                'DATA_CONFIG': {\n",
    "                    'cache_size': 10000,\n",
    "                    'normalize_range': (-1, 1)\n",
    "                },\n",
    "                'max_sequence_gap': 1,          # 允许的最大期号间隔\n",
    "                'max_threads': 8,  # 留出4线程给系统\n",
    "                'base_batch_size': 16,  # 初始批次\n",
    "                'gpu_mem_limit': 1536,  # MB (保留500MB给系统)\n",
    "                'cpu_util_threshold': 70,  # CPU使用率阈值\n",
    "                'SAMPLE_CONFIG': {\n",
    "                    'input_length': 144000,  # 修改这里\n",
    "                    'target_length': 2880,   # 修改这里\n",
    "                    'total_fetch': lambda: (  # 自动计算总获取量\n",
    "                        self.SYSTEM_CONFIG['SAMPLE_CONFIG']['input_length'] \n",
    "                        + self.SYSTEM_CONFIG['SAMPLE_CONFIG']['target_length']\n",
    "                    )\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            self.initialized = True\n",
    "    \n",
    "    def get_db_config(self) -> Dict[str, str]:\n",
    "        \"\"\"获取数据库配置\"\"\"\n",
    "        return self.DB_CONFIG.copy()\n",
    "    \n",
    "    def get_training_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取训练配置\"\"\"\n",
    "        return self.TRAINING_CONFIG.copy()\n",
    "    \n",
    "    def get_system_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取系统配置\"\"\"\n",
    "        return self.SYSTEM_CONFIG.copy()\n",
    "    \n",
    "    def update_config(self, config_name: str, updates: Dict[str, Any]) -> bool:\n",
    "        \"\"\"更新指定配置\"\"\"\n",
    "        try:\n",
    "            config = getattr(self, f'{config_name}_CONFIG')\n",
    "            config.update(updates)\n",
    "            return True\n",
    "        except AttributeError:\n",
    "            logging.error(f\"配置 {config_name} 不存在\")\n",
    "            return False\n",
    "    \n",
    "    def save_config(self, config_name: str) -> bool:\n",
    "        \"\"\"保存配置到文件\"\"\"\n",
    "        try:\n",
    "            config = getattr(self, f'{config_name}_CONFIG')\n",
    "            save_path = os.path.join(self.BASE_DIR, 'configs', f'{config_name.lower()}_config.json')\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            \n",
    "            with open(save_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(config, f, indent=4, ensure_ascii=False)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"保存配置失败: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def load_config(self, config_name: str) -> bool:\n",
    "        \"\"\"从文件加载配置\"\"\"\n",
    "        try:\n",
    "            load_path = os.path.join(self.BASE_DIR, 'configs', f'{config_name.lower()}_config.json')\n",
    "            if not os.path.exists(load_path):\n",
    "                return False\n",
    "            \n",
    "            with open(load_path, 'r', encoding='utf-8') as f:\n",
    "                config = json.load(f)\n",
    "            \n",
    "            setattr(self, f'{config_name}_CONFIG', config)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"加载配置失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "class ConfigValidator:\n",
    "    \"\"\"配置验证器\"\"\"\n",
    "    @staticmethod\n",
    "    def validate_db_config(config: Dict[str, str]) -> bool:\n",
    "        \"\"\"验证数据库配置\"\"\"\n",
    "        required_fields = ['host', 'user', 'password', 'database', 'charset']\n",
    "        return all(field in config for field in required_fields)\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_training_config(config: Dict[str, Any]) -> bool:\n",
    "        \"\"\"验证训练配置\"\"\"\n",
    "        try:\n",
    "            assert config['batch_size'] > 0\n",
    "            assert 0 < config['learning_rate'] < 1\n",
    "            assert config['epochs'] > 0\n",
    "            return True\n",
    "        except (AssertionError, KeyError):\n",
    "            return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_system_config(config: Dict[str, Any]) -> bool:\n",
    "        \"\"\"验证系统配置\"\"\"\n",
    "        try:\n",
    "            assert config['memory_limit'] > 0\n",
    "            assert config['gpu_memory_limit'] > 0\n",
    "            assert config['cleanup_interval'] > 0\n",
    "            assert config['log_retention_days'] > 0\n",
    "            return True\n",
    "        except (AssertionError, KeyError):\n",
    "            return False\n",
    "\n",
    "# 创建全局实例\n",
    "config_instance = ConfigManager()\n",
    "\n",
    "# 导出常用配置变量\n",
    "BASE_DIR = config_instance.BASE_DIR\n",
    "LOG_DIR = config_instance.LOG_DIR\n",
    "MODEL_DIR = config_instance.MODEL_DIR\n",
    "DATA_DIR = config_instance.DATA_DIR\n",
    "CHECKPOINT_DIR = config_instance.CHECKPOINT_DIR\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"配置验证：\")\n",
    "    print(f\"数据库配置: {config_instance.DB_CONFIG}\")\n",
    "    print(f\"训练配置: {config_instance.TRAINING_CONFIG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d6d2e5-c654-4129-8195-0e768c80f0e2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 数据管理器\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import pymysql\n",
    "from datetime import datetime, timedelta\n",
    "import threading\n",
    "from collections import deque\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from core.database_manager import db_manager  # 导入数据库管理器实例\n",
    "from core.config_manager import config_instance  # 导入配置管理器实例\n",
    "\n",
    "# 获取logger实例\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataManager:\n",
    "    \"\"\"数据管理器 - 单例模式\"\"\"\n",
    "    _instance = None\n",
    "    \n",
    "    def __new__(cls, db_config=None):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self, db_config=None):\n",
    "        if not hasattr(self, 'initialized'):\n",
    "            # 初始化组件\n",
    "            self.data_pool = DataPool()\n",
    "            self.data_processor = DataProcessor()\n",
    "            self.data_validator = DataValidator()\n",
    "            \n",
    "            # 数据缓存参数\n",
    "            self.cache_size = config_instance.SYSTEM_CONFIG['DATA_CONFIG']['cache_size']\n",
    "            self.batch_size = 32\n",
    "            self.sequence_length = 14400\n",
    "            \n",
    "            # 线程锁\n",
    "            self.lock = threading.Lock()\n",
    "            \n",
    "            # 新增配置项\n",
    "            self.comparison_dir = os.path.join(config_instance.BASE_DIR, 'comparison')\n",
    "            os.makedirs(self.comparison_dir, exist_ok=True)\n",
    "            self.issue_file = os.path.join(self.comparison_dir, 'issue_number.txt')\n",
    "            \n",
    "            # 标记初始化完成\n",
    "            self.initialized = True\n",
    "            logger.info(\"数据管理器初始化完成\")\n",
    "            \n",
    "            # 在DataManager中增加锁\n",
    "            self.issue_lock = threading.Lock()\n",
    "            \n",
    "            # 获取配置参数\n",
    "            self.normalize_range = config_instance.SYSTEM_CONFIG['DATA_CONFIG']['normalize_range']\n",
    "    \n",
    "    def get_training_batch(self, batch_size=None):\n",
    "        \"\"\"获取训练批次\"\"\"\n",
    "        try:\n",
    "            batch_size = batch_size or self.batch_size\n",
    "            with self.lock:\n",
    "                return self.data_processor.get_training_batch(\n",
    "                    self.data_pool.get_latest_data(),\n",
    "                    batch_size\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取训练批次时出错: {str(e)}\")\n",
    "            return None, None\n",
    "    \n",
    "    def update_data(self):\n",
    "        \"\"\"更新数据\"\"\"\n",
    "        try:\n",
    "            new_data = self._fetch_new_data()\n",
    "            if new_data is not None:\n",
    "                with self.lock:\n",
    "                    self.data_pool.update_data(new_data)\n",
    "                logger.info(f\"数据更新成功，当前数据量: {len(self.data_pool.data)}\")\n",
    "                return True\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"更新数据时出错: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def _get_next_issue(self, current_issue):\n",
    "        \"\"\"计算下一期号\"\"\"\n",
    "        date_str, period = current_issue.split('-')\n",
    "        date = datetime.strptime(date_str, '%Y%m%d')\n",
    "        period = int(period)\n",
    "        \n",
    "        if period == 1440:\n",
    "            next_date = date + timedelta(days=1)\n",
    "            next_period = 1\n",
    "        else:\n",
    "            next_date = date\n",
    "            next_period = period + 1\n",
    "            \n",
    "        return f\"{next_date.strftime('%Y%m%d')}-{next_period:04d}\"\n",
    "\n",
    "    def _fetch_new_data(self):\n",
    "        \"\"\"根据144000+2880的数据需求获取样本\"\"\"\n",
    "        try:\n",
    "            with self.issue_lock:\n",
    "                with open(self.issue_file, 'r+') as f:\n",
    "                    last_issue = f.read().strip()\n",
    "                    \n",
    "                    # 使用配置获取总数\n",
    "                    total = config_instance.SYSTEM_CONFIG['SAMPLE_CONFIG']['total_fetch']()\n",
    "                    \n",
    "                    # 构建精确查询\n",
    "                    query = f\"\"\"\n",
    "                        SELECT date_period, number \n",
    "                        FROM admin_tab \n",
    "                        WHERE date_period {'>' if last_issue else ''}= '{last_issue}'\n",
    "                        ORDER BY date_period \n",
    "                        LIMIT {total}\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    records = db_manager.execute_query(query)\n",
    "                    \n",
    "                    # 验证数据连续性\n",
    "                    if not self._validate_sequence(records):\n",
    "                        raise ValueError(\"数据存在断层\")\n",
    "                    \n",
    "                    # 更新期号文件\n",
    "                    new_last = records[-1]['date_period']\n",
    "                    f.seek(0)\n",
    "                    f.write(new_last)\n",
    "                    \n",
    "                    return self._process_numbers(records)  # 处理五位号码\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"数据获取失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _process_numbers(self, records):\n",
    "        \"\"\"处理五位数字号码\"\"\"\n",
    "        processed = []\n",
    "        for r in records:\n",
    "            # 将\"00236\"转换为[0,0,2,3,6]\n",
    "            numbers = [int(d) for d in r['number'].zfill(5)]\n",
    "            processed.append({\n",
    "                'date_period': r['date_period'],\n",
    "                'numbers': numbers,\n",
    "                'time_features': self.time_feature_extractor.extract_features(r['date_period'])\n",
    "            })\n",
    "        return processed\n",
    "\n",
    "    def validate_data(self, data):\n",
    "        \"\"\"验证数据有效性\"\"\"\n",
    "        return self.data_validator.validate(data)\n",
    "    \n",
    "    def get_data_stats(self):\n",
    "        \"\"\"获取数据统计信息\"\"\"\n",
    "        try:\n",
    "            with self.lock:\n",
    "                return {\n",
    "                    'total_samples': len(self.data_pool.data),\n",
    "                    'cache_size': self.data_pool.get_cache_size(),\n",
    "                    'last_update': self.data_pool.last_update_time\n",
    "                }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取数据统计信息时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def get_data_by_period(self, start_period, end_period):\n",
    "        \"\"\"获取指定期间的数据\"\"\"\n",
    "        try:\n",
    "            query = \"\"\"\n",
    "                SELECT date_period, number \n",
    "                FROM admin_tab \n",
    "                WHERE date_period BETWEEN %s AND %s\n",
    "                ORDER BY date_period\n",
    "            \"\"\"\n",
    "            records = db_manager.execute_query(\n",
    "                query, \n",
    "                params=(start_period, end_period),\n",
    "                use_cache=True\n",
    "            )\n",
    "            \n",
    "            if not records:\n",
    "                return None\n",
    "            \n",
    "            return self._process_numbers(records)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取指定期间数据时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "# 创建全局实例\n",
    "data_manager = DataManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb1b39-2cb7-45d9-8b18-22871b7affdd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 模型集成类\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU, Bidirectional, Conv1D\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add\n",
    "import os\n",
    "import json\n",
    "import threading\n",
    "from core import config_manager\n",
    "from optimizers import model_optimizer\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import torch\n",
    "\n",
    "# 获取logger实例\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ModelEnsemble:\n",
    "    \"\"\"模型集成类\"\"\"\n",
    "    \n",
    "    def __init__(self, sequence_length=14400, feature_dim=5):\n",
    "        \"\"\"\n",
    "        初始化模型集成\n",
    "        Args:\n",
    "            sequence_length: 输入序列长度\n",
    "            feature_dim: 特征维度\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.feature_dim = feature_dim\n",
    "        self.prediction_range = 2880  # 预测范围\n",
    "        self.models = []  # 模型列表\n",
    "        self.weights = np.ones(6) / 6  # 初始权重平均分配\n",
    "        \n",
    "        # 模型性能追踪\n",
    "        self.performance_history = {i: [] for i in range(6)}\n",
    "        \n",
    "        # 新增训练同步机制\n",
    "        self.training_lock = threading.Lock()\n",
    "        self.finished_models = 0\n",
    "        self.total_models = 6\n",
    "        \n",
    "        # 初始化模型\n",
    "        self._build_models()\n",
    "        logger.info(\"模型集成初始化完成\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def enhanced_match_loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        增强型匹配损失函数 - 结合匹配度评判和方向性引导\n",
    "        \n",
    "        Args:\n",
    "            y_true: 真实值 (batch_size, 2880, 5) - 2880期目标值\n",
    "            y_pred: 预测值 (batch_size, 5) - 预测的一组五位数\n",
    "            \n",
    "        Returns:\n",
    "            total_loss: 综合损失值,包含匹配损失和方向性损失\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 1. 预处理\n",
    "            y_pred_expanded = tf.expand_dims(y_pred, axis=1)  # (batch_size, 1, 5)\n",
    "            y_pred_rounded = tf.round(y_pred_expanded)\n",
    "            \n",
    "            # 2. 计算匹配情况\n",
    "            matches = tf.cast(tf.equal(y_true, y_pred_rounded), tf.float32)  # (batch_size, 2880, 5)\n",
    "            match_counts = tf.reduce_sum(matches, axis=-1)  # (batch_size, 2880)\n",
    "            \n",
    "            # 3. 找出最佳匹配的目标值\n",
    "            best_match_indices = tf.argmax(match_counts, axis=1)  # (batch_size,)\n",
    "            best_targets = tf.gather(y_true, best_match_indices, batch_dims=1)  # (batch_size, 5)\n",
    "            best_match_counts = tf.reduce_max(match_counts, axis=1)  # (batch_size,)\n",
    "            \n",
    "            # 4. 计算基础匹配损失\n",
    "            base_loss = 5.0 - best_match_counts  # (batch_size,)\n",
    "            \n",
    "            # 5. 计算方向性损失\n",
    "            # 计算预测值与最佳匹配目标的差异\n",
    "            value_diff = tf.squeeze(best_targets - y_pred, axis=1)  # (batch_size, 5)\n",
    "            \n",
    "            # 创建方向性掩码(只对未匹配的位置计算方向性损失)\n",
    "            direction_mask = tf.cast(\n",
    "                tf.not_equal(y_pred_rounded, tf.expand_dims(best_targets, axis=1)),\n",
    "                tf.float32\n",
    "            )  # (batch_size, 1, 5)\n",
    "            \n",
    "            # 使用sigmoid函数平滑方向性梯度\n",
    "            direction_factor = tf.sigmoid(value_diff * 2.0) * 2.0 - 1.0  # 范围(-1, 1)\n",
    "            \n",
    "            # 计算方向性损失(差异越大,损失越大)\n",
    "            direction_loss = tf.reduce_mean(\n",
    "                direction_mask * direction_factor * tf.abs(value_diff),\n",
    "                axis=-1\n",
    "            )  # (batch_size,)\n",
    "            \n",
    "            # 6. 完全匹配时损失为0\n",
    "            perfect_match = tf.cast(tf.equal(best_match_counts, 5.0), tf.float32)\n",
    "            \n",
    "            # 7. 组合损失(动态权重)\n",
    "            # 匹配数越少,方向性损失权重越大\n",
    "            direction_weight = tf.exp(-best_match_counts / 5.0) * 0.5  # 随匹配数增加呈指数衰减\n",
    "            total_loss = base_loss * (1.0 - perfect_match) + direction_weight * direction_loss\n",
    "            \n",
    "            # 8. 添加调试信息\n",
    "            tf.debugging.assert_all_finite(\n",
    "                total_loss,\n",
    "                \"Loss computation resulted in invalid values\"\n",
    "            )\n",
    "            \n",
    "            return total_loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"计算损失时出错: {str(e)}\")\n",
    "            return 5.0 * tf.ones_like(y_pred[:, 0])\n",
    "        \n",
    "    def _build_models(self):\n",
    "        \"\"\"构建所有模型\"\"\"\n",
    "        try:\n",
    "            model_params = {\n",
    "                'model_1': {\n",
    "                    'conv_filters': [32, 64, 128],\n",
    "                    'lstm_units': 128,\n",
    "                    'attention_heads': 8\n",
    "                },\n",
    "                'model_2': {\n",
    "                    'conv_filters': [64, 128],\n",
    "                    'lstm_units': 256,\n",
    "                    'dropout': 0.2\n",
    "                },\n",
    "                'model_3': {\n",
    "                    'gru_units': 128,\n",
    "                    'dense_units': [256, 128],\n",
    "                    'learning_rate': 0.001\n",
    "                },\n",
    "                'model_4': {\n",
    "                    'num_heads': 8,\n",
    "                    'key_dim': 64,\n",
    "                    'ff_dim': 256\n",
    "                },\n",
    "                'model_5': {\n",
    "                    'lstm_units': 128,\n",
    "                    'attention_heads': 4,\n",
    "                    'dropout': 0.1\n",
    "                },\n",
    "                'model_6': {\n",
    "                    'conv_filters': [32, 64],\n",
    "                    'gru_units': 128,\n",
    "                    'dense_units': [128, 64]\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            for i in range(6):\n",
    "                logger.info(f\"构建模型 {i+1}/6...\")\n",
    "                # 这里省略了具体的模型构建代码\n",
    "                # 你可以根据需要添加具体的模型构建逻辑\n",
    "                # self.models.append(构建的模型)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建模型时出错: {str(e)}\")\n",
    "    \n",
    "    def parallel_training(self, data):\n",
    "        \"\"\"补全并行训练逻辑\"\"\"\n",
    "        try:\n",
    "            # 根据当前线程数调整\n",
    "            with ThreadPoolExecutor(max_workers=config_manager.SYSTEM_CONFIG['max_threads']) as executor:\n",
    "                # 动态分配任务\n",
    "                chunk_size = len(data) // self.total_models\n",
    "                futures = []\n",
    "                for i in range(self.total_models):\n",
    "                    chunk = data[i*chunk_size : (i+1)*chunk_size]\n",
    "                    futures.append(executor.submit(self._train_chunk, chunk))\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"训练失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _train_chunk(self, chunk):\n",
    "        \"\"\"训练数据块\"\"\"\n",
    "        # 这里省略了具体的训练逻辑\n",
    "        # 你可以根据需要添加具体的训练逻辑\n",
    "        pass\n",
    "\n",
    "    def save_model(self, model_idx):\n",
    "        model = self.models[model_idx]\n",
    "        model.save(os.path.join(config_manager.MODEL_DIR, f'model_{model_idx}')) \n",
    "\n",
    "    def save_ensemble_info(self):\n",
    "        with open('ensemble_info.json', 'w') as f:\n",
    "            json.dump({\n",
    "                'weights': self.weights.tolist(),\n",
    "                'performance': self.performance_history\n",
    "            }, f) \n",
    "\n",
    "    def load_pretrained_models(self, model_paths):\n",
    "        for path in model_paths:\n",
    "            # 添加device参数适配CPU\n",
    "            model = torch.load(path, map_location=torch.device('cpu'))  \n",
    "            self.models.append(model) \n",
    "\n",
    "# 创建全局实例\n",
    "model_ensemble = ModelEnsemble()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d791b2-0215-496d-8ca2-0bc8cdd7441d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 训练策略优化器\n",
    "import numpy as np\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.util import Events\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import psutil\n",
    "\n",
    "# 获取logger实例\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TrainingOptimizer:\n",
    "    \"\"\"训练策略优化器类\"\"\"\n",
    "    \n",
    "    def __init__(self, model_ensemble, data_processor):\n",
    "        \"\"\"\n",
    "        初始化训练优化器\n",
    "        Args:\n",
    "            model_ensemble: 模型集成实例\n",
    "            data_processor: 数据处理器实例\n",
    "        \"\"\"\n",
    "        self.model_ensemble = model_ensemble\n",
    "        self.data_processor = data_processor\n",
    "        self.best_params = None\n",
    "        self.optimization_history = []\n",
    "        self.best_match_history = []  # 记录最佳匹配数历史\n",
    "        self.match_distribution = {i: 0 for i in range(6)}  # 记录各匹配数的分布\n",
    "        \n",
    "        # 定义参数范围\n",
    "        self.param_ranges = {\n",
    "            # 1. 优化器参数\n",
    "            'optimizer_params': {\n",
    "                'learning_rate': (1e-5, 1e-2),\n",
    "                'beta_1': (0.8, 0.999),\n",
    "                'beta_2': (0.8, 0.999),\n",
    "                'epsilon': (1e-8, 1e-6)\n",
    "            },\n",
    "            \n",
    "            # 2. 学习率调度参数\n",
    "            'lr_schedule_params': {\n",
    "                'decay_rate': (0.9, 0.99),\n",
    "                'decay_steps': (100, 1000),\n",
    "                'warmup_steps': (0, 100),\n",
    "                'min_lr': (1e-6, 1e-4)\n",
    "            },\n",
    "            \n",
    "            # 3. 训练控制参数\n",
    "            'training_control': {\n",
    "                'batch_size': (16, 128),\n",
    "                'epochs_per_iteration': (1, 10),\n",
    "                'validation_frequency': (1, 10),\n",
    "                'early_stopping_patience': (10, 50)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 定义离散参数\n",
    "        self.discrete_params = {\n",
    "            'optimizer_type': ['adam', 'adamw', 'radam'],\n",
    "            'scheduler_type': ['exponential', 'cosine', 'step']\n",
    "        }\n",
    "        \n",
    "        logger.info(\"训练优化器初始化完成\")\n",
    "\n",
    "    def optimize(self, n_iter=50):\n",
    "        \"\"\"\n",
    "        运行优化过程\n",
    "        Args:\n",
    "            n_iter: 优化迭代次数\n",
    "        Returns:\n",
    "            dict: 最佳参数\n",
    "        \"\"\"\n",
    "        try:\n",
    "            optimizer = BayesianOptimization(\n",
    "                f=self._objective_function,\n",
    "                pbounds=self._flatten_param_ranges(),\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            # 定义优化回调\n",
    "            def optimization_callback(res):\n",
    "                progress = self._check_optimization_progress()\n",
    "                if progress and not progress['is_improving'] and progress['is_stable']:\n",
    "                    logger.info(\"优化进入稳定阶段，但仍将继续探索\")\n",
    "            \n",
    "            optimizer.subscribe(Events.OPTIMIZATION_STEP, optimization_callback)\n",
    "            \n",
    "            optimizer.maximize(\n",
    "                init_points=5,\n",
    "                n_iter=n_iter\n",
    "            )\n",
    "            \n",
    "            self.best_params = self._process_params(optimizer.max['params'])\n",
    "            self._save_best_params()\n",
    "            \n",
    "            # 最终优化结果总结\n",
    "            final_progress = self._check_optimization_progress()\n",
    "            if final_progress:\n",
    "                logger.info(\"优化完成总结:\")\n",
    "                logger.info(f\"最终得分: {final_progress['current_score']:.4f}\")\n",
    "                logger.info(f\"最佳得分: {final_progress['best_score']:.4f}\")\n",
    "                logger.info(f\"优化稳定性: {final_progress['std_score']:.4f}\")\n",
    "            \n",
    "            return self.best_params\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"运行优化过程时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _objective_function(self, **params):\n",
    "        \"\"\"优化目标函数\"\"\"\n",
    "        try:\n",
    "            # 1. 验证参数\n",
    "            is_valid, message = self._validate_params(params)\n",
    "            if not is_valid:\n",
    "                logger.warning(f\"参数验证失败: {message}\")\n",
    "                return float('-inf')\n",
    "            \n",
    "            # 2. 更新训练参数\n",
    "            self._update_training_params(params)\n",
    "            \n",
    "            # 3. 获取评估数据\n",
    "            X, y = self.data_processor.get_validation_data()\n",
    "            if X is None or y is None:\n",
    "                return float('-inf')\n",
    "            \n",
    "            # 4. 评估模型性能\n",
    "            score = self._evaluate_model(X, y)\n",
    "            \n",
    "            # 5. 记录优化历史\n",
    "            self.optimization_history.append({\n",
    "                'params': params,\n",
    "                'score': score\n",
    "            })\n",
    "            \n",
    "            return score\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"优化目标函数执行出错: {str(e)}\")\n",
    "            return float('-inf')\n",
    "\n",
    "    def _evaluate_model(self, X, y):\n",
    "        \"\"\"评估模型性能\"\"\"\n",
    "        try:\n",
    "            # 这里省略了具体的评估逻辑\n",
    "            # 你可以根据需要添加具体的评估逻辑\n",
    "            return np.random.rand()  # 返回随机得分作为示例\n",
    "        except Exception as e:\n",
    "            logger.error(f\"评估模型性能时出错: {str(e)}\")\n",
    "            return 0.0\n",
    "\n",
    "    def _validate_params(self, params):\n",
    "        \"\"\"验证参数\"\"\"\n",
    "        try:\n",
    "            # 这里省略了具体的验证逻辑\n",
    "            # 你可以根据需要添加具体的验证逻辑\n",
    "            return True, \"\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"验证参数时出错: {str(e)}\")\n",
    "            return False, str(e)\n",
    "\n",
    "    def _update_training_params(self, params):\n",
    "        \"\"\"更新训练参数\"\"\"\n",
    "        # 这里省略了具体的更新逻辑\n",
    "        # 你可以根据需要添加具体的更新逻辑\n",
    "        pass\n",
    "\n",
    "    def _flatten_param_ranges(self):\n",
    "        \"\"\"将嵌套参数范围展平\"\"\"\n",
    "        flat_ranges = {}\n",
    "        for category, params in self.param_ranges.items():\n",
    "            for param_name, param_range in params.items():\n",
    "                flat_name = f\"{category}__{param_name}\"\n",
    "                flat_ranges[flat_name] = param_range\n",
    "        return flat_ranges\n",
    "\n",
    "    def _process_params(self, flat_params):\n",
    "        \"\"\"将展平的参数重构为嵌套结构\"\"\"\n",
    "        nested_params = {}\n",
    "        for flat_name, value in flat_params.items():\n",
    "            category, param_name = flat_name.split('__')\n",
    "            if category not in nested_params:\n",
    "                nested_params[category] = {}\n",
    "            nested_params[category][param_name] = value\n",
    "        return nested_params\n",
    "\n",
    "    def _save_best_params(self):\n",
    "        \"\"\"保存最佳参数\"\"\"\n",
    "        try:\n",
    "            save_path = os.path.join('best_params.json')\n",
    "            with open(save_path, 'w') as f:\n",
    "                json.dump(self.best_params, f, indent=4)\n",
    "            logger.info(f\"最佳参数已保存到: {save_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"保存最佳参数时出错: {str(e)}\")\n",
    "\n",
    "# 创建全局实例\n",
    "training_optimizer = TrainingOptimizer(model_ensemble=None, data_processor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477ce1a2-e933-4a3b-870c-be6a80c1de7c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 性能监控器\n",
    "import logging\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from collections import deque\n",
    "import threading\n",
    "\n",
    "# 获取logger实例\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"性能监控器 - 负责收集和分析性能指标\"\"\"\n",
    "    def __init__(self, save_dir, window_size=100):\n",
    "        self.save_dir = save_dir\n",
    "        self.window_size = window_size\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "        # 性能指标存储\n",
    "        self.metrics = {\n",
    "            'loss': deque(maxlen=window_size),\n",
    "            'accuracy': deque(maxlen=window_size),\n",
    "            'training_time': deque(maxlen=window_size),\n",
    "            'prediction_time': deque(maxlen=window_size),\n",
    "            'memory_usage': deque(maxlen=window_size),\n",
    "            'gpu_usage': deque(maxlen=window_size),  # 新增GPU使用率指标\n",
    "            'batch_time': deque(maxlen=window_size)  # 新增批次处理时间指标\n",
    "        }\n",
    "        \n",
    "        # 警报阈值\n",
    "        self.thresholds = {\n",
    "            'loss_increase': 0.2,        # 损失增加超过20%\n",
    "            'accuracy_drop': 0.1,        # 准确率下降超过10%\n",
    "            'training_time_increase': 0.5,  # 训练时间增加超过50%\n",
    "            'memory_usage': 0.9,         # 内存使用率超过90%\n",
    "            'gpu_usage': 0.95,           # GPU使用率超过95%\n",
    "            'batch_time_increase': 0.3   # 批次时间增加超过30%\n",
    "        }\n",
    "        \n",
    "        # 性能趋势分析\n",
    "        self.trend_window = 10  # 趋势分析窗口\n",
    "        self.trend_threshold = 0.05  # 趋势判定阈值\n",
    "        \n",
    "        # 初始化性能日志文件\n",
    "        self._init_log_file()\n",
    "        \n",
    "    def _init_log_file(self):\n",
    "        \"\"\"初始化性能日志文件\"\"\"\n",
    "        try:\n",
    "            self.log_file = os.path.join(\n",
    "                self.save_dir, \n",
    "                f'performance_{datetime.now().strftime(\"%Y%m%d\")}.json'\n",
    "            )\n",
    "            os.makedirs(self.save_dir, exist_ok=True)\n",
    "            \n",
    "            if not os.path.exists(self.log_file):\n",
    "                with open(self.log_file, 'w') as f:\n",
    "                    json.dump([], f)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"初始化性能日志文件失败: {str(e)}\")\n",
    "    \n",
    "    def update_metrics(self, metrics_dict):\n",
    "        \"\"\"更新性能指标\"\"\"\n",
    "        try:\n",
    "            with self.lock:\n",
    "                # 更新指标\n",
    "                for metric_name, value in metrics_dict.items():\n",
    "                    if metric_name in self.metrics:\n",
    "                        self.metrics[metric_name].append(value)\n",
    "                \n",
    "                # 记录到日志文件\n",
    "                self._log_metrics(metrics_dict)\n",
    "                \n",
    "                # 检查警报\n",
    "                self._check_alerts()\n",
    "                \n",
    "                # 分析趋势\n",
    "                self._analyze_trends()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"更新性能指标失败: {str(e)}\")\n",
    "    \n",
    "    def _log_metrics(self, metrics_dict):\n",
    "        \"\"\"记录性能指标到日志文件\"\"\"\n",
    "        try:\n",
    "            log_entry = {\n",
    "                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'metrics': metrics_dict\n",
    "            }\n",
    "            \n",
    "            # 读取现有日志\n",
    "            with open(self.log_file, 'r') as f:\n",
    "                logs = json.load(f)\n",
    "                \n",
    "            # 添加新记录\n",
    "            logs.append(log_entry)\n",
    "            \n",
    "            # 写回文件\n",
    "            with open(self.log_file, 'w') as f:\n",
    "                json.dump(logs, f, indent=4)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"记录性能指标失败: {str(e)}\")\n",
    "    \n",
    "    def _check_alerts(self):\n",
    "        \"\"\"检查性能警报\"\"\"\n",
    "        try:\n",
    "            for metric_name, values in self.metrics.items():\n",
    "                if len(values) < 2:\n",
    "                    continue\n",
    "                \n",
    "                # 检查是否超过阈值\n",
    "                if metric_name in self.thresholds:\n",
    "                    threshold = self.thresholds[metric_name]\n",
    "                    if values[-1] > threshold:\n",
    "                        logger.warning(f\"{metric_name} 超过阈值: {values[-1]:.2f} > {threshold:.2f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"检查性能警报失败: {str(e)}\")\n",
    "    \n",
    "    def _analyze_trends(self):\n",
    "        \"\"\"分析性能趋势\"\"\"\n",
    "        try:\n",
    "            for metric_name, values in self.metrics.items():\n",
    "                if len(values) < self.trend_window:\n",
    "                    continue\n",
    "                \n",
    "                # 计算趋势\n",
    "                trend = np.polyfit(range(self.trend_window), list(values)[-self.trend_window:], 1)[0]\n",
    "                if abs(trend) > self.trend_threshold:\n",
    "                    trend_direction = \"上升\" if trend > 0 else \"下降\"\n",
    "                    logger.info(f\"{metric_name} 趋势: {trend_direction} (斜率: {trend:.4f})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"分析性能趋势失败: {str(e)}\")\n",
    "\n",
    "# 创建全局实例\n",
    "performance_monitor = PerformanceMonitor(save_dir='logs/performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcebe67-d603-4acf-99c7-d2d56aa35482",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 资源监控器\n",
    "import psutil\n",
    "import logging\n",
    "import threading\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "\n",
    "# 获取logger实例\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ResourceMonitor:\n",
    "    \"\"\"资源监控器 - 负责监控CPU、内存、GPU等资源使用情况\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=100, check_interval=5):\n",
    "        self.window_size = window_size\n",
    "        self.check_interval = check_interval\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "        # 监控指标存储\n",
    "        self.metrics = {\n",
    "            'cpu_usage': deque(maxlen=window_size),\n",
    "            'memory_usage': deque(maxlen=window_size),\n",
    "            'disk_usage': deque(maxlen=window_size),\n",
    "            'gpu_usage': None,\n",
    "            'gpu_memory': None\n",
    "        }\n",
    "        \n",
    "        # 警报阈值\n",
    "        self.thresholds = {\n",
    "            'cpu_usage': 90,    # CPU使用率超过90%\n",
    "            'memory_usage': 90,  # 内存使用率超过90%\n",
    "            'disk_usage': 90,    # 磁盘使用率超过90%\n",
    "            'gpu_usage': 90,     # GPU使用率超过90%\n",
    "            'gpu_memory': 90     # GPU内存使用率超过90%\n",
    "        }\n",
    "        \n",
    "        # 监控线程\n",
    "        self.monitor_thread = None\n",
    "        self.is_running = False\n",
    "        \n",
    "        # 警报历史\n",
    "        self.alerts = []\n",
    "        \n",
    "        self.memory_usage = 0.0  # 确保有此属性初始化\n",
    "        self.cpu_usage = 0.0\n",
    "        self.gpu_usage = 0.0\n",
    "        \n",
    "        logger.info(\"资源监控器初始化完成\")\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"启动资源监控\"\"\"\n",
    "        if self.monitor_thread and self.monitor_thread.is_alive():\n",
    "            logger.warning(\"资源监控器已在运行\")\n",
    "            return\n",
    "            \n",
    "        self.is_running = True\n",
    "        self.monitor_thread = threading.Thread(target=self._monitor_loop)\n",
    "        self.monitor_thread.daemon = True\n",
    "        self.monitor_thread.start()\n",
    "        logger.info(\"资源监控器已启动\")\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"停止资源监控\"\"\"\n",
    "        self.is_running = False\n",
    "        if self.monitor_thread:\n",
    "            self.monitor_thread.join()\n",
    "        logger.info(\"资源监控器已停止\")\n",
    "    \n",
    "    def _monitor_loop(self):\n",
    "        \"\"\"监控循环\"\"\"\n",
    "        while self.is_running:\n",
    "            try:\n",
    "                self._collect_metrics()\n",
    "                self._check_alerts()\n",
    "                time.sleep(self.check_interval)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"资源监控循环出错: {str(e)}\")\n",
    "    \n",
    "    def _collect_metrics(self):\n",
    "        \"\"\"收集资源指标\"\"\"\n",
    "        with self.lock:\n",
    "            # CPU使用率\n",
    "            self.cpu_usage = psutil.cpu_percent()\n",
    "            self.metrics['cpu_usage'].append(self.cpu_usage)\n",
    "            \n",
    "            # 内存使用率\n",
    "            mem = psutil.virtual_memory()\n",
    "            self.memory_usage = mem.percent  # 确保有此属性更新\n",
    "            self.metrics['memory_usage'].append(self.memory_usage)\n",
    "            \n",
    "            # 磁盘使用率\n",
    "            disk = psutil.disk_usage('/')\n",
    "            self.metrics['disk_usage'].append(disk.percent)\n",
    "    \n",
    "    def _check_alerts(self):\n",
    "        \"\"\"检查警报\"\"\"\n",
    "        with self.lock:\n",
    "            for metric, values in self.metrics.items():\n",
    "                if not values:\n",
    "                    continue\n",
    "                current = values[-1]\n",
    "                if current > self.thresholds[metric]:\n",
    "                    alert = {\n",
    "                        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        'metric': metric,\n",
    "                        'value': current,\n",
    "                        'threshold': self.thresholds[metric]\n",
    "                    }\n",
    "                    self.alerts.append(alert)\n",
    "                    logger.warning(f\"资源警报: {metric} = {current}% (阈值: {self.thresholds[metric]}%)\")\n",
    "\n",
    "# 创建全局实例\n",
    "resource_monitor = ResourceMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53728f26-2338-4107-8eb4-fee3fb003f31",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 数据库管理器\n",
    "import pymysql\n",
    "import logging\n",
    "import threading\n",
    "from pymysql.cursors import DictCursor\n",
    "from sqlalchemy.pool import QueuePool  # 使用SQLAlchemy自带的连接池\n",
    "from datetime import datetime, timedelta\n",
    "from core.config_manager import config_instance\n",
    "\n",
    "# 获取logger实例\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DatabaseManager:\n",
    "    \"\"\"数据库管理器 - 单例模式\"\"\"\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "    \n",
    "    def __new__(cls, db_config=None):\n",
    "        if cls._instance is None:\n",
    "            with cls._lock:\n",
    "                if cls._instance is None:\n",
    "                    cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self, db_config=None):\n",
    "        if not hasattr(self, 'initialized'):\n",
    "            self.DB_CONFIG = {\n",
    "                'host': 'localhost',\n",
    "                'user': 'root',\n",
    "                'password': 'tt198803',  # 使用提供的密码\n",
    "                'database': 'admin_data',\n",
    "                'charset': 'utf8mb4'\n",
    "            }\n",
    "            \n",
    "            # 创建数据库连接池\n",
    "            self.pool = self._create_pool()\n",
    "            \n",
    "            # 初始化查询缓存\n",
    "            self.query_cache = {}\n",
    "            self.cache_timeout = 300  # 5分钟缓存超时\n",
    "            \n",
    "            # 标记初始化完成\n",
    "            self.initialized = True\n",
    "            logger.info(\"数据库管理器初始化完成\")\n",
    "    \n",
    "    def _create_pool(self):\n",
    "        \"\"\"创建数据库连接池\"\"\"\n",
    "        try:\n",
    "            self.pool = QueuePool(\n",
    "                creator=lambda: pymysql.connect(**self.DB_CONFIG),\n",
    "                pool_size=10,\n",
    "                max_overflow=20,\n",
    "                timeout=30\n",
    "            )\n",
    "            logger.info(\"数据库连接池创建成功\")\n",
    "            return self.pool\n",
    "        except Exception as e:\n",
    "            logger.error(f\"创建数据库连接池失败: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def execute_query(self, query, params=None, use_cache=False):\n",
    "        \"\"\"执行查询\"\"\"\n",
    "        try:\n",
    "            # 检查缓存\n",
    "            if use_cache:\n",
    "                cache_key = f\"{query}_{str(params)}\"\n",
    "                cached_result = self._get_from_cache(cache_key)\n",
    "                if cached_result is not None:\n",
    "                    return cached_result\n",
    "            \n",
    "            # 获取连接和游标\n",
    "            connection = self.pool.connect()\n",
    "            try:\n",
    "                cursor = connection.cursor(DictCursor)\n",
    "                # 执行查询\n",
    "                cursor.execute(query, params)\n",
    "                result = cursor.fetchall()\n",
    "                \n",
    "                # 更新缓存\n",
    "                if use_cache:\n",
    "                    self._update_cache(cache_key, result)\n",
    "                \n",
    "                return result\n",
    "            finally:\n",
    "                cursor.close()\n",
    "                connection.close()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"执行查询失败: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def execute_batch(self, query, params_list):\n",
    "        \"\"\"批量执行查询\"\"\"\n",
    "        try:\n",
    "            connection = self.pool.connect()\n",
    "            cursor = connection.cursor()\n",
    "            \n",
    "            try:\n",
    "                cursor.executemany(query, params_list)\n",
    "                connection.commit()\n",
    "                return True\n",
    "            finally:\n",
    "                cursor.close()\n",
    "                connection.close()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"批量执行查询失败: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def _get_from_cache(self, key):\n",
    "        \"\"\"从缓存获取数据\"\"\"\n",
    "        if key in self.query_cache:\n",
    "            timestamp, data = self.query_cache[key]\n",
    "            if datetime.now() - timestamp < timedelta(seconds=self.cache_timeout):\n",
    "                return data\n",
    "            else:\n",
    "                del self.query_cache[key]\n",
    "        return None\n",
    "    \n",
    "    def _update_cache(self, key, data):\n",
    "        \"\"\"更新缓存\"\"\"\n",
    "        self.query_cache[key] = (datetime.now(), data)\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"清理缓存\"\"\"\n",
    "        self.query_cache.clear()\n",
    "        logger.info(\"查询缓存已清理\")\n",
    "\n",
    "    def get_records_by_issue(self, start_issue, limit):\n",
    "        \"\"\"按期号范围获取记录\"\"\"\n",
    "        query = f\"\"\"\n",
    "            SELECT * FROM admin_tab \n",
    "            WHERE date_period >= %s\n",
    "            ORDER BY date_period ASC\n",
    "            LIMIT %s\n",
    "        \"\"\"\n",
    "        return self.execute_query(query, (start_issue, limit))\n",
    "\n",
    "# 创建全局实例\n",
    "db_manager = DatabaseManager()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (ace)",
   "language": "python",
   "name": "ace"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
