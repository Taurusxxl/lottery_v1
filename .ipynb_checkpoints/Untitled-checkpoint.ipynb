{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2076b008-b28a-4b47-95c3-68be2c7c1d7a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ruanjiananzhuang\\anaconda\\envs\\ace\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-18 15:17:55,822] INFO - 配置系统初始化完成\n",
      "[2025-02-18 15:17:55,823] INFO - 配置系统测试通过\n",
      "[2025-02-18 15:17:55,824] INFO - 核心管理器初始化完成\n"
     ]
    }
   ],
   "source": [
    "#1 Core Configuration Manager / 核心配置管理器\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import logging.handlers\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "from IPython.display import clear_output\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional\n",
    "from cell3_monitor import (\n",
    "    DailyRotatingFileHandler, \n",
    "    CustomFormatter,\n",
    "    ProgressHandler, \n",
    "    LogDisplayManager\n",
    ")\n",
    "import numpy as np  # 确保所有使用np的地方都有导入\n",
    "\n",
    "class CoreManager:\n",
    "    \"\"\"核心管理器 - 整合配置和日志管理\"\"\"\n",
    "    _instance = None\n",
    "    \n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self):\n",
    "        if not hasattr(self, 'initialized'):\n",
    "            # 初始化基础目录结构\n",
    "            self._init_directories()\n",
    "            # 初始化日志系统\n",
    "            self._init_logging_system()\n",
    "            # 初始化配置系统（需要最先完成）\n",
    "            self._init_config_system()\n",
    "            \n",
    "            # 添加配置验证规则\n",
    "            self.validation_rules = {\n",
    "                'learning_rate': lambda x: 0 < x < 1,\n",
    "                'batch_size': lambda x: x > 0 and x & (x-1) == 0\n",
    "            }\n",
    "            \n",
    "            # 初始化时执行配置测试\n",
    "            self._test_config_system()\n",
    "            \n",
    "            # 最后设置初始化完成标记\n",
    "            self.initialized = True\n",
    "            self.logger.info(\"核心管理器初始化完成\")\n",
    "\n",
    "    def _init_directories(self):\n",
    "        \"\"\"初始化目录结构\"\"\"\n",
    "        # 基础路径配置\n",
    "        self.BASE_DIR = 'D:\\\\JupyterWork'\n",
    "        self.LOG_DIR = os.path.join(self.BASE_DIR, 'logs')\n",
    "        self.MODEL_DIR = os.path.join(self.BASE_DIR, 'models')\n",
    "        self.DATA_DIR = os.path.join(self.BASE_DIR, 'data')\n",
    "        self.CHECKPOINT_DIR = os.path.join(self.BASE_DIR, 'checkpoints')\n",
    "        \n",
    "        # 创建必要的目录\n",
    "        for dir_path in [self.LOG_DIR, self.MODEL_DIR, self.DATA_DIR, self.CHECKPOINT_DIR]:\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    def _init_logging_system(self):\n",
    "        \"\"\"初始化日志系统\"\"\"\n",
    "        # 创建主日志处理器\n",
    "        self.logger = logging.getLogger('ACE_System')\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # 日常日志处理器\n",
    "        daily_handler = self._create_daily_rotating_handler()\n",
    "        self.logger.addHandler(daily_handler)\n",
    "        \n",
    "        # 控制台处理器\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setFormatter(self._create_custom_formatter())\n",
    "        self.logger.addHandler(console_handler)\n",
    "        \n",
    "        # 进度条处理器\n",
    "        progress_handler = self._create_progress_handler()\n",
    "        self.logger.addHandler(progress_handler)\n",
    "\n",
    "    def _init_config_system(self):\n",
    "        \"\"\"初始化配置系统\"\"\"\n",
    "        # 系统配置\n",
    "        self.SYSTEM_CONFIG = {\n",
    "            'TRAINING_CONFIG': {  # 添加训练配置\n",
    "                'batch_size': 64,\n",
    "                'max_epochs': 100,\n",
    "                'early_stopping_patience': 10,\n",
    "                'learning_rate': 0.001,\n",
    "                'model_checkpoint_interval': 5\n",
    "            },\n",
    "            'DATA_CONFIG': {\n",
    "                'cache_size': 10000,\n",
    "                'min_sequence_length': 14400,\n",
    "                'normalize_range': (-1, 1)\n",
    "            },\n",
    "            'SAMPLE_CONFIG': {\n",
    "                'input_length': 14400,\n",
    "                'target_length': 2880\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 单独的训练配置\n",
    "        self.TRAINING_CONFIG = self.SYSTEM_CONFIG['TRAINING_CONFIG']\n",
    "        \n",
    "        # 数据库配置\n",
    "        self.DB_CONFIG = {\n",
    "            'host': 'localhost',\n",
    "            'port': 3306,\n",
    "            'user': 'root',\n",
    "            'password': 'tt198803',\n",
    "            'database': 'admin_data',\n",
    "            'charset': 'utf8mb4'\n",
    "        }\n",
    "        \n",
    "        # 优化器配置\n",
    "        self.OPTIMIZER_CONFIG = {\n",
    "            'learning_rate': 0.001,\n",
    "            'beta_1': 0.9,\n",
    "            'beta_2': 0.999,\n",
    "            'epsilon': 1e-07\n",
    "        }\n",
    "        \n",
    "        # 日志配置\n",
    "        self.LOG_CONFIG = {\n",
    "            'log_level': 'INFO',\n",
    "            'log_format': '%(asctime)s [%(levelname)s] %(name)s - %(message)s',\n",
    "            'log_file': 'system.log',\n",
    "            'log_retention_days': 7\n",
    "        }\n",
    "        \n",
    "        # 模型配置\n",
    "        self.MODEL_CONFIG = {\n",
    "            'model_type': 'transformer',\n",
    "            'num_layers': 6,\n",
    "            'num_heads': 8,\n",
    "            'd_model': 512,\n",
    "            'dff': 2048,\n",
    "            'dropout_rate': 0.1\n",
    "        }\n",
    "        \n",
    "        self.logger.info(\"配置系统初始化完成\")\n",
    "\n",
    "    def update_config(self, config_name: str, updates: Dict[str, Any]) -> bool:\n",
    "        \"\"\"更新指定配置\"\"\"\n",
    "        try:\n",
    "            config = getattr(self, f'{config_name}_CONFIG')\n",
    "            config.update(updates)\n",
    "            return True\n",
    "        except AttributeError:\n",
    "            self.logger.error(f\"配置 {config_name} 不存在\")\n",
    "            return False\n",
    "\n",
    "    def save_config(self, config_name: str) -> bool:\n",
    "        \"\"\"保存配置到文件\"\"\"\n",
    "        try:\n",
    "            config = getattr(self, f'{config_name}_CONFIG')\n",
    "            save_path = os.path.join(self.BASE_DIR, 'configs', f'{config_name.lower()}_config.json')\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            \n",
    "            with open(save_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(config, f, indent=4, ensure_ascii=False)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"保存配置失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def load_config(self, config_name: str) -> bool:\n",
    "        \"\"\"从文件加载配置\"\"\"\n",
    "        try:\n",
    "            load_path = os.path.join(self.BASE_DIR, 'configs', f'{config_name.lower()}_config.json')\n",
    "            if not os.path.exists(load_path):\n",
    "                return False\n",
    "            \n",
    "            with open(load_path, 'r', encoding='utf-8') as f:\n",
    "                config = json.load(f)\n",
    "            \n",
    "            setattr(self, f'{config_name}_CONFIG', config)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"加载配置失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def validate_config(self, config_name: str) -> bool:\n",
    "        \"\"\"验证配置有效性\"\"\"\n",
    "        try:\n",
    "            if config_name == 'DB':\n",
    "                return self._validate_db_config(self.DB_CONFIG)\n",
    "            elif config_name == 'SYSTEM':\n",
    "                return self._validate_system_config(self.SYSTEM_CONFIG)\n",
    "            elif config_name == 'TRAINING':\n",
    "                return self._validate_training_config(self.TRAINING_CONFIG)\n",
    "            else:\n",
    "                self.logger.error(f\"未知的配置类型: {config_name}\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"验证配置失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _validate_db_config(self, config: Dict[str, str]) -> bool:\n",
    "        \"\"\"验证数据库配置\"\"\"\n",
    "        required_fields = ['host', 'user', 'password', 'database', 'charset']\n",
    "        return all(field in config for field in required_fields)\n",
    "\n",
    "    def _validate_training_config(self, config: Dict[str, Any]) -> bool:\n",
    "        \"\"\"验证训练配置\"\"\"\n",
    "        try:\n",
    "            assert config['batch_size'] > 0\n",
    "            assert config['max_epochs'] > 0\n",
    "            return True\n",
    "        except (AssertionError, KeyError):\n",
    "            return False\n",
    "\n",
    "    def _validate_system_config(self, config: Dict[str, Any]) -> bool:\n",
    "        \"\"\"验证系统配置\"\"\"\n",
    "        try:\n",
    "            assert config['memory_limit'] > 0\n",
    "            assert config['gpu_memory_limit'] > 0\n",
    "            assert config['cleanup_interval'] > 0\n",
    "            assert config['log_retention_days'] > 0\n",
    "            return True\n",
    "        except (AssertionError, KeyError):\n",
    "            return False\n",
    "\n",
    "    def _create_daily_rotating_handler(self):\n",
    "        \"\"\"创建每日轮转的日志处理器\"\"\"\n",
    "        handler = DailyRotatingFileHandler(\n",
    "            base_dir=self.LOG_DIR,\n",
    "            prefix='system'\n",
    "        )\n",
    "        handler.setFormatter(self._create_custom_formatter())\n",
    "        return handler\n",
    "        \n",
    "    # 添加从cell4中的LogManager类的功能\n",
    "    def setup_continuous_logging(self):\n",
    "        \"\"\"设置持续训练的日志系统\"\"\"\n",
    "        self.continuous_logger = logging.getLogger('ContinuousTraining')\n",
    "        self.continuous_logger.setLevel(logging.INFO)\n",
    "        self.continuous_log_buffer = deque(maxlen=100)\n",
    "        \n",
    "        # 添加持续训练的文件处理器\n",
    "        continuous_handler = DailyRotatingFileHandler(\n",
    "            base_dir=self.LOG_DIR,\n",
    "            prefix='continuous_training'\n",
    "        )\n",
    "        formatter = logging.Formatter(\n",
    "            '[%(asctime)s] %(levellevel)s: %(message)s',\n",
    "            datefmt='%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "        continuous_handler.setFormatter(formatter)\n",
    "        self.continuous_logger.addHandler(continuous_handler)\n",
    "        \n",
    "    def update_training_progress(self, model_idx: int, progress: float):\n",
    "        \"\"\"更新训练进度\"\"\"\n",
    "        if hasattr(self, 'display_manager'):\n",
    "            self.display_manager.progress_bars[model_idx] = progress\n",
    "            self.display_manager._display_logs()\n",
    "            \n",
    "    def add_training_log(self, message: str):\n",
    "        \"\"\"添加训练日志\"\"\"\n",
    "        if hasattr(self, 'continuous_log_buffer'):\n",
    "            self.continuous_log_buffer.append(message)\n",
    "            if hasattr(self, 'display_manager'):\n",
    "                self.display_manager.log_buffer.append(message)\n",
    "                self.display_manager._display_logs()\n",
    "\n",
    "    # 添加从cell1的ConfigValidator的功能\n",
    "    def validate_all_configs(self) -> Dict[str, bool]:\n",
    "        \"\"\"验证所有配置的有效性\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # 数据库配置验证\n",
    "        results['db_config'] = self._extended_validate_db_config()\n",
    "        \n",
    "        # 训练配置验证\n",
    "        results['training_config'] = self._extended_validate_training_config()\n",
    "        \n",
    "        # 系统配置验证\n",
    "        results['system_config'] = self._extended_validate_system_config()\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    def _extended_validate_db_config(self) -> bool:\n",
    "        \"\"\"扩展的数据库配置验证\"\"\"\n",
    "        try:\n",
    "            config = self.DB_CONFIG\n",
    "            \n",
    "            # 基本字段验证\n",
    "            required_fields = ['host', 'user', 'password', 'database', 'charset']\n",
    "            if not all(field in config for field in required_fields):\n",
    "                self.logger.error(\"数据库配置缺少必要字段\")\n",
    "                return False\n",
    "                \n",
    "            # 端口验证\n",
    "            if not isinstance(config.get('port', 3306), int):\n",
    "                self.logger.error(\"数据库端口必须是整数\")\n",
    "                return False\n",
    "                \n",
    "            # 字符集验证\n",
    "            if config.get('charset') not in ['utf8', 'utf8mb4']:\n",
    "                self.logger.error(\"不支持的字符集\")\n",
    "                return False\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"验证数据库配置时出错: {str(e)}\")\n",
    "            return False\n",
    "            \n",
    "    def _extended_validate_training_config(self) -> bool:\n",
    "        \"\"\"扩展的训练配置验证\"\"\"\n",
    "        try:\n",
    "            config = self.TRAINING_CONFIG\n",
    "            \n",
    "            # 参数范围验证\n",
    "            validations = [\n",
    "                config.get('batch_size', 0) > 0,\n",
    "                config.get('max_epochs', 0) > 0,\n",
    "                0 <= config.get('save_frequency', 100) <= 1000,\n",
    "                0 <= config.get('eval_frequency', 50) <= 500,\n",
    "                0 < config.get('min_improvement', 0.001) < 1\n",
    "            ]\n",
    "            \n",
    "            if not all(validations):\n",
    "                self.logger.error(\"训练配置参数范围无效\")\n",
    "                return False\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"验证训练配置时出错: {str(e)}\")\n",
    "            return False\n",
    "            \n",
    "    def _extended_validate_system_config(self) -> bool:\n",
    "        \"\"\"扩展的系统配置验证\"\"\"\n",
    "        try:\n",
    "            config = self.SYSTEM_CONFIG\n",
    "            \n",
    "            # 资源限制验证\n",
    "            if not all([\n",
    "                config.get('memory_limit', 0) >= 1000,\n",
    "                config.get('gpu_memory_limit', 0) >= 1000,\n",
    "                config.get('max_threads', 0) > 0\n",
    "            ]):\n",
    "                self.logger.error(\"系统资源限制配置无效\")\n",
    "                return False\n",
    "            \n",
    "            # 采样配置验证\n",
    "            sample_config = config.get('SAMPLE_CONFIG', {})\n",
    "            if not all([\n",
    "                sample_config.get('input_length', 0) > 0,\n",
    "                sample_config.get('target_length', 0) > 0\n",
    "            ]):\n",
    "                self.logger.error(\"采样配置无效\")\n",
    "                return False\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"验证系统配置时出错: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def export_configs(self, export_path: str = None) -> bool:\n",
    "        \"\"\"导出所有配置\"\"\"\n",
    "        if export_path is None:\n",
    "            export_path = os.path.join(self.BASE_DIR, 'configs', 'all_configs.json')\n",
    "            \n",
    "        try:\n",
    "            configs = {\n",
    "                'DB_CONFIG': self.DB_CONFIG,\n",
    "                'TRAINING_CONFIG': self.TRAINING_CONFIG,\n",
    "                'SYSTEM_CONFIG': self.SYSTEM_CONFIG,\n",
    "                'OPTUNA_CONFIG': self.OPTUNA_CONFIG\n",
    "            }\n",
    "            \n",
    "            os.makedirs(os.path.dirname(export_path), exist_ok=True)\n",
    "            with open(export_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(configs, f, indent=4, ensure_ascii=False)\n",
    "                \n",
    "            self.logger.info(f\"配置已导出到: {export_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"导出配置失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _create_custom_formatter(self):\n",
    "        \"\"\"创建自定义格式化器\"\"\"\n",
    "        return CustomFormatter()\n",
    "\n",
    "    def _create_progress_handler(self):\n",
    "        \"\"\"创建进度处理器\"\"\"\n",
    "        return ProgressHandler()\n",
    "\n",
    "    # 从cell4的LoggingManager类补充\n",
    "    def _configure_logging(self):\n",
    "        \"\"\"配置日志系统\"\"\"\n",
    "        # 配置根日志记录器\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s [%(levelname)s] %(name)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(os.path.join(self.LOG_DIR, \"system.log\")),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        # 捕获警告信息\n",
    "        logging.captureWarnings(True)\n",
    "\n",
    "    # 从cell4的LogManager类补充\n",
    "    def get_logger(self):\n",
    "        \"\"\"获取logger实例\"\"\"\n",
    "        return self.logger\n",
    "\n",
    "    def get_continuous_logger(self):\n",
    "        \"\"\"获取持续训练的logger实例\"\"\"\n",
    "        if not hasattr(self, 'continuous_logger'):\n",
    "            self.setup_continuous_logging()\n",
    "        return self.continuous_logger\n",
    "\n",
    "    # 从cell1的ConfigManager补充\n",
    "    def get_config(self, config_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"获取指定配置\n",
    "        Args:\n",
    "            config_name: 配置名称 ('DB', 'TRAINING', 'SYSTEM', 'OPTUNA')\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return getattr(self, f'{config_name}_CONFIG').copy()\n",
    "        except AttributeError:\n",
    "            self.logger.error(f\"配置 {config_name} 不存在\")\n",
    "            return {}\n",
    "\n",
    "    def clear_log_buffers(self):\n",
    "        \"\"\"清理日志缓冲区\"\"\"\n",
    "        if hasattr(self, 'continuous_log_buffer'):\n",
    "            self.continuous_log_buffer.clear()\n",
    "        if hasattr(self, 'display_manager'):\n",
    "            self.display_manager.log_buffer.clear()\n",
    "            self.display_manager._display_logs()\n",
    "\n",
    "    def cleanup_logs(self, days: int = None):\n",
    "        \"\"\"清理旧日志文件\n",
    "        Args:\n",
    "            days: 保留天数，默认使用配置中的值\n",
    "        \"\"\"\n",
    "        if days is None:\n",
    "            days = self.SYSTEM_CONFIG.get('log_retention_days', 7)\n",
    "            \n",
    "        try:\n",
    "            current_time = datetime.now()\n",
    "            for file in os.listdir(self.LOG_DIR):\n",
    "                if file.endswith('.log'):\n",
    "                    file_path = os.path.join(self.LOG_DIR, file)\n",
    "                    file_time = datetime.fromtimestamp(os.path.getctime(file_path))\n",
    "                    if (current_time - file_time).days > days:\n",
    "                        os.remove(file_path)\n",
    "                        self.logger.info(f\"已删除旧日志文件: {file}\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"清理日志文件时出错: {str(e)}\")\n",
    "\n",
    "    def _configure_metrics(self):\n",
    "        \"\"\"配置性能指标记录\"\"\"\n",
    "        self.metrics = {\n",
    "            'training_loss': deque(maxlen=1000),\n",
    "            'validation_loss': deque(maxlen=1000),\n",
    "            'learning_rates': deque(maxlen=1000),\n",
    "            'batch_times': deque(maxlen=100)\n",
    "        }\n",
    "\n",
    "    def log_metric(self, metric_name: str, value: float):\n",
    "        \"\"\"记录性能指标\"\"\"\n",
    "        if hasattr(self, 'metrics') and metric_name in self.metrics:\n",
    "            self.metrics[metric_name].append(value)\n",
    "\n",
    "    def get_metrics_summary(self):\n",
    "        \"\"\"获取性能指标摘要\"\"\"\n",
    "        if not hasattr(self, 'metrics'):\n",
    "            return {}\n",
    "            \n",
    "        return {\n",
    "            name: {\n",
    "                'mean': np.mean(values),\n",
    "                'min': np.min(values),\n",
    "                'max': np.max(values)\n",
    "            }\n",
    "            for name, values in self.metrics.items()\n",
    "            if values\n",
    "        }\n",
    "\n",
    "    def _test_config_system(self):\n",
    "        \"\"\"测试配置系统完整性\"\"\"\n",
    "        try:\n",
    "            required_keys = ['host', 'port', 'user', 'password'] \n",
    "            assert all(key in self.DB_CONFIG for key in required_keys), \"数据库配置缺失必要参数\"\n",
    "            self.logger.info(\"配置系统测试通过\")\n",
    "        except AssertionError as e:\n",
    "            self.logger.error(f\"配置系统测试失败: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def validate_config_values(self, config: dict) -> bool:\n",
    "        \"\"\"验证配置参数值 (from ConfigValidator)\"\"\"\n",
    "        valid = True\n",
    "        for key, rule in self.validation_rules.items():\n",
    "            if key in config:\n",
    "                if not rule(config[key]):\n",
    "                    self.logger.warning(f\"参数 {key} 的值 {config[key]} 无效\")\n",
    "                    valid = False\n",
    "        return valid\n",
    "\n",
    "class DailyRotatingFileHandler(logging.FileHandler):\n",
    "    \"\"\"每日自动分文件的日志处理器\"\"\"\n",
    "    def __init__(self, base_dir, prefix='log', max_bytes=50*1024*1024):\n",
    "        self.base_dir = base_dir\n",
    "        self.prefix = prefix\n",
    "        self.max_bytes = max_bytes\n",
    "        self.current_date = None\n",
    "        self.current_file = None\n",
    "        self.current_size = 0\n",
    "        self.file_count = 1\n",
    "        \n",
    "        os.makedirs(base_dir, exist_ok=True)\n",
    "        self._init_file()\n",
    "        super().__init__(self.current_file, mode='a', encoding='utf-8')\n",
    "    \n",
    "    def _init_file(self):\n",
    "        \"\"\"初始化日志文件\"\"\"\n",
    "        self.current_file = self._get_file_path()\n",
    "        self.current_date = datetime.now().strftime('%Y%m%d')\n",
    "        if os.path.exists(self.current_file):\n",
    "            self.current_size = os.path.getsize(self.current_file)\n",
    "        else:\n",
    "            self.current_size = 0\n",
    "    \n",
    "    def _get_file_path(self):\n",
    "        \"\"\"获取当前日志文件路径\"\"\"\n",
    "        today = datetime.now().strftime('%Y%m%d')\n",
    "        if self.current_size >= self.max_bytes:\n",
    "            self.file_count += 1\n",
    "            return os.path.join(self.base_dir, f'{self.prefix}_{today}_{self.file_count}.log')\n",
    "        elif today != self.current_date:\n",
    "            self.file_count = 1\n",
    "            self.current_date = today\n",
    "            return os.path.join(self.base_dir, f'{self.prefix}_{today}.log')\n",
    "        return self.current_file\n",
    "    \n",
    "    def emit(self, record):\n",
    "        \"\"\"重写emit方法，在写入日志前检查文件状态\"\"\"\n",
    "        try:\n",
    "            new_file = self._get_file_path()\n",
    "            if new_file != self.current_file:\n",
    "                if self.stream:\n",
    "                    self.stream.close()\n",
    "                self.current_file = new_file\n",
    "                self.baseFilename = new_file\n",
    "                self.current_size = 0\n",
    "                self.stream = self._open()\n",
    "            \n",
    "            msg = self.format(record) + '\\n'\n",
    "            self.stream.write(msg)\n",
    "            self.stream.flush()\n",
    "            self.current_size += len(msg.encode('utf-8'))\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.handleError(record)\n",
    "\n",
    "class ProgressHandler(logging.Handler):\n",
    "    \"\"\"进度条处理器\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.progress = 0\n",
    "        \n",
    "    def emit(self, record):\n",
    "        if hasattr(record, 'progress'):\n",
    "            print('\\r' + ' ' * 80, end='\\r')  # 清除当前行\n",
    "            progress = int(record.progress * 50)\n",
    "            print(f'\\rTraining Progress: [{\"=\"*progress}{\" \"*(50-progress)}] {record.progress*100:.1f}%', end='')\n",
    "\n",
    "class CustomFormatter(logging.Formatter):\n",
    "    \"\"\"自定义日志格式化器\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.formatters = {\n",
    "            logging.DEBUG: logging.Formatter(\n",
    "                '[%(asctime)s] %(name)s - %(levelname)s - %(message)s'\n",
    "            ),\n",
    "            logging.INFO: logging.Formatter(\n",
    "                '[%(asctime)s] %(levelname)s - %(message)s'\n",
    "            ),\n",
    "            logging.WARNING: logging.Formatter(\n",
    "                '[%(asctime)s] WARNING - %(message)s'\n",
    "            ),\n",
    "            logging.ERROR: logging.Formatter(\n",
    "                '[%(asctime)s] ERROR - %(message)s\\n\\tat %(pathname)s:%(lineno)d'\n",
    "            ),\n",
    "            logging.CRITICAL: logging.Formatter(\n",
    "                '[%(asctime)s] CRITICAL - %(message)s\\n\\tat %(pathname)s:%(lineno)d\\n%(exc_info)s'\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    def format(self, record):\n",
    "        formatter = self.formatters.get(record.levelno)\n",
    "        return formatter.format(record)\n",
    "\n",
    "class LogDisplayManager:\n",
    "    \"\"\"日志显示管理器\"\"\"\n",
    "    def __init__(self, max_lines=10):\n",
    "        self.max_lines = max_lines\n",
    "        self.log_buffer = []\n",
    "        self.progress_bars = {i: 0.0 for i in range(6)}\n",
    "        self._clear_output()\n",
    "    \n",
    "    def _clear_output(self):\n",
    "        \"\"\"清空输出\"\"\"\n",
    "        clear_output(wait=True)\n",
    "    \n",
    "    def _display_logs(self):\n",
    "        \"\"\"显示日志\"\"\"\n",
    "        self._clear_output()\n",
    "        \n",
    "        start_idx = max(0, len(self.log_buffer) - self.max_lines)\n",
    "        for log in self.log_buffer[start_idx:]:\n",
    "            if log.strip():\n",
    "                print(log)\n",
    "        \n",
    "        print('-' * 80)\n",
    "        \n",
    "        for model_idx, progress in self.progress_bars.items():\n",
    "            bar_length = 50\n",
    "            filled = int(progress * bar_length)\n",
    "            bar = f\"Model {model_idx + 1}: [{'='*filled}{' '*(bar_length-filled)}] {progress*100:.1f}%\"\n",
    "            print(bar)\n",
    "\n",
    "# 创建全局实例\n",
    "core_manager = CoreManager()\n",
    "\n",
    "# 导出常用变量\n",
    "logger = core_manager.logger\n",
    "config = core_manager\n",
    "BASE_DIR = core_manager.BASE_DIR\n",
    "LOG_DIR = core_manager.LOG_DIR\n",
    "MODEL_DIR = core_manager.MODEL_DIR\n",
    "DATA_DIR = core_manager.DATA_DIR\n",
    "CHECKPOINT_DIR = core_manager.CHECKPOINT_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13c1fab7-c53f-4075-bd58-9cfe556be1cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#2 Utility Functions / 工具函数模块\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import psutil\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "import shutil\n",
    "\n",
    "# 获取logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DateUtils:\n",
    "    \"\"\"日期处理工具类\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_issue(issue_str: str) -> Tuple[str, int]:\n",
    "        \"\"\"解析期号字符串\"\"\"\n",
    "        match = re.match(r\"(\\d{8})-(\\d{4})\", issue_str)\n",
    "        if not match:\n",
    "            raise ValueError(\"无效的期号格式\")\n",
    "        return match.group(1), int(match.group(2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_next_issue(current_issue: str) -> str:\n",
    "        \"\"\"获取下一期号\"\"\"\n",
    "        date_str, period = DateUtils.parse_issue(current_issue)\n",
    "        date = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "        \n",
    "        if period == 1440:\n",
    "            new_date = date + timedelta(days=1)\n",
    "            new_period = 1\n",
    "        else:\n",
    "            new_date = date\n",
    "            new_period = period + 1\n",
    "        \n",
    "        return f\"{new_date.strftime('%Y%m%d')}-{new_period:04d}\"\n",
    "\n",
    "class MemoryManager:\n",
    "    \"\"\"内存管理工具类\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                warning_threshold_mb: int = 8000,\n",
    "                critical_threshold_mb: int = 10000,\n",
    "                cleanup_interval: int = 300,\n",
    "                full_cleanup_interval: int = 14400):\n",
    "        \"\"\"\n",
    "        初始化内存管理器\n",
    "        Args:\n",
    "            warning_threshold_mb: 警告阈值(MB)\n",
    "            critical_threshold_mb: 临界阈值(MB)\n",
    "            cleanup_interval: 常规清理间隔(秒)\n",
    "            full_cleanup_interval: 全面清理间隔(秒)\n",
    "        \"\"\"\n",
    "        self.warning_threshold = warning_threshold_mb * 1024 * 1024  # 转换为字节\n",
    "        self.critical_threshold = critical_threshold_mb * 1024 * 1024\n",
    "        self.cleanup_interval = cleanup_interval\n",
    "        self.full_cleanup_interval = full_cleanup_interval\n",
    "        \n",
    "        self.last_cleanup_time = time.time()\n",
    "        self.last_full_cleanup_time = time.time()\n",
    "        \n",
    "        logger.info(f\"内存管理器初始化完成 - 警告阈值:{warning_threshold_mb}MB, 临界阈值:{critical_threshold_mb}MB\")\n",
    "\n",
    "    def check_memory_status(self) -> bool:\n",
    "        \"\"\"\n",
    "        检查内存状态并在必要时执行清理\n",
    "        Returns:\n",
    "            bool: 内存状态是否正常\n",
    "        \"\"\"\n",
    "        try:\n",
    "            current_usage = self.get_memory_usage()\n",
    "            current_time = time.time()\n",
    "\n",
    "            # 检查是否需要执行清理\n",
    "            if current_time - self.last_cleanup_time > self.cleanup_interval:\n",
    "                self._regular_cleanup()\n",
    "                self.last_cleanup_time = current_time\n",
    "\n",
    "            if current_time - self.last_full_cleanup_time > self.full_cleanup_interval:\n",
    "                self._full_cleanup()\n",
    "                self.last_full_cleanup_time = current_time\n",
    "\n",
    "            # 检查内存使用是否超过阈值\n",
    "            if current_usage > self.critical_threshold:\n",
    "                logger.warning(f\"内存使用超过临界值: {current_usage/1024/1024:.1f}MB\")\n",
    "                self._emergency_cleanup()\n",
    "                return False\n",
    "            elif current_usage > self.warning_threshold:\n",
    "                logger.warning(f\"内存使用超过警告值: {current_usage/1024/1024:.1f}MB\")\n",
    "                self._optimize_memory()\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"检查内存状态时出错: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def get_memory_usage(self) -> int:\n",
    "        \"\"\"\n",
    "        获取当前进程的内存使用量(字节)\n",
    "        Returns:\n",
    "            int: 内存使用量(字节)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            process = psutil.Process(os.getpid())\n",
    "            return process.memory_info().rss\n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取内存使用量时出错: {str(e)}\")\n",
    "            return 0\n",
    "\n",
    "    def get_memory_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        获取详细的内存使用信息\n",
    "        Returns:\n",
    "            Dict: 内存使用信息\n",
    "        \"\"\"\n",
    "        try:\n",
    "            memory = psutil.virtual_memory()\n",
    "            process = psutil.Process(os.getpid())\n",
    "            \n",
    "            return {\n",
    "                'total': memory.total,\n",
    "                'available': memory.available,\n",
    "                'used': memory.used,\n",
    "                'free': memory.free,\n",
    "                'percent': memory.percent,\n",
    "                'process_usage': process.memory_info().rss,\n",
    "                'process_percent': process.memory_percent()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取内存信息时出错: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "    def _regular_cleanup(self):\n",
    "        \"\"\"执行常规清理\"\"\"\n",
    "        try:\n",
    "            # 1. 清理Python垃圾\n",
    "            gc.collect()\n",
    "            \n",
    "            # 2. 清理TF会话\n",
    "            tf.keras.backend.clear_session()\n",
    "            \n",
    "            # 3. 清理不用的变量\n",
    "            for name in list(globals().keys()):\n",
    "                if name.startswith('_temp_'):\n",
    "                    del globals()[name]\n",
    "                    \n",
    "            logger.info(\"完成常规内存清理\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"常规清理时出错: {str(e)}\")\n",
    "\n",
    "    def _full_cleanup(self):\n",
    "        \"\"\"执行全面清理\"\"\"\n",
    "        try:\n",
    "            # 1. 执行常规清理\n",
    "            self._regular_cleanup()\n",
    "            \n",
    "            # 2. 重置TensorFlow状态\n",
    "            gpus = tf.config.list_physical_devices('GPU')\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.reset_memory_stats(gpu.name)\n",
    "            \n",
    "            # 3. 清理模型缓存\n",
    "            self._cleanup_model_cache()\n",
    "            \n",
    "            logger.info(\"完成全面内存清理\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"全面清理时出错: {str(e)}\")\n",
    "\n",
    "    def _emergency_cleanup(self):\n",
    "        \"\"\"执行紧急清理\"\"\"\n",
    "        try:\n",
    "            # 三级清理策略\n",
    "            gc.collect(2)  # 强制回收老年代内存\n",
    "            tf.keras.backend.clear_session()\n",
    "            \n",
    "            # 释放多GPU内存\n",
    "            for gpu in tf.config.list_physical_devices('GPU'):\n",
    "                try:\n",
    "                    tf.config.experimental.reset_memory_stats(gpu.name)\n",
    "                except RuntimeError as e:\n",
    "                    logger.warning(f\"GPU内存重置失败: {str(e)}\")\n",
    "\n",
    "            # 清理临时文件缓存\n",
    "            self._clean_temp_files()\n",
    "            \n",
    "            logger.warning(\"执行紧急内存清理\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            logger.error(f\"运行时错误: {str(e)}\")\n",
    "        except MemoryError as e:\n",
    "            logger.critical(\"内存严重不足，无法完成清理！\")\n",
    "        except IOError as e:\n",
    "            logger.error(f\"文件清理失败: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"未预期的清理错误: {str(e)}\", exc_info=True)\n",
    "\n",
    "    def _clean_temp_files(self) -> None:\n",
    "        \"\"\"清理临时文件\"\"\"\n",
    "        try:\n",
    "            temp_dir = os.path.join(core_manager.BASE_DIR, 'temp')\n",
    "            if os.path.exists(temp_dir):\n",
    "                shutil.rmtree(temp_dir)\n",
    "                os.makedirs(temp_dir, exist_ok=True)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"临时文件清理失败: {str(e)}\")\n",
    "\n",
    "    def _optimize_memory(self):\n",
    "        \"\"\"优化内存使用\"\"\"\n",
    "        try:\n",
    "            # 1. 检查并清理大对象\n",
    "            for obj in gc.get_objects():\n",
    "                if hasattr(obj, 'nbytes') and getattr(obj, 'nbytes', 0) > 1e8:  # >100MB\n",
    "                    del obj\n",
    "            \n",
    "            # 2. 执行垃圾回收\n",
    "            gc.collect()\n",
    "            \n",
    "            logger.info(\"完成内存优化\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"内存优化时出错: {str(e)}\")\n",
    "\n",
    "    def _cleanup_model_cache(self):\n",
    "        \"\"\"清理模型缓存\"\"\"\n",
    "        try:\n",
    "            # 清理Keras后端缓存\n",
    "            tf.keras.backend.clear_session()\n",
    "            \n",
    "            # 清理模型检查点文件\n",
    "            checkpoint_dir = os.path.join(os.getcwd(), 'checkpoints')\n",
    "            if os.path.exists(checkpoint_dir):\n",
    "                for item in os.listdir(checkpoint_dir):\n",
    "                    if item.endswith('.temp'):\n",
    "                        os.remove(os.path.join(checkpoint_dir, item))\n",
    "                        \n",
    "            # 释放TensorFlow占用的缓存\n",
    "            gpus = tf.config.list_physical_devices('GPU')\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.reset_memory_stats(gpu.name)\n",
    "            \n",
    "            logger.info(\"完成模型缓存清理\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"清理模型缓存时出错: {str(e)}\")\n",
    "\n",
    "    def optimize_for_large_data(self):\n",
    "        \"\"\"针对大样本的优化策略\"\"\"\n",
    "        # 新增大样本优化策略\n",
    "        self.enable_memmap = True  # 启用内存映射\n",
    "        self.chunk_size = 10000    # 分块加载\n",
    "        tf.keras.backend.set_floatx('float16')  # 压缩精度\n",
    "        logger.info(\"已启用大数据优化策略\")\n",
    "\n",
    "    def optimize_for_hardware(self) -> bool:\n",
    "        \"\"\"硬件定制优化\"\"\"\n",
    "        try:\n",
    "            # 1. 限制TensorFlow内存使用\n",
    "            gpus = tf.config.list_physical_devices('GPU')\n",
    "            if gpus:\n",
    "                tf.config.set_logical_device_configuration(\n",
    "                    gpus[0],\n",
    "                    [tf.config.LogicalDeviceConfiguration(memory_limit=1536)]\n",
    "                )\n",
    "            \n",
    "            # 2. 配置CPU并行线程\n",
    "            tf.config.threading.set_intra_op_parallelism_threads(6)\n",
    "            tf.config.threading.set_inter_op_parallelism_threads(4)\n",
    "            \n",
    "            # 3. 启用内存映射\n",
    "            self.enable_memmap = True\n",
    "            logger.info(\"已完成硬件优化配置\")\n",
    "            return True\n",
    "        except RuntimeError as e:\n",
    "            logger.error(f\"运行时配置错误: {str(e)}\")\n",
    "            return False\n",
    "        except ValueError as e:\n",
    "            logger.error(f\"无效的配置参数: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "# 创建全局实例\n",
    "date_utils = DateUtils()\n",
    "memory_manager = MemoryManager()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a6864b-e443-44ca-acf1-b3ef4c1fab67",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#3 System Monitor / 系统监控模块\n",
    "import psutil\n",
    "import logging\n",
    "import threading\n",
    "import time\n",
    "import gc\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "\n",
    "# 获取logger实例\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 从cell1移入的日志处理器类\n",
    "class DailyRotatingFileHandler(logging.FileHandler):\n",
    "    \"\"\"每日自动分文件的日志处理器\"\"\"\n",
    "    def __init__(self, base_dir, prefix='log', max_bytes=50*1024*1024):\n",
    "        self.base_dir = base_dir \n",
    "        self.prefix = prefix\n",
    "        self.max_bytes = max_bytes\n",
    "        self.current_date = None\n",
    "        self.current_file = None\n",
    "        self.current_size = 0\n",
    "        self.file_count = 1\n",
    "        \n",
    "        os.makedirs(base_dir, exist_ok=True)\n",
    "        self._init_file()\n",
    "        super().__init__(self.current_file, mode='a', encoding='utf-8')\n",
    "        \n",
    "    def _init_file(self):\n",
    "        \"\"\"初始化日志文件\"\"\"\n",
    "        self.current_file = self._get_file_path()\n",
    "        self.current_date = datetime.now().strftime('%Y%m%d')\n",
    "        if os.path.exists(self.current_file):\n",
    "            self.current_size = os.path.getsize(self.current_file)\n",
    "        else:\n",
    "            self.current_size = 0\n",
    "            \n",
    "    def _get_file_path(self):\n",
    "        \"\"\"获取当前日志文件路径\"\"\"\n",
    "        today = datetime.now().strftime('%Y%m%d')\n",
    "        if self.current_size >= self.max_bytes:\n",
    "            self.file_count += 1\n",
    "            return os.path.join(self.base_dir, f'{self.prefix}_{today}_{self.file_count}.log')\n",
    "        elif today != self.current_date:\n",
    "            self.file_count = 1\n",
    "            self.current_date = today\n",
    "            return os.path.join(self.base_dir, f'{self.prefix}_{today}.log')\n",
    "        return self.current_file\n",
    "        \n",
    "    def emit(self, record):\n",
    "        \"\"\"重写emit方法,在写入日志前检查文件状态\"\"\"\n",
    "        try:\n",
    "            new_file = self._get_file_path()\n",
    "            if new_file != self.current_file:\n",
    "                if self.stream:\n",
    "                    self.stream.close()\n",
    "                self.current_file = new_file\n",
    "                self.baseFilename = new_file\n",
    "                self.current_size = 0\n",
    "                self.stream = self._open()\n",
    "            \n",
    "            msg = self.format(record) + '\\n'\n",
    "            self.stream.write(msg)\n",
    "            self.stream.flush()\n",
    "            self.current_size += len(msg.encode('utf-8'))\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.handleError(record)\n",
    "\n",
    "class ProgressHandler(logging.Handler):\n",
    "    \"\"\"进度条处理器\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.progress = 0\n",
    "        \n",
    "    def emit(self, record):\n",
    "        if hasattr(record, 'progress'):\n",
    "            print('\\r' + ' ' * 80, end='\\r')  # 清除当前行\n",
    "            progress = int(record.progress * 50)\n",
    "            print(f'\\rTraining Progress: [{\"=\"*progress}{\" \"*(50-progress)}] {record.progress*100:.1f}%', end='')\n",
    "\n",
    "class CustomFormatter(logging.Formatter):\n",
    "    \"\"\"自定义日志格式化器\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.formatters = {\n",
    "            logging.DEBUG: logging.Formatter(\n",
    "                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "            ),\n",
    "            logging.INFO: logging.Formatter(\n",
    "                '%(asctime)s - %(levelname)s - %(message)s'\n",
    "            ),\n",
    "            logging.WARNING: logging.Formatter(\n",
    "                '%(asctime)s - %(levelname)s - WARNING: %(message)s'\n",
    "            ),\n",
    "            logging.ERROR: logging.Formatter(\n",
    "                '%(asctime)s - %(levelname)s - ERROR: %(message)s\\n%(pathname)s:%(lineno)d'\n",
    "            ),\n",
    "            logging.CRITICAL: logging.Formatter(\n",
    "                '%(asctime)s - %(levelname)s - CRITICAL: %(message)s\\n%(pathname)s:%(lineno)d\\n%(exc_info)s'\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    def format(self, record):\n",
    "        formatter = self.formatters.get(record.levelno)\n",
    "        return formatter.format(record)\n",
    "\n",
    "class ResourceMonitor:\n",
    "    \"\"\"资源监控器 - 负责监控CPU、内存、GPU等资源使用情况\"\"\"\n",
    "    def __init__(self, window_size=100, check_interval=5):\n",
    "        self.window_size = window_size\n",
    "        self.check_interval = check_interval\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "        # 监控指标存储\n",
    "        self.metrics = {\n",
    "            'cpu_usage': deque(maxlen=window_size),\n",
    "            'memory_usage': deque(maxlen=window_size),\n",
    "            'disk_usage': deque(maxlen=window_size),\n",
    "            'gpu_usage': None,\n",
    "            'gpu_memory': None\n",
    "        }\n",
    "        \n",
    "        # 添加缺失的警报阈值初始化\n",
    "        self.thresholds = {\n",
    "            'cpu_usage': 90,    # CPU使用率超过90%\n",
    "            'memory_usage': 90,  # 内存使用率超过90%\n",
    "            'disk_usage': 90,    # 磁盘使用率超过90%\n",
    "            'gpu_usage': 90,     # GPU使用率超过90%\n",
    "            'gpu_memory': 90     # GPU内存使用率超过90%\n",
    "        }\n",
    "        \n",
    "        # 添加警报历史\n",
    "        self.alerts = []\n",
    "        \n",
    "        # 确保基础属性初始化\n",
    "        self._memory_usage = 0.0\n",
    "        self.cpu_usage = 0.0\n",
    "        self.gpu_usage = 0.0\n",
    "\n",
    "        # 添加线程控制\n",
    "        self._running = False\n",
    "        self._thread = threading.Thread(target=self._monitor_loop)\n",
    "\n",
    "        # 确保初始化时收集初始指标\n",
    "        self._monitor_loop()  # 添加初始数据收集\n",
    "        self.start()  # 启动监控线程\n",
    "\n",
    "    def _monitor_loop(self):\n",
    "        \"\"\"监控循环的完整实现\"\"\"\n",
    "        while self._running:\n",
    "            try:\n",
    "                self._collect_metrics()\n",
    "                self._check_alerts()\n",
    "                time.sleep(self.check_interval)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"资源监控循环出错: {str(e)}\")\n",
    "                \n",
    "    def _check_alerts(self):\n",
    "        \"\"\"完整的警报检查逻辑\"\"\"\n",
    "        with self.lock:\n",
    "            for metric, values in self.metrics.items():\n",
    "                if not values:\n",
    "                    continue\n",
    "                current = values[-1]\n",
    "                if current > self.thresholds[metric]:\n",
    "                    alert = {\n",
    "                        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        'metric': metric,\n",
    "                        'value': current,\n",
    "                        'threshold': self.thresholds[metric]\n",
    "                    }\n",
    "                    self.alerts.append(alert)\n",
    "                    logger.warning(f\"资源警报: {metric} = {current}% (阈值: {self.thresholds[metric]}%)\")\n",
    "\n",
    "    def _collect_metrics(self):\n",
    "        \"\"\"收集资源指标\"\"\"\n",
    "        try:\n",
    "            with self.lock:\n",
    "                # CPU使用率\n",
    "                self.cpu_usage = psutil.cpu_percent()\n",
    "                self.metrics['cpu_usage'].append(self.cpu_usage)\n",
    "                \n",
    "                # 内存使用率\n",
    "                mem = psutil.virtual_memory()\n",
    "                self._memory_usage = mem.percent\n",
    "                self.metrics['memory_usage'].append(self._memory_usage)\n",
    "                \n",
    "                # 磁盘使用率\n",
    "                disk = psutil.disk_usage('/')\n",
    "                self.metrics['disk_usage'].append(disk.percent)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"收集资源指标时出错: {str(e)}\")\n",
    "\n",
    "    def get_memory_usage(self):\n",
    "        \"\"\"获取当前内存使用率\"\"\"\n",
    "        return self._memory_usage\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"启动资源监控\"\"\"\n",
    "        if not self._running:\n",
    "            self._running = True\n",
    "            self._thread.start()\n",
    "            logger.info(\"资源监控已启动\")\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"停止资源监控\"\"\"\n",
    "        self._running = False\n",
    "        if self._thread.is_alive():\n",
    "            self._thread.join(timeout=5)\n",
    "        logger.info(\"资源监控已停止\")\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"性能监控器 - 负责收集和分析性能指标\"\"\"\n",
    "    def __init__(self, save_dir='logs/performance', window_size=1000):\n",
    "        self.save_dir = save_dir\n",
    "        self.window_size = window_size\n",
    "        self.metrics = {\n",
    "            'cpu_usage': deque(maxlen=window_size),\n",
    "            'memory_usage': deque(maxlen=window_size),\n",
    "            'gpu_usage': deque(maxlen=window_size),\n",
    "            'loss': deque(maxlen=window_size),\n",
    "            'accuracy': deque(maxlen=window_size)\n",
    "        }\n",
    "        self._running = False\n",
    "        self._thread = threading.Thread(target=self._monitor_loop)\n",
    "        \n",
    "        # 添加内存监控器初始化\n",
    "        from cell2_utils import memory_manager\n",
    "        self.memory_monitor = memory_manager\n",
    "\n",
    "    def _monitor_loop(self):\n",
    "        \"\"\"性能监控主循环\"\"\"\n",
    "        while self._running:\n",
    "            try:\n",
    "                # 收集CPU使用率\n",
    "                cpu_usage = psutil.cpu_percent()\n",
    "                self.metrics['cpu_usage'].append(cpu_usage)\n",
    "                \n",
    "                # 收集内存使用率\n",
    "                mem_usage = psutil.virtual_memory().percent\n",
    "                self.metrics['memory_usage'].append(mem_usage)\n",
    "                \n",
    "                # 收集GPU使用率（需要安装GPU监控库）\n",
    "                gpu_usage = 0  # 这里需要根据实际GPU监控库实现\n",
    "                self.metrics['gpu_usage'].append(gpu_usage)\n",
    "                \n",
    "                # 保存指标到文件\n",
    "                self._save_metrics()\n",
    "                \n",
    "                time.sleep(1)  # 每秒收集一次\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"性能监控出错: {str(e)}\")\n",
    "                break\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"启动监控线程\"\"\"\n",
    "        if not self._running:\n",
    "            self._running = True\n",
    "            self._thread.start()\n",
    "            logger.info(\"性能监控已启动\")\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"停止监控线程\"\"\"\n",
    "        self._running = False\n",
    "        if self._thread.is_alive():\n",
    "            self._thread.join(timeout=5)\n",
    "        logger.info(\"性能监控已停止\")\n",
    "\n",
    "    def _save_metrics(self):\n",
    "        \"\"\"保存指标到文件\"\"\"\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = os.path.join(self.save_dir, f\"metrics_{timestamp}.json\")\n",
    "            \n",
    "            metrics_to_save = {\n",
    "                k: list(v) for k, v in self.metrics.items()\n",
    "            }\n",
    "            \n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(metrics_to_save, f)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"保存性能指标失败: {str(e)}\")\n",
    "\n",
    "class SystemManager:\n",
    "    \"\"\"系统管理器 - 单例模式\"\"\"\n",
    "    _instance = None\n",
    "    \n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self):\n",
    "        if not hasattr(self, 'initialized'):\n",
    "            # 初始化各个监控器\n",
    "            self.resource_monitor = ResourceMonitor()\n",
    "            self.performance_monitor = PerformanceMonitor(save_dir='logs/performance')\n",
    "            \n",
    "            # 初始化其他组件\n",
    "            self.memory_monitor = MemoryMonitor()\n",
    "            self.system_cleaner = SystemCleaner()\n",
    "            \n",
    "            # 添加缺失的阈值初始化\n",
    "            self.memory_warning_threshold = 0.85  # 85%内存使用率警告\n",
    "            self.memory_critical_threshold = 0.95  # 95%内存使用率危险\n",
    "            self.cpu_warning_threshold = 0.85  # 85% CPU使用率警告\n",
    "            \n",
    "            # 添加缺失的状态初始化\n",
    "            self.last_cleanup_time = time.time()\n",
    "            self.cleanup_interval = 300  # 5分钟执行一次清理\n",
    "            self.compatibility_checked = False\n",
    "            \n",
    "            # 添加系统状态监控配置\n",
    "            self.status_check_interval = 60  # 60秒检查一次系统状态\n",
    "            self.last_status_check = time.time()\n",
    "            self.status_history = deque(maxlen=1000)  # 存储最近1000次状态检查结果\n",
    "            \n",
    "            # 添加资源监控配置\n",
    "            self.resource_warning_count = 0\n",
    "            self.max_warning_threshold = 5  # 最大警告次数，超过后采取行动\n",
    "            \n",
    "            # 添加日志配置\n",
    "            self.log_dir = 'logs/system'\n",
    "            os.makedirs(self.log_dir, exist_ok=True)\n",
    "            \n",
    "            # 添加系统恢复机制\n",
    "            self.recovery_attempts = 0\n",
    "            self.max_recovery_attempts = 3\n",
    "            \n",
    "            # 启动基础监控\n",
    "            self.start_basic_monitoring()\n",
    "            \n",
    "            self.initialized = True\n",
    "            \n",
    "            logger.info(\"系统管理器初始化完成\")\n",
    "\n",
    "    def check_system_compatibility(self):\n",
    "        \"\"\"检查系统兼容性\"\"\"\n",
    "        try:\n",
    "            compatibility = {\n",
    "                'python_version': sys.version,\n",
    "                'tensorflow_version': tf.__version__,\n",
    "                'gpu_available': bool(tf.config.list_physical_devices('GPU')),\n",
    "                'memory_sufficient': psutil.virtual_memory().total >= 8 * (1024 ** 3),  # 最少8GB内存\n",
    "                'disk_sufficient': psutil.disk_usage('/').free >= 10 * (1024 ** 3)  # 最少10GB可用空间\n",
    "            }\n",
    "            \n",
    "            self.compatibility_checked = True\n",
    "            return compatibility\n",
    "        except Exception as e:\n",
    "            logger.error(f\"检查系统兼容性时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def get_system_status(self):\n",
    "        \"\"\"获取完整的系统状态报告\"\"\"\n",
    "        try:\n",
    "            status = {\n",
    "                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'system_metrics': self.get_system_metrics(),\n",
    "                'performance_metrics': self.performance_monitor.get_summary(),\n",
    "                'memory_status': self.memory_monitor.check_memory(),\n",
    "                'compatibility': self.check_system_compatibility() if not self.compatibility_checked else None\n",
    "            }\n",
    "            return status\n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取系统状态报告时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def check_system_health(self):\n",
    "        \"\"\"检查系统健康状态\"\"\"\n",
    "        try:\n",
    "            # 检查内存使用\n",
    "            memory_info = self.memory_monitor.check_memory()\n",
    "            if not memory_info['healthy']:\n",
    "                self.handle_memory_warning(memory_info)\n",
    "            \n",
    "            # 检查系统状态\n",
    "            system_status = self.get_system_metrics()\n",
    "            if any(status > self.cpu_warning_threshold for status in [\n",
    "                system_status.get('cpu', 0),\n",
    "                system_status.get('memory', 0),\n",
    "                system_status.get('disk', 0)\n",
    "            ]):\n",
    "                self.handle_system_warning(system_status)\n",
    "            \n",
    "            # 定期清理\n",
    "            self._perform_periodic_cleanup()\n",
    "            \n",
    "            return memory_info['healthy'] and all(\n",
    "                status < self.cpu_warning_threshold for status in [\n",
    "                    system_status.get('cpu', 0),\n",
    "                    system_status.get('memory', 0),\n",
    "                    system_status.get('disk', 0)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"检查系统健康状态时出错: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def handle_memory_warning(self, memory_info):\n",
    "        \"\"\"处理内存警告\"\"\"\n",
    "        try:\n",
    "            if memory_info['usage_percent'] > self.memory_critical_threshold:\n",
    "                logger.critical(\"内存使用率超过临界值，执行紧急清理\")\n",
    "                self.system_cleaner._emergency_cleanup()\n",
    "            elif memory_info['usage_percent'] > self.memory_warning_threshold:\n",
    "                logger.warning(\"内存使用率较高，执行常规清理\")\n",
    "                self.system_cleaner._regular_cleanup()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"处理内存警告时出错: {str(e)}\")\n",
    "\n",
    "    def handle_system_warning(self, status):\n",
    "        \"\"\"处理系统警告\"\"\"\n",
    "        try:\n",
    "            if status.get('cpu', 0) > self.cpu_warning_threshold:\n",
    "                logger.warning(f\"CPU使用率过高: {status['cpu']}%\")\n",
    "            if status.get('memory', 0) > self.memory_warning_threshold:\n",
    "                logger.warning(f\"内存使用率过高: {status['memory']}%\")\n",
    "            if status.get('disk', 0) > 90:  # 磁盘空间阈值固定为90%\n",
    "                logger.warning(f\"磁盘使用率过高: {status['disk']}%\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"处理系统警告时出错: {str(e)}\")\n",
    "\n",
    "    def check_dependencies(self):\n",
    "        \"\"\"系统启动时自动调用\"\"\"\n",
    "        try:\n",
    "            from ..notebooks.Untitled import check_requirements\n",
    "            need_install = check_requirements(requirements)\n",
    "            if need_install:\n",
    "                self.install_dependencies(need_install)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"检查依赖时出错: {str(e)}\")\n",
    "\n",
    "    def install_dependencies(self, packages):\n",
    "        \"\"\"受控安装方法\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"自动安装依赖: {packages}\")\n",
    "            # 这里可以添加实际的包安装逻辑\n",
    "            # pip.main(['install'] + packages)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"安装依赖时出错: {str(e)}\")\n",
    "\n",
    "    def get_system_metrics(self):\n",
    "        \"\"\"获取系统指标\"\"\"\n",
    "        try:\n",
    "            return {\n",
    "                'memory': self.memory_monitor.check_memory(),\n",
    "                'cpu': psutil.cpu_percent(),\n",
    "                'disk': psutil.disk_usage('/').percent,\n",
    "                'gpu': self._get_gpu_metrics()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取系统指标时出错: {str(e)}\")\n",
    "            return {}\n",
    "            \n",
    "    def _get_gpu_metrics(self):\n",
    "        \"\"\"获取GPU指标\"\"\"\n",
    "        try:\n",
    "            if not tf.config.list_physical_devices('GPU'):\n",
    "                return None\n",
    "                \n",
    "            result = subprocess.check_output(\n",
    "                ['nvidia-smi', '--query-gpu=memory.used,memory.total,temperature.gpu', \n",
    "                 '--format=csv,nounits,noheader'],\n",
    "                encoding='utf-8'\n",
    "            )\n",
    "            used, total, temp = map(int, result.strip().split(','))\n",
    "            return {\n",
    "                'memory_used': used,\n",
    "                'memory_total': total,\n",
    "                'temperature': temp,\n",
    "                'utilization': used / total * 100\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取GPU指标时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _perform_periodic_cleanup(self):\n",
    "        \"\"\"执行定期清理\"\"\"\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_cleanup_time > self.cleanup_interval:\n",
    "            try:\n",
    "                self.system_cleaner.check_and_cleanup()\n",
    "                self.last_cleanup_time = current_time\n",
    "            except Exception as e:\n",
    "                logger.error(f\"执行定期清理时出错: {str(e)}\")\n",
    "\n",
    "    def analyze_system_performance(self):\n",
    "        \"\"\"分析系统整体性能\"\"\"\n",
    "        try:\n",
    "            perf_summary = self.performance_monitor.get_summary()\n",
    "            sys_metrics = self.get_system_metrics()\n",
    "            \n",
    "            analysis = {\n",
    "                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'performance_status': perf_summary,\n",
    "                'system_status': sys_metrics,\n",
    "                'health_check': self.check_system_health(),\n",
    "                'recommendations': self._generate_recommendations(perf_summary, sys_metrics)\n",
    "            }\n",
    "            \n",
    "            return analysis\n",
    "        except Exception as e:\n",
    "            logger.error(f\"分析系统性能时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _generate_recommendations(self, perf_summary, sys_metrics):\n",
    "        \"\"\"生成系统优化建议\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # 检查内存使用\n",
    "        if sys_metrics.get('memory', 0) > self.memory_warning_threshold:\n",
    "            recommendations.append(\"建议清理内存或增加内存容量\")\n",
    "            \n",
    "        # 检查CPU使用\n",
    "        if sys_metrics.get('cpu', 0) > self.cpu_warning_threshold:\n",
    "            recommendations.append(\"建议优化计算密集型任务或增加CPU资源\")\n",
    "            \n",
    "        # 检查GPU使用\n",
    "        gpu_metrics = sys_metrics.get('gpu')\n",
    "        if gpu_metrics and gpu_metrics.get('utilization', 0) > 90:\n",
    "            recommendations.append(\"建议优化GPU使用效率或考虑增加GPU资源\")\n",
    "            \n",
    "        return recommendations\n",
    "\n",
    "    def _handle_resource_warnings(self):\n",
    "        \"\"\"处理资源警告的升级机制\"\"\"\n",
    "        self.resource_warning_count += 1\n",
    "        if self.resource_warning_count >= self.max_warning_threshold:\n",
    "            logger.critical(\"资源警告次数过多，执行紧急清理\")\n",
    "            self.system_cleaner._emergency_cleanup()\n",
    "            self.resource_warning_count = 0\n",
    "\n",
    "    def _perform_system_recovery(self):\n",
    "        \"\"\"系统恢复机制\"\"\"\n",
    "        try:\n",
    "            if self.recovery_attempts >= self.max_recovery_attempts:\n",
    "                logger.critical(\"系统恢复次数超过限制，需要人工干预\")\n",
    "                return False\n",
    "                \n",
    "            logger.warning(f\"尝试系统恢复，第{self.recovery_attempts + 1}次\")\n",
    "            \n",
    "            # 执行恢复步骤\n",
    "            self.system_cleaner._emergency_cleanup()\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "            \n",
    "            self.recovery_attempts += 1\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"执行系统恢复时出错: {str(e)}\")\n",
    "            return False\n",
    "            \n",
    "    def reset_recovery_count(self):\n",
    "        \"\"\"重置恢复计数\"\"\"\n",
    "        self.recovery_attempts = 0\n",
    "\n",
    "    def get_system_summary(self):\n",
    "        \"\"\"获取系统综合报告\"\"\"\n",
    "        try:\n",
    "            return {\n",
    "                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'system_health': self.check_system_health(),\n",
    "                'performance_metrics': self.performance_monitor.get_summary(),\n",
    "                'resource_metrics': self.resource_monitor.metrics,\n",
    "                'memory_status': self.memory_monitor.check_memory(),\n",
    "                'recovery_attempts': self.recovery_attempts,\n",
    "                'last_cleanup': datetime.fromtimestamp(self.last_cleanup_time).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'warnings_count': self.resource_warning_count\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取系统综合报告时出错: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "    def _test_config_system(self):\n",
    "        \"\"\"测试配置系统完整性\"\"\"\n",
    "        try:\n",
    "            required_keys = ['host', 'port', 'user', 'password'] \n",
    "            assert all(key in self.DB_CONFIG for key in required_keys), \"数据库配置缺失必要参数\"\n",
    "            logger.info(\"配置系统测试通过\")\n",
    "        except AssertionError as e:\n",
    "            logger.error(f\"配置系统测试失败: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def start_basic_monitoring(self):\n",
    "        \"\"\"启动基础监控\"\"\"\n",
    "        try:\n",
    "            # 启动资源监控\n",
    "            if hasattr(self.resource_monitor, 'start'):\n",
    "                self.resource_monitor.start()\n",
    "            \n",
    "            # 启动性能监控\n",
    "            if hasattr(self.performance_monitor, 'start'):\n",
    "                self.performance_monitor.start()\n",
    "            \n",
    "            # 启动内存监控\n",
    "            if hasattr(self.memory_monitor, 'start'):\n",
    "                self.memory_monitor.start()\n",
    "            \n",
    "            logger.info(\"基础监控已启动\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"启动基础监控失败: {str(e)}\")\n",
    "            \n",
    "    def stop_basic_monitoring(self):\n",
    "        \"\"\"停止基础监控\"\"\"\n",
    "        try:\n",
    "            # 停止资源监控\n",
    "            if hasattr(self.resource_monitor, 'stop'):\n",
    "                self.resource_monitor.stop()\n",
    "            \n",
    "            # 停止性能监控\n",
    "            if hasattr(self.performance_monitor, 'stop'):\n",
    "                self.performance_monitor.stop()\n",
    "            \n",
    "            # 停止内存监控\n",
    "            if hasattr(self.memory_monitor, 'stop'):\n",
    "                self.memory_monitor.stop()\n",
    "            \n",
    "            logger.info(\"基础监控已停止\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"停止基础监控失败: {str(e)}\")\n",
    "\n",
    "# 从cell1移入的LogDisplayManager\n",
    "class LogDisplayManager:\n",
    "    \"\"\"日志显示管理器\"\"\"\n",
    "    def __init__(self, max_lines=10):\n",
    "        self.max_lines = max_lines\n",
    "        self.log_buffer = []\n",
    "        self.progress_bars = {i: 0.0 for i in range(6)}\n",
    "        self._clear_output()\n",
    "    \n",
    "    def _clear_output(self):\n",
    "        \"\"\"清空输出\"\"\"\n",
    "        clear_output(wait=True)\n",
    "    \n",
    "    def _display_logs(self):\n",
    "        \"\"\"显示日志\"\"\"\n",
    "        self._clear_output()\n",
    "        \n",
    "        start_idx = max(0, len(self.log_buffer) - self.max_lines)\n",
    "        for log in self.log_buffer[start_idx:]:\n",
    "            if log.strip():\n",
    "                print(log)\n",
    "        \n",
    "        print('-' * 80)\n",
    "        \n",
    "        for model_idx, progress in self.progress_bars.items():\n",
    "            bar_length = 50\n",
    "            filled = int(progress * bar_length)\n",
    "            bar = f\"Model {model_idx + 1}: [{'='*filled}{' '*(bar_length-filled)}] {progress*100:.1f}%\"\n",
    "            print(bar)\n",
    "\n",
    "# 辅助类定义\n",
    "class MemoryMonitor:\n",
    "    \"\"\"内存监控类\"\"\"\n",
    "    def check_memory(self):\n",
    "        \"\"\"检查内存状态\"\"\"\n",
    "        try:\n",
    "            memory = psutil.virtual_memory()\n",
    "            memory_info = {\n",
    "                'total': memory.total,\n",
    "                'available': memory.available,\n",
    "                'used': memory.used,\n",
    "                'usage_percent': memory.percent,\n",
    "                'healthy': memory.percent < 90,\n",
    "                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "            return memory_info\n",
    "        except Exception as e:\n",
    "            logger.error(f\"检查内存状态时出错: {str(e)}\")\n",
    "            return {'healthy': False, 'error': str(e)}\n",
    "\n",
    "    def get_memory_usage(self):\n",
    "        \"\"\"获取当前内存使用情况\"\"\"\n",
    "        try:\n",
    "            return psutil.virtual_memory().percent\n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取内存使用情况时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "class SystemCleaner:\n",
    "    \"\"\"系统清理类\"\"\"\n",
    "    def __init__(self):\n",
    "        self.temp_dirs = ['/tmp', os.path.expanduser('~/.cache')]\n",
    "        self.last_cleanup = time.time()\n",
    "        self.cleanup_interval = 3600  # 1小时\n",
    "\n",
    "    def check_and_cleanup(self):\n",
    "        \"\"\"检查并执行清理\"\"\"\n",
    "        try:\n",
    "            current_time = time.time()\n",
    "            if current_time - self.last_cleanup >= self.cleanup_interval:\n",
    "                self._regular_cleanup()\n",
    "                self.last_cleanup = current_time\n",
    "        except Exception as e:\n",
    "            logger.error(f\"执行定期清理时出错: {str(e)}\")\n",
    "\n",
    "    def _regular_cleanup(self):\n",
    "        \"\"\"常规清理\"\"\"\n",
    "        try:\n",
    "            # 清理Python缓存\n",
    "            gc.collect()\n",
    "            \n",
    "            # 清理TensorFlow会话\n",
    "            tf.keras.backend.clear_session()\n",
    "            \n",
    "            # 清理临时文件\n",
    "            self._cleanup_temp_files()\n",
    "            \n",
    "            logger.info(\"完成常规清理\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"执行常规清理时出错: {str(e)}\")\n",
    "\n",
    "    def _emergency_cleanup(self):\n",
    "        \"\"\"紧急清理\"\"\"\n",
    "        try:\n",
    "            # 强制垃圾回收\n",
    "            gc.collect()\n",
    "            \n",
    "            # 清理TensorFlow会话\n",
    "            tf.keras.backend.clear_session()\n",
    "            \n",
    "            # 清理临时文件\n",
    "            self._cleanup_temp_files(emergency=True)\n",
    "            \n",
    "            logger.warning(\"完成紧急清理\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"执行紧急清理时出错: {str(e)}\")\n",
    "\n",
    "    def _cleanup_temp_files(self, emergency=False):\n",
    "        \"\"\"清理临时文件\n",
    "        Args:\n",
    "            emergency: 是否为紧急清理\n",
    "        \"\"\"\n",
    "        for temp_dir in self.temp_dirs:\n",
    "            if os.path.exists(temp_dir):\n",
    "                try:\n",
    "                    if emergency:\n",
    "                        shutil.rmtree(temp_dir)\n",
    "                        os.makedirs(temp_dir)\n",
    "                    else:\n",
    "                        # 只删除超过1天的文件\n",
    "                        for root, dirs, files in os.walk(temp_dir):\n",
    "                            for f in files:\n",
    "                                path = os.path.join(root, f)\n",
    "                                if time.time() - os.path.getmtime(path) > 86400:\n",
    "                                    os.remove(path)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"清理临时文件时出错: {str(e)}\")\n",
    "\n",
    "# 创建资源监控器实例（确保在类定义之后）\n",
    "resource_monitor = ResourceMonitor()\n",
    "# 创建性能监控器实例\n",
    "performance_monitor = PerformanceMonitor()\n",
    "\n",
    "# 保持原有全局实例\n",
    "monitor_system = SystemManager()\n",
    "\n",
    "def init_monitoring():\n",
    "    \"\"\"初始化监控系统\"\"\"\n",
    "    monitor_system.resource_monitor.start()\n",
    "    monitor_system.performance_monitor.start()\n",
    "    logger.info(\"监控系统已启动\")\n",
    "\n",
    "def stop_monitoring():\n",
    "    \"\"\"停止监控系统\"\"\"\n",
    "    monitor_system.resource_monitor.stop()\n",
    "    monitor_system.performance_monitor.stop()\n",
    "    logger.info(\"监控系统已停止\")\n",
    "\n",
    "class TrainingMonitor(PerformanceMonitor):\n",
    "    \"\"\"训练专用监控\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics.update({\n",
    "            'model_loss': [deque(maxlen=1000) for _ in range(6)],\n",
    "            'param_values': deque(maxlen=1000)\n",
    "        })\n",
    "        \n",
    "    def log_training_metrics(self, model_idx, loss, params):\n",
    "        \"\"\"记录训练指标\"\"\"\n",
    "        self.metrics['model_loss'][model_idx].append(loss)\n",
    "        self.metrics['param_values'].append(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05757e28-d314-495e-81c9-295d49c49d80",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-18 15:17:57,267] INFO - 配置系统初始化完成\n",
      "[2025-02-18 15:17:57,267] INFO - 配置系统初始化完成\n",
      "[2025-02-18 15:17:57,269] INFO - 配置系统测试通过\n",
      "[2025-02-18 15:17:57,269] INFO - 配置系统测试通过\n",
      "[2025-02-18 15:17:57,271] INFO - 核心管理器初始化完成\n",
      "[2025-02-18 15:17:57,271] INFO - 核心管理器初始化完成\n"
     ]
    }
   ],
   "source": [
    "#4 Data Management System / 数据管理系统\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import pymysql\n",
    "import threading\n",
    "from datetime import datetime, timedelta\n",
    "from collections import deque, OrderedDict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from sqlalchemy.pool import QueuePool\n",
    "from pymysql.cursors import DictCursor\n",
    "from cell1_core import core_manager\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# 获取logger实例\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataManager:\n",
    "    \"\"\"统一数据管理器 - 整合数据库管理和数据管道功能\"\"\"\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "    \n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            with cls._lock:\n",
    "                if cls._instance is None:\n",
    "                    cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self):\n",
    "        if not core_manager.initialized:\n",
    "            raise RuntimeError(\"核心配置未初始化！请先运行cell1_core.py\")\n",
    "        \n",
    "        # 获取配置\n",
    "        try:\n",
    "            # 直接从SYSTEM_CONFIG中获取DATA_CONFIG\n",
    "            data_config = core_manager.SYSTEM_CONFIG['DATA_CONFIG']\n",
    "        except (KeyError, AttributeError):\n",
    "            logger.error(\"数据配置缺失，使用默认值\")\n",
    "            data_config = {\n",
    "                'cache_size': 10000,\n",
    "                'min_sequence_length': 14400,\n",
    "                'normalize_range': (-1, 1)\n",
    "            }\n",
    "        \n",
    "        # 设置数据管理器属性\n",
    "        self.cache_size = data_config['cache_size']\n",
    "        self.normalize_range = data_config['normalize_range']\n",
    "        \n",
    "        # 初始化数据库配置\n",
    "        self.DB_CONFIG = core_manager.DB_CONFIG\n",
    "        \n",
    "        # 初始化连接池\n",
    "        self.pool = self._create_pool()\n",
    "        \n",
    "        # 初始化数据处理组件\n",
    "        self.data_pool = DataPool()\n",
    "        self.data_processor = DataProcessor()\n",
    "        self.data_validator = DataValidator()\n",
    "        self.time_feature_extractor = TimeFeatureExtractor()\n",
    "        \n",
    "        # 初始化数据缓存\n",
    "        self.query_cache = {}\n",
    "        self.cache_timeout = 300  # 5分钟缓存超时\n",
    "        \n",
    "        # 初始化数据配置\n",
    "        self.sequence_length = data_config['min_sequence_length']\n",
    "        \n",
    "        # 初始化目录\n",
    "        self._init_directories()\n",
    "        \n",
    "        # 修改数据库连接初始化\n",
    "        self.connection = None\n",
    "        self._init_connection()\n",
    "        \n",
    "        logger.info(\"数据管理器初始化完成\")\n",
    "\n",
    "    def _init_db_config(self):\n",
    "        \"\"\"初始化数据库配置\"\"\"\n",
    "        db_config = config_instance.get_db_config()\n",
    "        db_config.update({\n",
    "            'database': 'admin_data',\n",
    "            'charset': 'utf8mb4'\n",
    "        })\n",
    "        return db_config\n",
    "\n",
    "    def _init_directories(self):\n",
    "        \"\"\"初始化目录结构\"\"\"\n",
    "        self.comparison_dir = os.path.join(core_manager.BASE_DIR, 'comparison')\n",
    "        os.makedirs(self.comparison_dir, exist_ok=True)\n",
    "        self.issue_file = os.path.join(self.comparison_dir, 'issue_number.txt')\n",
    "\n",
    "    def _create_pool(self):\n",
    "        \"\"\"创建数据库连接池\"\"\"\n",
    "        try:\n",
    "            pool = QueuePool(\n",
    "                creator=lambda: pymysql.connect(**self.DB_CONFIG),\n",
    "                pool_size=10,\n",
    "                max_overflow=20,\n",
    "                timeout=30\n",
    "            )\n",
    "            logger.info(\"数据库连接池创建成功\")\n",
    "            return pool\n",
    "        except Exception as e:\n",
    "            logger.error(f\"创建数据库连接池失败: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def execute_query(self, query, params=None, retry=3, use_cache=False):\n",
    "        \"\"\"执行SQL查询（增加数据量检查）\"\"\"\n",
    "        for attempt in range(retry):\n",
    "            try:\n",
    "                # 检查缓存\n",
    "                if use_cache:\n",
    "                    cache_key = f\"{query}_{str(params)}\"\n",
    "                    cached_result = self._get_from_cache(cache_key)\n",
    "                    if cached_result is not None:\n",
    "                        logger.info(f\"成功获取数据 {len(cached_result)} 条\")\n",
    "                        return cached_result\n",
    "                \n",
    "                # 获取连接和游标\n",
    "                connection = self.pool.connect()\n",
    "                try:\n",
    "                    cursor = connection.cursor(DictCursor)\n",
    "                    cursor.execute(query, params)\n",
    "                    result = cursor.fetchall()\n",
    "                    \n",
    "                    if use_cache:\n",
    "                        self._update_cache(cache_key, result)\n",
    "                    logger.info(f\"成功获取数据 {len(result)} 条\")\n",
    "                    return result\n",
    "                finally:\n",
    "                    cursor.close()\n",
    "                    connection.close()\n",
    "                \n",
    "            except pymysql.OperationalError as e:\n",
    "                if attempt < retry - 1:\n",
    "                    logger.warning(f\"数据库连接失败，尝试重连({attempt+1}/{retry})\")\n",
    "                    self.pool = self._create_pool()  # 重建连接池\n",
    "                    time.sleep(2 ** attempt)\n",
    "                    continue\n",
    "                raise\n",
    "        return None\n",
    "\n",
    "    def execute_batch(self, query, params_list):\n",
    "        \"\"\"批量执行查询\"\"\"\n",
    "        try:\n",
    "            connection = self.pool.connect()\n",
    "            cursor = connection.cursor()\n",
    "            \n",
    "            try:\n",
    "                cursor.executemany(query, params_list)\n",
    "                connection.commit()\n",
    "                return True\n",
    "            finally:\n",
    "                cursor.close()\n",
    "                connection.close()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"批量执行查询失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def get_records_by_issue(self, start_issue, limit):\n",
    "        \"\"\"按期号范围获取记录\"\"\"\n",
    "        query = f\"\"\"\n",
    "            SELECT * FROM admin_tab \n",
    "            WHERE date_period >= %s\n",
    "            ORDER BY date_period ASC\n",
    "            LIMIT %s\n",
    "        \"\"\"\n",
    "        return self.execute_query(query, (start_issue, limit))\n",
    "\n",
    "    def close_all(self):\n",
    "        \"\"\"关闭所有数据库连接\"\"\"\n",
    "        if self.pool:\n",
    "            self.pool.dispose()\n",
    "            logger.info(\"已关闭所有数据库连接\")\n",
    "\n",
    "    def get_data_stats(self):\n",
    "        \"\"\"获取数据统计信息\"\"\"\n",
    "        try:\n",
    "            with self.lock:\n",
    "                return {\n",
    "                    'total_samples': len(self.data_pool.data),\n",
    "                    'cache_size': self.data_pool.get_cache_size(),\n",
    "                    'last_update': self.data_pool.last_update_time,\n",
    "                    'memory_usage': self._get_memory_usage(),\n",
    "                    'database_connections': self.pool.size()\n",
    "                }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取数据统计信息时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _get_memory_usage(self):\n",
    "        \"\"\"获取内存使用情况\"\"\"\n",
    "        try:\n",
    "            import psutil\n",
    "            process = psutil.Process()\n",
    "            return process.memory_info().rss / (1024 * 1024)  # 转换为MB\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def get_training_batch(self, batch_size=None):\n",
    "        \"\"\"获取训练批次\"\"\"\n",
    "        try:\n",
    "            batch_size = batch_size or self.batch_size\n",
    "            with self._lock:\n",
    "                # 获取原始数据\n",
    "                data = self.data_pool.get_latest_data()\n",
    "                # 处理数据\n",
    "                processed = self.data_processor.process_records(data)\n",
    "                # 验证数据\n",
    "                if not self.data_validator.validate(processed):\n",
    "                    raise ValueError(\"数据验证失败\")\n",
    "                return processed\n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取训练批次时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def update_data(self):\n",
    "        \"\"\"更新数据\"\"\"\n",
    "        try:\n",
    "            new_data = self._fetch_new_data()\n",
    "            if new_data is not None:\n",
    "                with self._lock:\n",
    "                    self.data_pool.update_data(new_data)\n",
    "                logger.info(f\"数据更新成功，当前数据量: {len(self.data_pool.data)}\")\n",
    "                return True\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"更新数据时出错: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _fetch_new_data(self):\n",
    "        \"\"\"获取新数据\"\"\"\n",
    "        try:\n",
    "            with self.issue_lock:\n",
    "                # 读取最后一期期号\n",
    "                with open(self.issue_file, 'r+') as f:\n",
    "                    last_issue = f.read().strip()\n",
    "                    \n",
    "                    # 获取总数据量\n",
    "                    total = config_instance.SYSTEM_CONFIG['SAMPLE_CONFIG']['total_fetch']()\n",
    "                    \n",
    "                    # 构建查询\n",
    "                    query = f\"\"\"\n",
    "                        SELECT date_period, number \n",
    "                        FROM admin_tab \n",
    "                        WHERE date_period {'>' if last_issue else ''}= '{last_issue}'\n",
    "                        ORDER BY date_period \n",
    "                        LIMIT {total}\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    records = self.execute_query(query)\n",
    "                    \n",
    "                    # 验证数据连续性\n",
    "                    if not self._validate_sequence(records):\n",
    "                        raise ValueError(\"数据存在断层\")\n",
    "                    \n",
    "                    # 更新期号文件\n",
    "                    if records:\n",
    "                        new_last = records[-1]['date_period']\n",
    "                        f.seek(0)\n",
    "                        f.write(new_last)\n",
    "                        f.truncate()\n",
    "                    \n",
    "                    return self._process_numbers(records)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取新数据失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _validate_sequence(self, records):\n",
    "        \"\"\"验证数据序列连续性\"\"\"\n",
    "        if not records or len(records) < 2:\n",
    "            return True\n",
    "            \n",
    "        max_gap = config_instance.SYSTEM_CONFIG['max_sequence_gap']\n",
    "        \n",
    "        for i in range(1, len(records)):\n",
    "            current = records[i]['date_period']\n",
    "            previous = records[i-1]['date_period']\n",
    "            if not self._is_consecutive_periods(previous, current, max_gap):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def _is_consecutive_periods(self, prev_period, curr_period, max_gap):\n",
    "        \"\"\"检查两个期号是否连续\"\"\"\n",
    "        try:\n",
    "            prev_date, prev_num = prev_period.split('-')\n",
    "            curr_date, curr_num = curr_period.split('-')\n",
    "            \n",
    "            prev_dt = datetime.strptime(prev_date, '%Y%m%d')\n",
    "            curr_dt = datetime.strptime(curr_date, '%Y%m%d')\n",
    "            \n",
    "            if prev_dt == curr_dt:\n",
    "                return int(curr_num) - int(prev_num) <= max_gap\n",
    "            \n",
    "            if curr_dt - prev_dt == timedelta(days=1):\n",
    "                return int(prev_num) == 1440 and int(curr_num) == 1\n",
    "                \n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"检查期号连续性时出错: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _process_numbers(self, records):\n",
    "        \"\"\"处理数字号码\"\"\"\n",
    "        try:\n",
    "            processed = []\n",
    "            for r in records:\n",
    "                numbers = [int(d) for d in r['number'].zfill(5)]\n",
    "                processed.append({\n",
    "                    'date_period': r['date_period'],\n",
    "                    'numbers': numbers,\n",
    "                    'time_features': self.time_feature_extractor.extract_features(r['date_period'])\n",
    "                })\n",
    "            return processed\n",
    "        except Exception as e:\n",
    "            logger.error(f\"处理号码时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _get_from_cache(self, key):\n",
    "        \"\"\"从缓存获取数据\"\"\"\n",
    "        if key in self.query_cache:\n",
    "            timestamp, data = self.query_cache[key]\n",
    "            if datetime.now() - timestamp < timedelta(seconds(self.cache_timeout)):\n",
    "                return data\n",
    "            del self.query_cache[key]\n",
    "        return None\n",
    "\n",
    "    def _update_cache(self, key, data):\n",
    "        \"\"\"更新缓存\"\"\"\n",
    "        self.query_cache[key] = (datetime.now(), data)\n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"清空所有数据缓存\"\"\"\n",
    "        with self.lock:  # 使用锁保证线程安全\n",
    "            self.query_cache.clear()\n",
    "            self.data_pool.data.clear()\n",
    "            logger.info(\"已清空所有数据缓存\")\n",
    "\n",
    "    def get_data_by_date_range(self, start_date, end_date):\n",
    "        \"\"\"按日期范围获取数据\"\"\"\n",
    "        try:\n",
    "            query = \"\"\"\n",
    "                SELECT * FROM admin_tab \n",
    "                WHERE DATE(SUBSTRING_INDEX(date_period, '-', 1)) \n",
    "                BETWEEN %s AND %s\n",
    "                ORDER BY date_period\n",
    "            \"\"\"\n",
    "            return self.execute_query(query, (start_date, end_date))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取日期范围数据失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def check_data_continuity(self, data):\n",
    "        \"\"\"检查数据连续性\"\"\"\n",
    "        try:\n",
    "            if not data or len(data) < 2:\n",
    "                return True\n",
    "                \n",
    "            periods = [d['date_period'] for d in data]\n",
    "            for i in range(1, len(periods)):\n",
    "                curr_period = periods[i]\n",
    "                prev_period = periods[i-1]\n",
    "                \n",
    "                if not self._is_consecutive_periods(prev_period, curr_period):\n",
    "                    logger.warning(f\"数据不连续: {prev_period} -> {curr_period}\")\n",
    "                    return False\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"检查数据连续性失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def generate_test_batch(self, size=1000):\n",
    "        \"\"\"生成测试批次\"\"\"\n",
    "        try:\n",
    "            with self._lock:\n",
    "                latest_data = self.data_pool.get_latest_data(size)\n",
    "                if not latest_data:\n",
    "                    return None\n",
    "                    \n",
    "                test_data = self.data_processor.process_records(latest_data)\n",
    "                if not self.data_validator.validate(test_data):\n",
    "                    raise ValueError(\"测试数据验证失败\")\n",
    "                    \n",
    "                return test_data\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"生成测试批次失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def save_cache_to_disk(self, cache_path=None):\n",
    "        \"\"\"保存缓存到磁盘\"\"\"\n",
    "        try:\n",
    "            if cache_path is None:\n",
    "                cache_path = os.path.join(config_instance.BASE_DIR, 'cache', 'data_cache.pkl')\n",
    "                \n",
    "            os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n",
    "            \n",
    "            cache_data = {\n",
    "                'pool_data': self.data_pool.data,\n",
    "                'query_cache': self.query_cache,\n",
    "                'last_update': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "            \n",
    "            with open(cache_path, 'wb') as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "                \n",
    "            logger.info(f\"缓存已保存到: {cache_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"保存缓存失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def load_training_samples(self, limit=1000):\n",
    "        \"\"\"加载训练样本\"\"\"\n",
    "        try:\n",
    "            # 修正字段名\n",
    "            query = \"SELECT number, date_period FROM admin_tab ORDER BY date_period DESC LIMIT %s\"\n",
    "            raw_data = self.execute_query(query, (limit,))\n",
    "            \n",
    "            if not raw_data:\n",
    "                logger.error(\"未查询到任何数据\")\n",
    "                return None\n",
    "                \n",
    "            processed = self.data_processor.process_training_data(raw_data)\n",
    "            if processed is None:\n",
    "                return None\n",
    "                \n",
    "            sequences = self._create_sequences(processed, self.sequence_length)\n",
    "            if sequences is None or len(sequences) == 0:\n",
    "                logger.error(\"无法创建有效序列，可能原因：\\n\"\n",
    "                             \"1. 输入数据长度不足\\n\"\n",
    "                             f\"2. 序列长度设置过大（当前设置：{self.sequence_length}）\\n\"\n",
    "                             \"3. 数据预处理失败\")\n",
    "                return None\n",
    "                \n",
    "            logger.info(f\"成功加载 {len(sequences)} 个训练样本\")\n",
    "            return sequences\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"加载训练样本失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _create_sequences(self, data, seq_length):\n",
    "        \"\"\"创建时间序列数据\"\"\"\n",
    "        try:\n",
    "            sequences = []\n",
    "            for i in range(len(data) - seq_length + 1):  # 修正循环范围\n",
    "                seq = data[i:i+seq_length]\n",
    "                sequences.append(seq)\n",
    "            return np.array(sequences)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"创建序列失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def process_training_data(self, raw_data):\n",
    "        \"\"\"处理原始训练数据\"\"\"\n",
    "        try:\n",
    "            if not raw_data:\n",
    "                logger.error(\"原始数据为空\")\n",
    "                return None\n",
    "                \n",
    "            processed = []\n",
    "            for idx, record in enumerate(raw_data):\n",
    "                try:\n",
    "                    # 验证必要字段存在\n",
    "                    if 'number' not in record or 'date_period' not in record:\n",
    "                        logger.warning(f\"记录{idx}缺失必要字段，已跳过\")\n",
    "                        continue\n",
    "                        \n",
    "                    # 处理数字格式：将字符串拆分为单个字符\n",
    "                    number_str = str(record['number']).strip()\n",
    "                    if len(number_str) != 5:\n",
    "                        logger.warning(f\"记录{idx}号码长度错误: {number_str}\")\n",
    "                        continue\n",
    "                        \n",
    "                    numbers = [int(c) for c in number_str if c.isdigit()]\n",
    "                    if len(numbers) != 5:\n",
    "                        logger.warning(f\"记录{idx}包含非数字字符: {number_str}\")\n",
    "                        continue\n",
    "                        \n",
    "                    time_feat = self._parse_time_features(str(record['date_period']))\n",
    "                    processed.append(numbers + time_feat)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"处理记录{idx}时出错: {str(e)}，已跳过该记录\")\n",
    "\n",
    "            if not processed:\n",
    "                logger.error(\"无有效数据可处理\")\n",
    "                return None\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            return scaler.fit_transform(processed)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"数据处理失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _parse_time_features(self, date_period):\n",
    "        \"\"\"解析时间特征\"\"\"\n",
    "        date_str, period = date_period.split('-')\n",
    "        dt = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "        return [\n",
    "            dt.hour/24.0,  # 小时归一化\n",
    "            dt.weekday()/7.0,  # 星期归一化\n",
    "            int(period)/1440.0  # 期号归一化\n",
    "        ]\n",
    "\n",
    "    def check_connection(self):\n",
    "        \"\"\"检查数据库连接状态\"\"\"\n",
    "        try:\n",
    "            result = self.execute_query(\"SELECT 1\", retry=1)\n",
    "            return bool(result)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"数据库连接检查失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def get_sequence(self, start_issue, end_issue):\n",
    "        \"\"\"获取指定期号范围的序列数据\"\"\"\n",
    "        try:\n",
    "            # 确保连接有效\n",
    "            if not self.connection or not self.connection.open:\n",
    "                self._init_connection()\n",
    "            \n",
    "            # 使用新的连接执行查询\n",
    "            with self.connection.cursor() as cursor:\n",
    "                query = \"\"\"\n",
    "                    SELECT number, date_period \n",
    "                    FROM admin_tab \n",
    "                    WHERE date_period >= %s AND date_period <= %s\n",
    "                    ORDER BY date_period ASC\n",
    "                \"\"\"\n",
    "                cursor.execute(query, (start_issue, end_issue))\n",
    "                results = cursor.fetchall()\n",
    "                \n",
    "                if not results:\n",
    "                    logger.warning(f\"未找到期号范围 {start_issue} 到 {end_issue} 的数据\")\n",
    "                    return None\n",
    "                    \n",
    "                # 转换数据格式\n",
    "                sequence = []\n",
    "                for row in results:\n",
    "                    numbers = [int(n) for n in str(row['number']).zfill(5)]\n",
    "                    normalized = [(n - 4.5) / 4.5 for n in numbers]\n",
    "                    sequence.append(normalized)\n",
    "                    \n",
    "                sequence = np.array(sequence, dtype=np.float32)\n",
    "                logger.info(f\"获取到序列数据 {len(sequence)} 条\")\n",
    "                return sequence\n",
    "                \n",
    "        except pymysql.Error as e:\n",
    "            logger.error(f\"数据库操作失败: {str(e)}\")\n",
    "            self._init_connection()  # 尝试重新连接\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取序列数据失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _init_connection(self):\n",
    "        \"\"\"初始化数据库连接\"\"\"\n",
    "        try:\n",
    "            if self.connection and self.connection.open:\n",
    "                self.connection.close()\n",
    "                \n",
    "            # 重新创建连接\n",
    "            self.connection = pymysql.connect(\n",
    "                host=self.DB_CONFIG['host'],\n",
    "                port=self.DB_CONFIG['port'],\n",
    "                user=self.DB_CONFIG['user'],\n",
    "                password=self.DB_CONFIG['password'],\n",
    "                database=self.DB_CONFIG['database'],\n",
    "                charset=self.DB_CONFIG['charset'],\n",
    "                cursorclass=pymysql.cursors.DictCursor,\n",
    "                autocommit=True,\n",
    "                connect_timeout=60\n",
    "            )\n",
    "            logger.info(\"数据库连接初始化成功\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"数据库连接初始化失败: {str(e)}\")\n",
    "            self.connection = None\n",
    "            raise\n",
    "\n",
    "class DataPool:\n",
    "    \"\"\"数据池 - 负责数据缓存和管理\"\"\"\n",
    "    def __init__(self, max_size=10000):\n",
    "        self.data = []\n",
    "        self.cache = OrderedDict()\n",
    "        self.max_size = max_size\n",
    "        self.lock = threading.Lock()\n",
    "        self.last_update_time = None\n",
    "        \n",
    "        # 初始化数据缩放器\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.is_scaler_fitted = False\n",
    "    \n",
    "    def update_data(self, new_data):\n",
    "        \"\"\"更新数据\"\"\"\n",
    "        with self.lock:\n",
    "            self.data.extend(new_data)\n",
    "            self.last_update_time = datetime.now()\n",
    "            \n",
    "            # 如果还没有拟合scaler，进行拟合\n",
    "            if not self.is_scaler_fitted and len(self.data) > 0:\n",
    "                self.scaler.fit(np.array([d['numbers'] for d in self.data]))\n",
    "                self.is_scaler_fitted = True\n",
    "    \n",
    "    def get_latest_data(self, n=1000):\n",
    "        \"\"\"获取最新的n条数据\"\"\"\n",
    "        with self.lock:\n",
    "            return self.data[-n:] if self.data else []\n",
    "    \n",
    "    def get_cache_size(self):\n",
    "        \"\"\"获取缓存大小\"\"\"\n",
    "        return len(self.cache)\n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"清理数据缓存\"\"\"\n",
    "        with self.lock:\n",
    "            self.cache.clear()\n",
    "\n",
    "    def add_batch(self, batch):\n",
    "        \"\"\"添加数据批次\"\"\"\n",
    "        # 添加对齐检查\n",
    "        aligned = self._align_sequences(batch)\n",
    "        with self.lock:\n",
    "            self.data.extend(aligned)\n",
    "        \n",
    "    def _align_sequences(self, batch):\n",
    "        \"\"\"对齐序列长度\"\"\"\n",
    "        max_len = max(len(item['input']) for item in batch)\n",
    "        aligned = []\n",
    "        for item in batch:\n",
    "            aligned_item = item.copy()\n",
    "            aligned_item['input'] = np.pad(\n",
    "                item['input'], \n",
    "                (0, max_len - len(item['input'])),\n",
    "                'constant'\n",
    "            )\n",
    "            aligned.append(aligned_item)\n",
    "        return aligned\n",
    "\n",
    "    def get_training_data(self, sequence_length):\n",
    "        \"\"\"获取训练数据\"\"\"\n",
    "        with self.lock:\n",
    "            if len(self.data) < sequence_length:\n",
    "                return None\n",
    "            data = np.array([d['numbers'] for d in self.data])\n",
    "            if self.is_scaler_fitted:\n",
    "                data = self.scaler.transform(data)\n",
    "            return data\n",
    "\n",
    "    def get_data_window(self, start_idx, window_size):\n",
    "        \"\"\"获取指定窗口的数据\"\"\"\n",
    "        with self.lock:\n",
    "            if start_idx + window_size > len(self.data):\n",
    "                return None\n",
    "            return self.data[start_idx:start_idx + window_size]\n",
    "\n",
    "    def get_latest_periods(self, n_periods):\n",
    "        \"\"\"获取最近n期数据\"\"\"\n",
    "        with self.lock:\n",
    "            return self.data[-n_periods:] if len(self.data) >= n_periods else None\n",
    "\n",
    "    def preload_data(self, start_date, end_date):\n",
    "        \"\"\"预加载指定日期范围的数据\"\"\"\n",
    "        try:\n",
    "            query = \"\"\"\n",
    "                SELECT * FROM admin_tab \n",
    "                WHERE DATE(SUBSTRING_INDEX(date_period, '-', 1)) \n",
    "                BETWEEN %s AND %s\n",
    "                ORDER BY date_period\n",
    "            \"\"\"\n",
    "            records = data_manager.execute_query(query, (start_date, end_date))\n",
    "            \n",
    "            if records:\n",
    "                self.update_data(records)\n",
    "                return True\n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"预加载数据失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"数据处理器 - 负责数据预处理和批次生成\"\"\"\n",
    "    def __init__(self):\n",
    "        self.scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        self.lock = threading.Lock()\n",
    "        # 添加时间特征提取器\n",
    "        self.time_feature_extractor = TimeFeatureExtractor()\n",
    "    \n",
    "    def process_records(self, records):\n",
    "        \"\"\"处理数据记录\"\"\"\n",
    "        try:\n",
    "            # 1. 清理数据\n",
    "            cleaned = self._remove_invalid_data(records)\n",
    "            \n",
    "            # 2. 特征工程\n",
    "            features = self._extract_features(cleaned)\n",
    "            \n",
    "            # 3. 数据标准化\n",
    "            normalized = self._normalize_features(features)\n",
    "            \n",
    "            # 4. 序列化处理\n",
    "            sequences = self._create_sequences(normalized)\n",
    "            \n",
    "            return sequences\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"处理数据记录时出错: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _remove_invalid_data(self, records):\n",
    "        \"\"\"移除无效数据\"\"\"\n",
    "        return [r for r in records if self._is_valid_record(r)]\n",
    "    \n",
    "    def _is_valid_record(self, record):\n",
    "        \"\"\"检查记录是否有效\"\"\"\n",
    "        try:\n",
    "            numbers = record['numbers']\n",
    "            return (\n",
    "                isinstance(numbers, list) and\n",
    "                len(numbers) == 5 and\n",
    "                all(isinstance(n, int) and 0 <= n <= 9 for n in numbers)\n",
    "            )\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def _extract_features(self, records):\n",
    "        \"\"\"提取特征\"\"\"\n",
    "        features = []\n",
    "        for record in records:\n",
    "            # 基础数字特征\n",
    "            number_features = np.array(record['numbers'])\n",
    "            # 时间特征\n",
    "            time_features = self.time_feature_extractor.extract_features(record['date_period'])\n",
    "            # 合并特征\n",
    "            combined = np.concatenate([number_features, time_features])\n",
    "            features.append(combined)\n",
    "        return np.array(features)\n",
    "    \n",
    "    def _normalize_features(self, features):\n",
    "        \"\"\"标准化特征\"\"\"\n",
    "        with self.lock:\n",
    "            return self.scaler.fit_transform(features)\n",
    "    \n",
    "    def _create_sequences(self, data):\n",
    "        \"\"\"创建序列数据\"\"\"\n",
    "        sequences = []\n",
    "        for i in range(len(data) - config_instance.SYSTEM_CONFIG['SAMPLE_CONFIG']['total_fetch']()):\n",
    "            seq = {\n",
    "                'input': data[i:i+config_instance.SYSTEM_CONFIG['SAMPLE_CONFIG']['input_length']],\n",
    "                'target': data[i+config_instance.SYSTEM_CONFIG['SAMPLE_CONFIG']['input_length']:\n",
    "                              i+config_instance.SYSTEM_CONFIG['SAMPLE_CONFIG']['total_fetch']()]\n",
    "            }\n",
    "            sequences.append(seq)\n",
    "        return sequences\n",
    "\n",
    "    def process_with_time_features(self, data):\n",
    "        \"\"\"处理数据并添加时间特征\"\"\"\n",
    "        try:\n",
    "            processed = []\n",
    "            for record in data:\n",
    "                # 基础特征\n",
    "                features = self._extract_base_features(record)\n",
    "                # 时间特征\n",
    "                time_features = self.time_feature_extractor.extract_features(record['date_period'])\n",
    "                # 组合特征\n",
    "                combined = np.concatenate([features, time_features])\n",
    "                processed.append(combined)\n",
    "            return np.array(processed)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"处理时间特征时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _extract_base_features(self, record):\n",
    "        \"\"\"提取基础特征\"\"\"\n",
    "        try:\n",
    "            numbers = np.array(record['numbers'])\n",
    "            # 添加统计特征\n",
    "            stats = [\n",
    "                np.mean(numbers),\n",
    "                np.std(numbers),\n",
    "                np.max(numbers),\n",
    "                np.min(numbers)\n",
    "            ]\n",
    "            return np.concatenate([numbers, stats])\n",
    "        except Exception as e:\n",
    "            logger.error(f\"提取基础特征时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def apply_feature_scaling(self, data, feature_range=(-1, 1)):\n",
    "        \"\"\"应用特征缩放\"\"\"\n",
    "        try:\n",
    "            self.scaler = MinMaxScaler(feature_range=feature_range)\n",
    "            return self.scaler.fit_transform(data)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"特征缩放失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def create_sliding_windows(self, data, window_size, stride=1):\n",
    "        \"\"\"创建滑动窗口数据\"\"\"\n",
    "        try:\n",
    "            windows = []\n",
    "            for i in range(0, len(data) - window_size + 1, stride):\n",
    "                windows.append(data[i:i + window_size])\n",
    "            return np.array(windows)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"创建滑动窗口失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def batch_normalize(self, batches):\n",
    "        \"\"\"批量数据标准化\"\"\"\n",
    "        try:\n",
    "            normalized_batches = []\n",
    "            for batch in batches:\n",
    "                normalized = self._normalize_features(batch)\n",
    "                normalized_batches.append(normalized)\n",
    "            return normalized_batches\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"批量标准化失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def process_training_data(self, raw_data):\n",
    "        \"\"\"处理原始训练数据\"\"\"\n",
    "        try:\n",
    "            if not raw_data:\n",
    "                logger.error(\"原始数据为空\")\n",
    "                return None\n",
    "                \n",
    "            processed = []\n",
    "            for idx, record in enumerate(raw_data):\n",
    "                try:\n",
    "                    # 验证必要字段存在\n",
    "                    if 'number' not in record or 'date_period' not in record:\n",
    "                        logger.warning(f\"记录{idx}缺失必要字段，已跳过\")\n",
    "                        continue\n",
    "                        \n",
    "                    # 处理数字格式：将字符串拆分为单个字符\n",
    "                    number_str = str(record['number']).strip()\n",
    "                    if len(number_str) != 5:\n",
    "                        logger.warning(f\"记录{idx}号码长度错误: {number_str}\")\n",
    "                        continue\n",
    "                        \n",
    "                    numbers = [int(c) for c in number_str if c.isdigit()]\n",
    "                    if len(numbers) != 5:\n",
    "                        logger.warning(f\"记录{idx}包含非数字字符: {number_str}\")\n",
    "                        continue\n",
    "                        \n",
    "                    time_feat = self._parse_time_features(str(record['date_period']))\n",
    "                    processed.append(numbers + time_feat)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"处理记录{idx}时出错: {str(e)}，已跳过该记录\")\n",
    "\n",
    "            if not processed:\n",
    "                logger.error(\"无有效数据可处理\")\n",
    "                return None\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            return scaler.fit_transform(processed)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"数据处理失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _parse_time_features(self, date_period):\n",
    "        \"\"\"解析时间特征\"\"\"\n",
    "        date_str, period = date_period.split('-')\n",
    "        dt = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "        return [\n",
    "            dt.hour/24.0,  # 小时归一化\n",
    "            dt.weekday()/7.0,  # 星期归一化\n",
    "            int(period)/1440.0  # 期号归一化\n",
    "        ]\n",
    "\n",
    "class DataValidator:\n",
    "    \"\"\"数据验证器 - 负责数据有效性检查\"\"\"\n",
    "    def __init__(self):\n",
    "        self.lock = threading.Lock()\n",
    "        self.validation_rules = {\n",
    "            'sequence_length': self._check_sequence_length,\n",
    "            'number_range': self._check_number_range,\n",
    "            'time_continuity': self._check_time_continuity,\n",
    "            'feature_completeness': self._check_feature_completeness\n",
    "        }\n",
    "    \n",
    "    def validate(self, data):\n",
    "        \"\"\"验证数据有效性\"\"\"\n",
    "        try:\n",
    "            with self.lock:\n",
    "                return all(\n",
    "                    rule(data) for rule in self.validation_rules.values()\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"数据验证时出错: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def _check_sequence_length(self, data):\n",
    "        \"\"\"检查序列长度\"\"\"\n",
    "        required_length = config_instance.SYSTEM_CONFIG['SAMPLE_CONFIG']['total_fetch']()\n",
    "        return len(data) >= required_length\n",
    "    \n",
    "    def _check_number_range(self, data):\n",
    "        \"\"\"检查数字范围\"\"\"\n",
    "        try:\n",
    "            numbers = np.array([d['numbers'] for d in data])\n",
    "            return np.all((numbers >= 0) & (numbers <= 9))\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def _check_time_continuity(self, data):\n",
    "        \"\"\"检查时间连续性\"\"\"\n",
    "        try:\n",
    "            periods = [d['date_period'] for d in data]\n",
    "            for i in range(1, len(periods)):\n",
    "                if not self._is_consecutive_periods(periods[i-1], periods[i]):\n",
    "                    return False\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def _check_feature_completeness(self, data):\n",
    "        \"\"\"检查特征完整性\"\"\"\n",
    "        try:\n",
    "            return all(\n",
    "                'numbers' in d and 'time_features' in d \n",
    "                for d in data\n",
    "            )\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def _is_consecutive_periods(self, prev, curr):\n",
    "        \"\"\"检查期号是否连续\"\"\"\n",
    "        try:\n",
    "            p_date, p_num = prev.split('-')\n",
    "            c_date, c_num = curr.split('-')\n",
    "            \n",
    "            if p_date == c_date:\n",
    "                return int(c_num) - int(p_num) == 1\n",
    "            \n",
    "            p_dt = datetime.strptime(p_date, '%Y%m%d')\n",
    "            c_dt = datetime.strptime(c_date, '%Y%m%d')\n",
    "            \n",
    "            return (c_dt - p_dt).days == 1 and int(p_num) == 1440 and int(c_num) == 1\n",
    "            \n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def validate_data_completeness(self, data):\n",
    "        \"\"\"验证数据完整性\"\"\"\n",
    "        try:\n",
    "            # 检查数据结构\n",
    "            if not isinstance(data, (list, np.ndarray)):\n",
    "                return False\n",
    "                \n",
    "            # 检查数据量\n",
    "            if len(data) < config_instance.SYSTEM_CONFIG['SAMPLE_CONFIG']['input_length']:\n",
    "                return False\n",
    "                \n",
    "            # 检查每条记录的完整性\n",
    "            for record in data:\n",
    "                if not self._check_record_completeness(record):\n",
    "                    return False\n",
    "                    \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"验证数据完整性时出错: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def _check_record_completeness(self, record):\n",
    "        \"\"\"检查单条记录的完整性\"\"\"\n",
    "        required_fields = ['date_period', 'numbers', 'time_features']\n",
    "        return all(field in record for field in required_fields)\n",
    "\n",
    "    def validate_batch_structure(self, batch):\n",
    "        \"\"\"验证批次数据结构\"\"\"\n",
    "        try:\n",
    "            if not isinstance(batch, dict):\n",
    "                return False\n",
    "                \n",
    "            required_keys = ['input', 'target']\n",
    "            if not all(key in batch for key in required_keys):\n",
    "                return False\n",
    "                \n",
    "            input_shape = batch['input'].shape\n",
    "            target_shape = batch['target'].shape\n",
    "            \n",
    "            if len(input_shape) != 3 or len(target_shape) != 3:\n",
    "                return False\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"验证批次结构失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _check_number_format(self, number_str):\n",
    "        \"\"\"验证号码格式是否正确\"\"\"\n",
    "        try:\n",
    "            parts = number_str.split(',')\n",
    "            if len(parts) != 5:\n",
    "                return False\n",
    "            return all(n.strip().isdigit() and 0 <= int(n) <= 9 for n in parts)\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def process_training_data(self, raw_data):\n",
    "        \"\"\"处理原始训练数据\"\"\"\n",
    "        try:\n",
    "            if not raw_data:\n",
    "                logger.error(\"原始数据为空\")\n",
    "                return None\n",
    "                \n",
    "            processed = []\n",
    "            for idx, record in enumerate(raw_data):\n",
    "                try:\n",
    "                    # 验证必要字段存在\n",
    "                    if 'number' not in record or 'date_period' not in record:\n",
    "                        logger.warning(f\"记录{idx}缺失必要字段，已跳过\")\n",
    "                        continue\n",
    "                        \n",
    "                    # 处理数字格式：将字符串拆分为单个字符\n",
    "                    number_str = str(record['number']).strip()\n",
    "                    if len(number_str) != 5:\n",
    "                        logger.warning(f\"记录{idx}号码长度错误: {number_str}\")\n",
    "                        continue\n",
    "                        \n",
    "                    numbers = [int(c) for c in number_str if c.isdigit()]\n",
    "                    if len(numbers) != 5:\n",
    "                        logger.warning(f\"记录{idx}包含非数字字符: {number_str}\")\n",
    "                        continue\n",
    "                        \n",
    "                    time_feat = self._parse_time_features(str(record['date_period']))\n",
    "                    processed.append(numbers + time_feat)\n",
    "                    \n",
    "                    if not self._check_number_format(str(record['number'])):\n",
    "                        logger.warning(f\"记录{idx}号码格式错误: {record['number']}\")\n",
    "                        continue\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"处理记录{idx}时出错: {str(e)}，已跳过该记录\")\n",
    "\n",
    "            if not processed:\n",
    "                logger.error(\"无有效数据可处理\")\n",
    "                return None\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            return scaler.fit_transform(processed)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"数据处理失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _parse_time_features(self, date_period):\n",
    "        \"\"\"解析时间特征\"\"\"\n",
    "        date_str, period = date_period.split('-')\n",
    "        dt = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "        return [\n",
    "            dt.hour/24.0,  # 小时归一化\n",
    "            dt.weekday()/7.0,  # 星期归一化\n",
    "            int(period)/1440.0  # 期号归一化\n",
    "        ]\n",
    "\n",
    "class TimeFeatureExtractor:\n",
    "    \"\"\"时间特征提取器\"\"\"\n",
    "    def __init__(self):\n",
    "        self.periodic_features = {\n",
    "            'hour_of_day': (24, lambda dt: dt.hour),\n",
    "            'minute_of_hour': (60, lambda dt: dt.minute),\n",
    "            'day_of_week': (7, lambda dt: dt.weekday()),\n",
    "            'day_of_month': (31, lambda dt: dt.day - 1),\n",
    "            'month_of_year': (12, lambda dt: dt.month - 1)\n",
    "        }\n",
    "    \n",
    "    def extract_features(self, date_period):\n",
    "        \"\"\"提取时间特征\"\"\"\n",
    "        try:\n",
    "            # 解析日期和期号\n",
    "            date_str, period = date_period.split('-')\n",
    "            date = datetime.strptime(date_str, '%Y%m%d')\n",
    "            period_num = int(period)\n",
    "            \n",
    "            features = []\n",
    "            \n",
    "            # 添加周期性特征\n",
    "            for period, func in self.periodic_features.values():\n",
    "                value = func(date)\n",
    "                # 转换为sin和cos特征以保持周期性\n",
    "                sin_value = np.sin(2 * np.pi * value / period)\n",
    "                cos_value = np.cos(2 * np.pi * value / period)\n",
    "                features.extend([sin_value, cos_value])\n",
    "            \n",
    "            # 添加期号特征\n",
    "            period_feature = (period_num - 1) / 1440  # 归一化到0-1\n",
    "            features.append(period_feature)\n",
    "            \n",
    "            return np.array(features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"提取时间特征时出错: {str(e)}\")\n",
    "            return np.zeros(len(self.periodic_features) * 2 + 1)  # 返回全零特征\n",
    "\n",
    "class EnhancedDataManager(DataManager):\n",
    "    \"\"\"增强型数据管理器，支持流式数据加载\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.data_buffer = deque(maxlen=14400*2)  # 双倍缓冲\n",
    "        self.last_processed = None\n",
    "        \n",
    "    def stream_training_samples(self):\n",
    "        \"\"\"实时数据流生成器\"\"\"\n",
    "        while True:\n",
    "            # 获取最新14400+2880期数据\n",
    "            latest = self.execute_query(\n",
    "                \"SELECT number, date_period FROM admin_tab \"\n",
    "                \"ORDER BY date_period DESC LIMIT 17280\"\n",
    "            )\n",
    "            if latest and latest != self.last_processed:\n",
    "                processed = self.data_processor.process_streaming_data(latest)\n",
    "                if processed is not None:\n",
    "                    sequences = self._create_sequences(processed, 14400)\n",
    "                    if sequences:\n",
    "                        yield sequences[0]  # 取最新序列\n",
    "                        self.last_processed = latest\n",
    "                time.sleep(58)  # 每58秒检查一次\n",
    "\n",
    "# 创建全局实例\n",
    "data_manager = DataManager()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3fa9229-8d24-4a2a-a3d8-4523ac865693",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#5 Feature Engineering System / 特征工程系统\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Lambda\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from typing import Optional\n",
    "\n",
    "# 获取logger实例\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class FeatureEngineering:\n",
    "    \"\"\"特征工程类\"\"\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"初始化特征工程组件\"\"\"\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.scaler: Optional[MinMaxScaler] = None\n",
    "    \n",
    "    def build_all_features(self, x):\n",
    "        \"\"\"构建所有特征\"\"\"\n",
    "        try:\n",
    "            # 1. 基础特征\n",
    "            basic_features = self._build_basic_features(x)\n",
    "            \n",
    "            # 2. 高级特征\n",
    "            advanced_features = self._build_advanced_features(x)\n",
    "            \n",
    "            # 3. 数字特征\n",
    "            digit_features = self._build_advanced_digit_features(x)\n",
    "            \n",
    "            # 4. 形态特征\n",
    "            pattern_features = self._build_pattern_features(x)\n",
    "            \n",
    "            # 5. 特征融合\n",
    "            all_features = tf.keras.layers.Concatenate()([\n",
    "                basic_features,\n",
    "                advanced_features,\n",
    "                digit_features,\n",
    "                pattern_features\n",
    "            ])\n",
    "            \n",
    "            return all_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"构建特征时出错: {str(e)}\")\n",
    "            return x\n",
    "\n",
    "    def _build_advanced_features(self, x):\n",
    "        \"\"\"构建高级特征分析\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # 1. 冷热号分析\n",
    "        hot_cold = self._analyze_hot_cold_numbers(x)\n",
    "        features.append(hot_cold)\n",
    "        \n",
    "        # 2. 号码频率统计\n",
    "        freq = self._analyze_frequency(x)\n",
    "        features.append(freq)\n",
    "        \n",
    "        # 3. 和值分析\n",
    "        sum_features = self._analyze_sum_value(x)\n",
    "        features.append(sum_features)\n",
    "        \n",
    "        # 4. 数字特征分析\n",
    "        digit_features = self._analyze_digit_patterns(x)\n",
    "        features.append(digit_features)\n",
    "        \n",
    "        # 5. 形态分析\n",
    "        pattern_features = self._analyze_number_patterns(x)\n",
    "        features.append(pattern_features)\n",
    "        \n",
    "        # 6. 012路分析\n",
    "        route_features = self._analyze_012_routes(x)\n",
    "        features.append(route_features)\n",
    "        \n",
    "        return tf.keras.layers.Concatenate()(features)\n",
    "\n",
    "    def _analyze_hot_cold_numbers(self, x, window_sizes=[100, 500, 1000]):\n",
    "        \"\"\"分析冷热号\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for window in window_sizes:\n",
    "            # 最近window期的频率\n",
    "            recent = x[:, -window:]\n",
    "            freq = tf.reduce_sum(tf.one_hot(tf.cast(recent, tf.int32), 10), axis=1)\n",
    "            features.append(freq)\n",
    "        \n",
    "        return tf.keras.layers.Concatenate()(features)\n",
    "\n",
    "    def _analyze_digit_patterns(self, x):\n",
    "        \"\"\"分析数字特征\"\"\"\n",
    "        # 1. 奇偶比\n",
    "        odd_even = tf.reduce_sum(tf.cast(x % 2 == 1, tf.float32), axis=-1, keepdims=True)\n",
    "        \n",
    "        # 2. 大小比 (5-9为大)\n",
    "        big_small = tf.reduce_sum(tf.cast(x >= 5, tf.float32), axis=-1, keepdims=True)\n",
    "        \n",
    "        # 3. 质合比\n",
    "        prime_numbers = tf.constant([2, 3, 5, 7])\n",
    "        is_prime = tf.reduce_sum(tf.cast(\n",
    "            tf.equal(x[..., None], prime_numbers), tf.float32\n",
    "        ), axis=-1)\n",
    "        prime_composite = tf.reduce_sum(is_prime, axis=-1, keepdims=True)\n",
    "        \n",
    "        # 4. 跨度\n",
    "        span = tf.reduce_max(x, axis=-1) - tf.reduce_min(x, axis=-1)\n",
    "        \n",
    "        return tf.concat([odd_even, big_small, prime_composite, span[..., None]], axis=-1)\n",
    "\n",
    "    def _analyze_sum_value(self, x):\n",
    "        \"\"\"分析和值特征\"\"\"\n",
    "        # 1. 计算和值\n",
    "        sum_value = tf.reduce_sum(x, axis=-1, keepdims=True)\n",
    "        \n",
    "        # 2. 和值分布区间\n",
    "        sum_ranges = [\n",
    "            (0, 10), (11, 20), (21, 30), (31, 40), (41, 45)\n",
    "        ]\n",
    "        sum_dist = []\n",
    "        for low, high in sum_ranges:\n",
    "            in_range = tf.logical_and(\n",
    "                sum_value >= low,\n",
    "                sum_value <= high\n",
    "            )\n",
    "            sum_dist.append(tf.cast(in_range, tf.float32))\n",
    "        \n",
    "        # 3. 和值特征\n",
    "        sum_features = tf.concat([sum_value, tf.concat(sum_dist, axis=-1)], axis=-1)\n",
    "        \n",
    "        return sum_features\n",
    "\n",
    "    def _analyze_012_routes(self, x):\n",
    "        \"\"\"分析012路特征\"\"\"\n",
    "        # 1. 计算每个数字的路数\n",
    "        routes = tf.math.floormod(x, 3)  # 对3取余\n",
    "        \n",
    "        # 2. 统计每个位置的路数分布\n",
    "        route_distributions = []\n",
    "        for i in range(5):  # 五个位置\n",
    "            digit_routes = routes[..., i:i+1]\n",
    "            # 统计0,1,2路的数量\n",
    "            route_counts = []\n",
    "            for r in range(3):\n",
    "                count = tf.reduce_sum(\n",
    "                    tf.cast(digit_routes == r, tf.float32),\n",
    "                    axis=-1, keepdims=True\n",
    "                )\n",
    "                route_counts.append(count)\n",
    "            route_distributions.append(tf.concat(route_counts, axis=-1))\n",
    "        \n",
    "        # 3. 计算整体012路比例\n",
    "        total_route_dist = tf.reduce_mean(tf.concat(route_distributions, axis=-1), axis=-1, keepdims=True)\n",
    "        \n",
    "        # 4. 分析路数组合特征\n",
    "        route_patterns = self._analyze_route_patterns(routes)\n",
    "        \n",
    "        # 5. 计算相邻位置的路数关系\n",
    "        route_transitions = []\n",
    "        for i in range(4):\n",
    "            transition = tf.cast(\n",
    "                routes[..., i:i+1] == routes[..., i+1:i+2],\n",
    "                tf.float32\n",
    "            )\n",
    "            route_transitions.append(transition)\n",
    "        \n",
    "        # 6. 特征组合\n",
    "        features = [\n",
    "            *route_distributions,  # 每位路数分布\n",
    "            total_route_dist,     # 整体路数比例\n",
    "            route_patterns,       # 路数组合特征\n",
    "            *route_transitions    # 相邻位置路数关系\n",
    "        ]\n",
    "        \n",
    "        return tf.concat(features, axis=-1)\n",
    "\n",
    "    def _analyze_route_patterns(self, routes):\n",
    "        \"\"\"分析路数组合模式\"\"\"\n",
    "        # 1. 全0路\n",
    "        all_zero = tf.reduce_all(routes == 0, axis=-1, keepdims=True)\n",
    "        \n",
    "        # 2. 全1路\n",
    "        all_one = tf.reduce_all(routes == 1, axis=-1, keepdims=True)\n",
    "        \n",
    "        # 3. 全2路\n",
    "        all_two = tf.reduce_all(routes == 2, axis=-1, keepdims=True)\n",
    "        \n",
    "        # 4. 012路是否均匀分布(各有至少一个)\n",
    "        has_zero = tf.reduce_any(routes == 0, axis=-1, keepdims=True)\n",
    "        has_one = tf.reduce_any(routes == 1, axis=-1, keepdims=True)\n",
    "        has_two = tf.reduce_any(routes == 2, axis=-1, keepdims=True)\n",
    "        balanced = tf.logical_and(\n",
    "            tf.logical_and(has_zero, has_one),\n",
    "            has_two\n",
    "        )\n",
    "        \n",
    "        # 5. 主路特征(出现最多的路数)\n",
    "        route_counts = []\n",
    "        for r in range(3):\n",
    "            count = tf.reduce_sum(\n",
    "                tf.cast(routes == r, tf.float32),\n",
    "                axis=-1, keepdims=True\n",
    "            )\n",
    "            route_counts.append(count)\n",
    "        main_route = tf.argmax(tf.concat(route_counts, axis=-1), axis=-1)\n",
    "        \n",
    "        return tf.concat([\n",
    "            tf.cast(all_zero, tf.float32),\n",
    "            tf.cast(all_one, tf.float32),\n",
    "            tf.cast(all_two, tf.float32),\n",
    "            tf.cast(balanced, tf.float32),\n",
    "            tf.cast(main_route, tf.float32)[..., tf.newaxis]\n",
    "        ], axis=-1)\n",
    "\n",
    "    def _build_advanced_digit_features(self, x):\n",
    "        \"\"\"构建高级数字特征\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # 1. 每位数字的独立特征\n",
    "        for i in range(5):\n",
    "            digit = x[..., i:i+1]  # 提取第i位数字\n",
    "            \n",
    "            # 数字频率统计\n",
    "            freq = tf.keras.layers.Lambda(\n",
    "                lambda x: tf.cast(tf.histogram_fixed_width(x, [0, 9], nbins=10), tf.float32)\n",
    "            )(digit)\n",
    "            \n",
    "            # 数字转换模式\n",
    "            transitions = self._build_digit_transitions(digit)\n",
    "            \n",
    "            features.extend([freq, transitions])\n",
    "        \n",
    "        # 2. 数字组合特征\n",
    "        for i in range(5):\n",
    "            for j in range(i+1, 5):\n",
    "                # 两位数字组合\n",
    "                pair = tf.stack([x[..., i], x[..., j]], axis=-1)\n",
    "                pair_features = self._build_pair_features(pair)\n",
    "                features.append(pair_features)\n",
    "        \n",
    "        # 3. 完整号码特征\n",
    "        full_number = tf.reshape(x, (-1, x.shape[1]))  # 将5位数字合并为一个完整号码\n",
    "        number_features = self._build_number_features(full_number)\n",
    "        features.append(number_features)\n",
    "        \n",
    "        return tf.keras.layers.Concatenate()(features)\n",
    "\n",
    "    def _build_digit_transitions(self, digit):\n",
    "        \"\"\"构建数字转换特征\"\"\"\n",
    "        # 计算相邻数字之间的转换\n",
    "        transitions = digit[:, 1:] - digit[:, :-1]\n",
    "        # 转换为one-hot编码\n",
    "        transitions_one_hot = tf.one_hot(tf.cast(transitions + 9, tf.int32), 19)  # -9到9共19种可能\n",
    "        return tf.reduce_mean(transitions_one_hot, axis=1)\n",
    "\n",
    "    def _build_pair_features(self, pair):\n",
    "        \"\"\"构建数字对特征\"\"\"\n",
    "        # 计算数字对的差值\n",
    "        diff = tf.abs(pair[..., 0] - pair[..., 1])\n",
    "        # 计算数字对的和\n",
    "        sum_pair = pair[..., 0] + pair[..., 1]\n",
    "        # 计算数字对的乘积\n",
    "        prod = pair[..., 0] * pair[..., 1]\n",
    "        \n",
    "        return tf.stack([diff, sum_pair, prod], axis=-1)\n",
    "\n",
    "    def _build_number_features(self, numbers):\n",
    "        \"\"\"构建完整号码特征\"\"\"\n",
    "        # 1. 计算整体统计特征\n",
    "        mean = tf.reduce_mean(numbers, axis=-1, keepdims=True)\n",
    "        std = tf.math.reduce_std(numbers, axis=-1, keepdims=True)\n",
    "        \n",
    "        # 2. 计算号码的数字频率分布\n",
    "        freq_dist = tf.keras.layers.Lambda(\n",
    "            lambda x: tf.cast(tf.histogram_fixed_width(x, [0, 9], nbins=10), tf.float32)\n",
    "        )(numbers)\n",
    "        \n",
    "        return tf.concat([mean, std, freq_dist], axis=-1)\n",
    "\n",
    "    def _analyze_frequency(self, x):\n",
    "        \"\"\"分析号码频率\"\"\"\n",
    "        # 转换为整数类型\n",
    "        x = tf.cast(x, tf.int32)\n",
    "        \n",
    "        # 计算每个数字的出现频率\n",
    "        freq = tf.zeros_like(x, dtype=tf.float32)\n",
    "        for i in range(10):\n",
    "            mask = tf.cast(x == i, tf.float32)\n",
    "            freq += mask * tf.reduce_mean(mask, axis=1, keepdims=True)\n",
    "        \n",
    "        return freq\n",
    "\n",
    "    def _analyze_number_patterns(self, x):\n",
    "        \"\"\"分析号码形态\"\"\"\n",
    "        # 1. 连号分析\n",
    "        consecutive = tf.reduce_sum(tf.cast(\n",
    "            x[:, 1:] == x[:, :-1] + 1, tf.float32\n",
    "        ), axis=-1, keepdims=True)\n",
    "        \n",
    "        # 2. 重复号分析\n",
    "        unique_counts = tf.reduce_sum(tf.one_hot(tf.cast(x, tf.int32), 10), axis=-2)\n",
    "        repeats = tf.reduce_sum(tf.cast(unique_counts > 1, tf.float32), axis=-1, keepdims=True)\n",
    "        \n",
    "        # 3. 形态识别\n",
    "        patterns = self._identify_patterns(x)\n",
    "        \n",
    "        return tf.concat([consecutive, repeats, patterns], axis=-1)\n",
    "\n",
    "    def _identify_patterns(self, x):\n",
    "        \"\"\"识别号码形态\"\"\"\n",
    "        patterns = []\n",
    "        \n",
    "        # 1. 豹子号(AAAAA)\n",
    "        baozi = tf.reduce_all(x == x[..., :1], axis=-1, keepdims=True)\n",
    "        patterns.append(tf.cast(baozi, tf.float32))\n",
    "        \n",
    "        # 2. 组5(AAAAB)\n",
    "        sorted_x = tf.sort(x, axis=-1)\n",
    "        zu5 = tf.logical_and(\n",
    "            tf.reduce_sum(tf.cast(sorted_x[..., :4] == sorted_x[..., :1], tf.float32), axis=-1) == 4,\n",
    "            sorted_x[..., 4] != sorted_x[..., 0]\n",
    "        )\n",
    "        patterns.append(tf.cast(zu5, tf.float32)[..., tf.newaxis])\n",
    "        \n",
    "        # 3. 组10(AAABB)\n",
    "        zu10 = tf.logical_and(\n",
    "            tf.reduce_sum(tf.cast(sorted_x[..., :3] == sorted_x[..., :1], tf.float32), axis=-1) == 3,\n",
    "            tf.reduce_sum(tf.cast(sorted_x[..., 3:] == sorted_x[..., 3:4], tf.float32), axis=-1) == 2\n",
    "        )\n",
    "        patterns.append(tf.cast(zu10, tf.float32)[..., tf.newaxis])\n",
    "        \n",
    "        # 4. 组20(AAABC)\n",
    "        zu20 = tf.logical_and(\n",
    "            tf.reduce_sum(tf.cast(sorted_x[..., :3] == sorted_x[..., :1], tf.float32), axis=-1) == 3,\n",
    "            sorted_x[..., 3] != sorted_x[..., 4]\n",
    "        )\n",
    "        patterns.append(tf.cast(zu20, tf.float32)[..., tf.newaxis])\n",
    "        \n",
    "        # 5. 组30(AABBC)\n",
    "        zu30 = tf.logical_and(\n",
    "            tf.reduce_sum(tf.cast(sorted_x[..., :2] == sorted_x[..., :1], tf.float32), axis=-1) == 2,\n",
    "            tf.reduce_sum(tf.cast(sorted_x[..., 2:4] == sorted_x[..., 2:3], tf.float32), axis=-1) == 2\n",
    "        )\n",
    "        patterns.append(tf.cast(zu30, tf.float32)[..., tf.newaxis])\n",
    "        \n",
    "        # 6. 组60(AABCD)\n",
    "        zu60 = tf.logical_and(\n",
    "            tf.reduce_sum(tf.cast(sorted_x[..., :2] == sorted_x[..., :1], tf.float32), axis=-1) == 2,\n",
    "            tf.reduce_all(sorted_x[..., 2:] != sorted_x[..., 1:4], axis=-1)\n",
    "        )\n",
    "        patterns.append(tf.cast(zu60, tf.float32)[..., tf.newaxis])\n",
    "        \n",
    "        # 7. 组120(ABCDE)\n",
    "        zu120 = tf.reduce_all(sorted_x[..., 1:] > sorted_x[..., :-1], axis=-1, keepdims=True)\n",
    "        patterns.append(tf.cast(zu120, tf.float32))\n",
    "        \n",
    "        return tf.concat(patterns, axis=-1)\n",
    "\n",
    "    def _build_periodic_features(self, x, periods=[60, 120, 360, 720, 1440]):\n",
    "        \"\"\"构建周期性特征\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for period in periods:\n",
    "            # 1. 提取周期模式\n",
    "            pattern = self._extract_periodic_pattern(x, period)\n",
    "            \n",
    "            # 2. 周期性偏差\n",
    "            deviation = x - pattern\n",
    "            \n",
    "            # 3. 周期强度\n",
    "            strength = tf.reduce_mean(tf.abs(pattern), axis=-1, keepdims=True)\n",
    "            \n",
    "            features.extend([pattern, deviation, strength])\n",
    "            \n",
    "        return tf.keras.layers.Concatenate()(features)\n",
    "\n",
    "    def _extract_periodic_pattern(self, x, period):\n",
    "        \"\"\"提取周期性模式\"\"\"\n",
    "        # 重塑以匹配周期\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        length = tf.shape(x)[1]\n",
    "        n_periods = length // period\n",
    "        \n",
    "        # 重塑为(batch, n_periods, period, features)\n",
    "        x_reshaped = tf.reshape(x[:, :n_periods*period], \n",
    "                               (batch_size, n_periods, period, -1))\n",
    "        \n",
    "        # 计算周期内模式\n",
    "        pattern = tf.reduce_mean(x_reshaped, axis=1)  # 平均周期模式\n",
    "        variance = tf.math.reduce_variance(x_reshaped, axis=1)  # 周期变异性\n",
    "        \n",
    "        return tf.concat([pattern, variance], axis=-1)\n",
    "\n",
    "    def _build_probability_features(self, x):\n",
    "        \"\"\"构建概率分布特征\"\"\"\n",
    "        # 1. 条件概率矩阵\n",
    "        cond_matrix = tf.zeros((10, 10, 5))  # 每位数字的转移概率\n",
    "        \n",
    "        # 2. 计算历史转移概率\n",
    "        for i in range(5):\n",
    "            current = tf.cast(x[..., i], tf.int32)\n",
    "            next_digit = tf.roll(current, shift=-1, axis=0)\n",
    "            \n",
    "            # 更新转移矩阵\n",
    "            for j in range(10):\n",
    "                for k in range(10):\n",
    "                    mask_current = tf.cast(current == j, tf.float32)\n",
    "                    mask_next = tf.cast(next_digit == k, tf.float32)\n",
    "                    prob = tf.reduce_mean(mask_current * mask_next)\n",
    "                    cond_matrix = tf.tensor_scatter_nd_update(\n",
    "                        cond_matrix,\n",
    "                        [[j, k, i]],\n",
    "                        [prob]\n",
    "                    )\n",
    "        \n",
    "        return cond_matrix\n",
    "\n",
    "    def _build_statistical_features(self, x):\n",
    "        \"\"\"构建统计特征\"\"\"\n",
    "        # 1. 移动统计\n",
    "        windows = [60, 360, 720]  # 1小时、6小时、12小时\n",
    "        stats = []\n",
    "        \n",
    "        for window in windows:\n",
    "            # 移动平均\n",
    "            ma = tf.keras.layers.AveragePooling1D(\n",
    "                pool_size=window, strides=1, padding='same')(x)\n",
    "            # 移动标准差\n",
    "            std = tf.math.reduce_std(\n",
    "                tf.stack([x, ma], axis=-1), axis=-1)\n",
    "            # 移动极差\n",
    "            pooled_max = tf.keras.layers.MaxPooling1D(\n",
    "                pool_size=window, strides=1, padding='same')(x)\n",
    "            pooled_min = -tf.keras.layers.MaxPooling1D(\n",
    "                pool_size=window, strides=1, padding='same')(-x)\n",
    "            range_stat = pooled_max - pooled_min\n",
    "            \n",
    "            stats.extend([ma, std, range_stat])\n",
    "        \n",
    "        # 2. 概率分布特征\n",
    "        probs = self._build_probability_features(x)\n",
    "        \n",
    "        return tf.concat(stats + [probs], axis=-1)\n",
    "\n",
    "    def _build_trend_features(self, x):\n",
    "        \"\"\"构建趋势特征\"\"\"\n",
    "        # 1. 短期趋势\n",
    "        short_ma = tf.keras.layers.AveragePooling1D(\n",
    "            pool_size=12, strides=1, padding='same')(x)\n",
    "        short_trend = tf.sign(x - short_ma)\n",
    "        \n",
    "        # 2. 中期趋势\n",
    "        medium_ma = tf.keras.layers.AveragePooling1D(\n",
    "            pool_size=60, strides=1, padding='same')(x)\n",
    "        medium_trend = tf.sign(x - medium_ma)\n",
    "        \n",
    "        # 3. 长期趋势\n",
    "        long_ma = tf.keras.layers.AveragePooling1D(\n",
    "            pool_size=360, strides=1, padding='same')(x)\n",
    "        long_trend = tf.sign(x - long_ma)\n",
    "        \n",
    "        # 4. 趋势一致性\n",
    "        trend_consistency = tf.reduce_mean(\n",
    "            tf.cast(short_trend == medium_trend, tf.float32) * \n",
    "            tf.cast(medium_trend == long_trend, tf.float32),\n",
    "            axis=-1, keepdims=True\n",
    "        )\n",
    "        \n",
    "        return tf.concat([short_trend, medium_trend, long_trend, trend_consistency], axis=-1)\n",
    "\n",
    "    def _build_correlation_features(self, x):\n",
    "        \"\"\"构建相关性特征\"\"\"\n",
    "        # 1. 位置间相关性\n",
    "        correlations = []\n",
    "        for i in range(5):\n",
    "            for j in range(i+1, 5):\n",
    "                corr = self._compute_correlation(x[..., i], x[..., j])\n",
    "                correlations.append(corr)\n",
    "        \n",
    "        # 2. 滞后相关性\n",
    "        lag_correlations = []\n",
    "        for lag in [1, 2, 3, 5, 10]:\n",
    "            lagged_corr = self._compute_lag_correlation(x, lag)\n",
    "            lag_correlations.append(lagged_corr)\n",
    "        \n",
    "        return tf.concat([*correlations, *lag_correlations], axis=-1)\n",
    "\n",
    "    def _compute_lag_correlation(self, x, lag):\n",
    "        \"\"\"计算滞后相关性\"\"\"\n",
    "        x_current = x[:, lag:]\n",
    "        x_lagged = x[:, :-lag]\n",
    "        \n",
    "        return self._compute_correlation(x_current, x_lagged)\n",
    "\n",
    "    def _build_pattern_features(self, x):\n",
    "        \"\"\"构建形态特征分析\"\"\"\n",
    "        # 1. 当前形态识别\n",
    "        current_patterns = self._identify_patterns(x)\n",
    "        \n",
    "        # 2. 形态遗漏值分析\n",
    "        pattern_gaps = self._analyze_pattern_gaps(x)\n",
    "        \n",
    "        # 3. 形态转换规律\n",
    "        pattern_transitions = self._analyze_pattern_transitions(x)\n",
    "        \n",
    "        # 4. 形态组合特征\n",
    "        pattern_combinations = self._analyze_pattern_combinations(x)\n",
    "        \n",
    "        # 5. 形态周期性分析\n",
    "        pattern_periodicity = self._analyze_pattern_periodicity(x)\n",
    "        \n",
    "        return tf.keras.layers.Concatenate()([\n",
    "            current_patterns,\n",
    "            pattern_gaps,\n",
    "            pattern_transitions,\n",
    "            pattern_combinations,\n",
    "            pattern_periodicity\n",
    "        ])\n",
    "\n",
    "    def _analyze_pattern_combinations(self, x):\n",
    "        \"\"\"分析形态组合特征\"\"\"\n",
    "        # 1. 计算所有可能的形态组合\n",
    "        patterns = []\n",
    "        for i in range(5):\n",
    "            for j in range(i+1, 5):\n",
    "                pair = tf.stack([x[..., i], x[..., j]], axis=-1)\n",
    "                patterns.append(self._analyze_digit_pair(pair))\n",
    "                \n",
    "        # 2. 组合形态间的关联性\n",
    "        pattern_corr = tf.stack([\n",
    "            self._compute_pattern_correlation(p1, p2)\n",
    "            for i, p1 in enumerate(patterns)\n",
    "            for j, p2 in enumerate(patterns) if i < j\n",
    "        ], axis=-1)\n",
    "        \n",
    "        return tf.concat([*patterns, pattern_corr], axis=-1)\n",
    "\n",
    "    def _analyze_pattern_periodicity(self, x):\n",
    "        \"\"\"分析形态周期性\"\"\"\n",
    "        periods = [12, 24, 60, 120, 360]\n",
    "        periodicity = []\n",
    "        \n",
    "        for period in periods:\n",
    "            # 1. 周期性模式提取\n",
    "            pattern = self._extract_pattern_cycle(x, period)\n",
    "            \n",
    "            # 2. 周期强度计算\n",
    "            strength = self._compute_cycle_strength(pattern)\n",
    "            \n",
    "            # 3. 周期稳定性分析\n",
    "            stability = self._analyze_cycle_stability(pattern)\n",
    "            \n",
    "            periodicity.extend([pattern, strength, stability])\n",
    "            \n",
    "        return tf.concat(periodicity, axis=-1)\n",
    "\n",
    "    def _extract_pattern_cycle(self, x, period):\n",
    "        \"\"\"提取形态周期模式\"\"\"\n",
    "        # 1. 重塑数据以匹配周期\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        n_cycles = tf.shape(x)[1] // period\n",
    "        x_cycles = tf.reshape(x[:, :n_cycles*period], \n",
    "                            [batch_size, n_cycles, period, -1])\n",
    "        \n",
    "        # 2. 计算周期内的形态分布\n",
    "        cycle_patterns = tf.reduce_mean(x_cycles, axis=1)\n",
    "        \n",
    "        # 3. 计算周期间的变异性\n",
    "        cycle_variance = tf.math.reduce_variance(x_cycles, axis=1)\n",
    "        \n",
    "        return tf.concat([cycle_patterns, cycle_variance], axis=-1)\n",
    "\n",
    "    def _compute_cycle_strength(self, pattern):\n",
    "        \"\"\"计算周期强度\"\"\"\n",
    "        # 1. 自相关分析\n",
    "        autocorr = tf.keras.layers.Conv1D(\n",
    "            filters=1, kernel_size=pattern.shape[1],\n",
    "            padding='same'\n",
    "        )(pattern)\n",
    "        \n",
    "        # 2. 周期性强度评分\n",
    "        strength = tf.reduce_mean(tf.abs(autocorr), axis=1, keepdims=True)\n",
    "        \n",
    "        return strength\n",
    "\n",
    "    def _analyze_cycle_stability(self, pattern):\n",
    "        \"\"\"分析周期稳定性\"\"\"\n",
    "        # 1. 计算相邻周期的差异\n",
    "        diffs = pattern[:, 1:] - pattern[:, :-1]\n",
    "        \n",
    "        # 2. 计算稳定性指标\n",
    "        stability = tf.reduce_mean(tf.abs(diffs), axis=1, keepdims=True)\n",
    "        stability = tf.exp(-stability)  # 转换到0-1范围\n",
    "        \n",
    "        return stability\n",
    "\n",
    "    def _analyze_digit_pair(self, pair):\n",
    "        \"\"\"分析数字对特征\"\"\"\n",
    "        # 1. 计算数字对基本特征\n",
    "        diff = tf.abs(pair[..., 0] - pair[..., 1])\n",
    "        sum_pair = pair[..., 0] + pair[..., 1]\n",
    "        prod = pair[..., 0] * pair[..., 1]\n",
    "        \n",
    "        # 2. 计算数字对的位置关系\n",
    "        is_adjacent = tf.cast(diff == 1, tf.float32)\n",
    "        is_complementary = tf.cast(sum_pair == 9, tf.float32)\n",
    "        \n",
    "        # 3. 计算组合特征\n",
    "        features = tf.stack([\n",
    "            diff, sum_pair, prod,\n",
    "            is_adjacent, is_complementary\n",
    "        ], axis=-1)\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def _compute_pattern_correlation(self, p1, p2):\n",
    "        \"\"\"计算形态相关性\"\"\"\n",
    "        # 标准化\n",
    "        p1_norm = (p1 - tf.reduce_mean(p1)) / (tf.math.reduce_std(p1) + 1e-6)\n",
    "        p2_norm = (p2 - tf.reduce_mean(p2)) / (tf.math.reduce_std(p2) + 1e-6)\n",
    "        \n",
    "        # 计算相关系数\n",
    "        corr = tf.reduce_mean(p1_norm * p2_norm, axis=-1, keepdims=True)\n",
    "        \n",
    "        return corr\n",
    "\n",
    "    def _analyze_pattern_gaps(self, x):\n",
    "        \"\"\"分析形态遗漏值\"\"\"\n",
    "        try:\n",
    "            # 1. 获取形态\n",
    "            patterns = self._identify_patterns(x)\n",
    "            \n",
    "            # 2. 初始化遗漏值计数器\n",
    "            gap_counters = tf.zeros_like(patterns)\n",
    "            \n",
    "            # 3. 计算每种形态的遗漏值\n",
    "            def update_gaps(sequence):\n",
    "                gaps = []\n",
    "                for i in range(7):  # 7种形态\n",
    "                    last_pos = -1\n",
    "                    current_gap = 0\n",
    "                    pattern_positions = tf.where(sequence[:, i])\n",
    "                    \n",
    "                    if tf.size(pattern_positions) > 0:\n",
    "                        last_pos = tf.reduce_max(pattern_positions)\n",
    "                        current_gap = tf.shape(sequence)[0] - 1 - last_pos\n",
    "                    \n",
    "                    gaps.append(current_gap)\n",
    "                return tf.stack(gaps)\n",
    "            \n",
    "            # 4. 应用遗漏值计算\n",
    "            gaps = tf.keras.layers.Lambda(update_gaps)(patterns)\n",
    "            \n",
    "            # 5. 构建遗漏值特征\n",
    "            gap_features = []\n",
    "            \n",
    "            # 当前遗漏值\n",
    "            gap_features.append(gaps)\n",
    "            \n",
    "            # 历史最大遗漏值\n",
    "            max_gaps = tf.reduce_max(gaps, axis=0, keepdims=True)\n",
    "            gap_features.append(max_gaps)\n",
    "            \n",
    "            # 历史平均遗漏值\n",
    "            mean_gaps = tf.reduce_mean(gaps, axis=0, keepdims=True)\n",
    "            gap_features.append(mean_gaps)\n",
    "            \n",
    "            # 遗漏值分布\n",
    "            gap_dist = tf.keras.layers.Lambda(\n",
    "                lambda x: tf.cast(tf.histogram_fixed_width(x, [0, 1000], nbins=50), tf.float32)\n",
    "            )(gaps)\n",
    "            gap_features.append(gap_dist)\n",
    "            \n",
    "            return tf.concat(gap_features, axis=-1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"分析形态遗漏值时出错: {str(e)}\")\n",
    "            return tf.zeros_like(x)\n",
    "\n",
    "    def _analyze_pattern_transitions(self, x):\n",
    "        \"\"\"分析形态转换规律\"\"\"\n",
    "        try:\n",
    "            # 1. 识别所有形态\n",
    "            patterns = self._identify_patterns(x)\n",
    "            \n",
    "            # 2. 计算形态转换矩阵\n",
    "            def get_transition_matrix(sequence):\n",
    "                matrix = tf.zeros((7, 7))  # 7x7转换矩阵\n",
    "                for i in range(len(sequence)-1):\n",
    "                    current = tf.argmax(sequence[i])\n",
    "                    next_pattern = tf.argmax(sequence[i+1])\n",
    "                    matrix = tf.tensor_scatter_nd_update(\n",
    "                        matrix,\n",
    "                        [[current, next_pattern]],\n",
    "                        [1.0]\n",
    "                    )\n",
    "                return matrix\n",
    "            \n",
    "            transition_matrix = tf.keras.layers.Lambda(get_transition_matrix)(patterns)\n",
    "            \n",
    "            # 3. 提取转换特征\n",
    "            transitions = []\n",
    "            \n",
    "            # 转换概率\n",
    "            prob_matrix = transition_matrix / (tf.reduce_sum(transition_matrix, axis=-1, keepdims=True) + 1e-7)\n",
    "            transitions.append(tf.reshape(prob_matrix, [-1]))\n",
    "            \n",
    "            # 最常见转换路径\n",
    "            common_transitions = tf.reduce_max(prob_matrix, axis=-1)\n",
    "            transitions.append(common_transitions)\n",
    "            \n",
    "            # 形态稳定性(自我转换概率)\n",
    "            stability = tf.linalg.diag_part(prob_matrix)\n",
    "            transitions.append(stability)\n",
    "            \n",
    "            return tf.concat(transitions, axis=-1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"分析形态转换规律时出错: {str(e)}\")\n",
    "            return tf.zeros_like(x)\n",
    "\n",
    "    def _build_basic_features(self, x):\n",
    "        \"\"\"构建基础特征\"\"\"\n",
    "        try:\n",
    "            # 1. 时间特征\n",
    "            time_features = self._add_positional_encoding(x)\n",
    "            \n",
    "            # 2. 多尺度特征\n",
    "            multi_scale = self._build_multi_scale_features(x)\n",
    "            \n",
    "            # 3. 统计特征\n",
    "            statistical = self._build_statistical_features(x)\n",
    "            \n",
    "            # 合并所有基础特征\n",
    "            x = tf.concat([time_features, multi_scale, statistical], axis=-1)\n",
    "            return x\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建基础特征时出错: {str(e)}\")\n",
    "            return x\n",
    "\n",
    "    def _add_positional_encoding(self, x):\n",
    "        \"\"\"添加位置编码\"\"\"\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        d_model = tf.shape(x)[-1]\n",
    "        \n",
    "        position = tf.range(seq_len, dtype=tf.float32)[:, tf.newaxis]\n",
    "        div_term = tf.exp(tf.range(0, d_model, 2, dtype=tf.float32) * -(tf.math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe = tf.zeros((seq_len, d_model))\n",
    "        pe = tf.tensor_scatter_nd_update(\n",
    "            pe,\n",
    "            tf.stack([tf.range(seq_len), tf.range(0, d_model, 2)], axis=1),\n",
    "            tf.sin(position * div_term)\n",
    "        )\n",
    "        pe = tf.tensor_scatter_nd_update(\n",
    "            pe,\n",
    "            tf.stack([tf.range(seq_len), tf.range(1, d_model, 2)], axis=1),\n",
    "            tf.cos(position * div_term)\n",
    "        )\n",
    "        \n",
    "        return x + pe[tf.newaxis, :, :]\n",
    "\n",
    "    def _build_model_features(self, x):\n",
    "        \"\"\"构建模型特征\"\"\"\n",
    "        # 1. 基础特征\n",
    "        base_features = self._build_basic_features(x)\n",
    "        \n",
    "        # 2. 时序特征\n",
    "        temporal_features = self._build_temporal_features(x)\n",
    "        \n",
    "        # 3. 模式特征\n",
    "        pattern_features = self._build_pattern_features(x)\n",
    "        \n",
    "        # 4. 高阶特征组合\n",
    "        combined_features = self._build_combined_features([\n",
    "            base_features,\n",
    "            temporal_features, \n",
    "            pattern_features\n",
    "        ])\n",
    "        \n",
    "        return combined_features\n",
    "\n",
    "    def _build_combined_features(self, feature_list):\n",
    "        \"\"\"构建高阶特征组合\"\"\"\n",
    "        try:\n",
    "            # 1. 特征连接\n",
    "            x = tf.keras.layers.Concatenate()(feature_list)\n",
    "            \n",
    "            # 2. 非线性变换\n",
    "            x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "            x = tf.keras.layers.Dropout(0.2)(x)\n",
    "            \n",
    "            # 3. 特征交互\n",
    "            x = self._build_feature_interactions(x)\n",
    "            \n",
    "            return x\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建组合特征时出错: {str(e)}\")\n",
    "            return tf.zeros_like(x)\n",
    "\n",
    "    def _build_feature_interactions(self, x):\n",
    "        \"\"\"构建特征交互\"\"\"\n",
    "        try:\n",
    "            # 1. 自注意力交互\n",
    "            att = tf.keras.layers.MultiHeadAttention(\n",
    "                num_heads=4,\n",
    "                key_dim=32\n",
    "            )(x, x)\n",
    "            x = tf.keras.layers.Add()([x, att])\n",
    "            x = tf.keras.layers.LayerNormalization()(x)\n",
    "            \n",
    "            # 2. 非线性特征组合\n",
    "            x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "            x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "            \n",
    "            return x\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建特征交互时出错: {str(e)}\")\n",
    "            return tf.zeros_like(x)\n",
    "\n",
    "    def _build_temporal_features(self, x):\n",
    "        \"\"\"构建时序特征\"\"\"\n",
    "        try:\n",
    "            # 1. 时间编码\n",
    "            time_encoding = self._add_temporal_encoding(x)\n",
    "            \n",
    "            # 2. 周期特征\n",
    "            periodic = self._build_periodic_features(x)\n",
    "            \n",
    "            # 3. 趋势特征\n",
    "            trend = self._build_trend_features(x)\n",
    "            \n",
    "            # 4. 相关性特征\n",
    "            correlation = self._build_correlation_features(x)\n",
    "            \n",
    "            return tf.concat([time_encoding, periodic, trend, correlation], axis=-1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建时序特征时出错: {str(e)}\")\n",
    "            return x\n",
    "\n",
    "    def _add_temporal_encoding(self, x):\n",
    "        \"\"\"添加时间编码\"\"\"\n",
    "        try:\n",
    "            seq_len = tf.shape(x)[1]\n",
    "            d_model = tf.shape(x)[-1]\n",
    "            \n",
    "            # 1. 位置编码\n",
    "            position = tf.range(seq_len, dtype=tf.float32)[:, tf.newaxis]\n",
    "            div_term = tf.exp(tf.range(0, d_model, 2, dtype=tf.float32) * -(tf.math.log(10000.0) / d_model))\n",
    "            \n",
    "            pe = tf.zeros((seq_len, d_model))\n",
    "            pe = tf.tensor_scatter_nd_update(\n",
    "                pe,\n",
    "                tf.stack([tf.range(seq_len), tf.range(0, d_model, 2)], axis=1),\n",
    "                tf.sin(position * div_term)\n",
    "            )\n",
    "            pe = tf.tensor_scatter_nd_update(\n",
    "                pe,\n",
    "                tf.stack([tf.range(seq_len), tf.range(1, d_model, 2)], axis=1),\n",
    "                tf.cos(position * div_term)\n",
    "            )\n",
    "            \n",
    "            # 2. 周期性时间特征\n",
    "            day_in_week = tf.cast(tf.math.floormod(position, 7), tf.float32) / 7.0\n",
    "            hour_in_day = tf.cast(tf.math.floormod(position, 24), tf.float32) / 24.0\n",
    "            minute_in_hour = tf.cast(tf.math.floormod(position, 60), tf.float32) / 60.0\n",
    "            \n",
    "            time_features = tf.concat([\n",
    "                pe,\n",
    "                day_in_week,\n",
    "                hour_in_day,\n",
    "                minute_in_hour\n",
    "            ], axis=-1)\n",
    "            \n",
    "            return time_features[tf.newaxis, :, :]\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"添加时间编码时出错: {str(e)}\")\n",
    "            return x\n",
    "\n",
    "    def evaluate_model_feature_importance(self, model, X):\n",
    "        \"\"\"评估模型特征重要性\"\"\"\n",
    "        try:\n",
    "            # 使用模型的权重评估特征重要性\n",
    "            feature_weights = model.get_layer('feature_layer').get_weights()[0]\n",
    "            importance = np.abs(feature_weights).mean(axis=1)\n",
    "            return importance\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"评估特征重要性时出错: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def select_top_features(self, importance, top_k=10):\n",
    "        \"\"\"选择最重要的特征\"\"\"\n",
    "        try:\n",
    "            top_indices = np.argsort(importance)[-top_k:]\n",
    "            return top_indices\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"选择重要特征时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _build_interaction_features(self, features):\n",
    "        \"\"\"构建交互特征\"\"\"\n",
    "        interactions = []\n",
    "        for i in range(len(features)):\n",
    "            for j in range(i + 1, len(features)):\n",
    "                interaction = features[i] * features[j]\n",
    "                interactions.append(interaction)\n",
    "        return np.array(interactions)\n",
    "\n",
    "    def _extract_derived_features(self, base_features):\n",
    "        \"\"\"提取衍生特征\"\"\"\n",
    "        derived = []\n",
    "        for feature in base_features:\n",
    "            derived.extend([\n",
    "                np.square(feature),\n",
    "                np.sqrt(np.abs(feature)),\n",
    "                np.log1p(np.abs(feature))\n",
    "            ])\n",
    "        return np.array(derived)\n",
    "\n",
    "    def _compute_feature_crosses(self, features):\n",
    "        \"\"\"计算特征交叉\"\"\"\n",
    "        crosses = []\n",
    "        for i in range(len(features)):\n",
    "            for j in range(i + 1, len(features)):\n",
    "                cross = np.outer(features[i], features[j]).ravel()\n",
    "                crosses.append(cross)\n",
    "        return np.array(crosses)\n",
    "\n",
    "# 创建全局实例\n",
    "feature_engineering = FeatureEngineering()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b93e121-57c7-4035-be63-e31e392b9920",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Mr Xu\\AppData\\Local\\Temp\\ipykernel_16348\\1494024.py:3: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\ruanjiananzhuang\\anaconda\\envs\\ace\\Lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From D:\\ruanjiananzhuang\\anaconda\\envs\\ace\\Lib\\site-packages\\keras\\src\\utils\\version_utils.py:76: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Mr Xu\\AppData\\Local\\Temp\\ipykernel_16348\\1494024.py:56: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "输入形状: (1, 14400, 5)\n",
      "编码后形状: (1, 14400, 5)\n",
      "编码示例(前3个时间步):\n",
      " [[ 0.8466895   1.3766761   0.15630183  1.8638542   0.31040782]\n",
      " [ 0.84151846  1.2634137   0.82372415  1.9615151   0.03049649]\n",
      " [ 0.9386499  -0.27177608  0.28308994  1.6084507   0.5598241 ]]\n"
     ]
    }
   ],
   "source": [
    "#6 Model Core System / 模型核心系统\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MultiHeadAttention, LayerNormalization, Bidirectional, Add\n",
    "from typing import Dict, Any, Optional\n",
    "from cell1_core import core_manager\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import math\n",
    "\n",
    "# 获取logger实例 \n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ModelCore:\n",
    "    \"\"\"整合后的模型核心类\"\"\"\n",
    "    \n",
    "    # 1. 配置管理 (from model_config.py)\n",
    "    def __init__(self, config_path: Optional[str] = None):\n",
    "        \"\"\"初始化模型核心类\n",
    "        Args:\n",
    "            config_path: 配置文件路径,如果为None则使用默认配置\n",
    "        \"\"\"\n",
    "        # 从base_model.py继承的属性\n",
    "        self.sequence_length = 14400\n",
    "        self.feature_dim = 5\n",
    "        self.prediction_range = 2880\n",
    "        self.performance_history = []\n",
    "        \n",
    "        # 从model_config.py继承的属性\n",
    "        self.config_path = config_path\n",
    "        self.config = self._load_config()\n",
    "        self.input_shape = core_manager.SYSTEM_CONFIG['SAMPLE_CONFIG']['input_length']\n",
    "        \n",
    "        # 添加预测相关的属性\n",
    "        self.prediction_history = deque(maxlen=1000)  # 预测历史记录\n",
    "        self.prediction_cache = {}  # 预测结果缓存\n",
    "        self.cache_timeout = 300   # 缓存超时时间(秒)\n",
    "        self.confidence_threshold = 0.8  # 置信度阈值\n",
    "\n",
    "        # 确保在初始化时创建新图\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            self.default_config = {\n",
    "                'filters': 64,\n",
    "                'units': 128,\n",
    "                'dense_units': 64\n",
    "            }\n",
    "            self.models = [self._build_model(self.default_config) for _ in range(6)]\n",
    "        self.session = tf.compat.v1.Session(graph=self.graph)\n",
    "        tf.compat.v1.keras.backend.set_session(self.session)\n",
    "        \n",
    "        # 初始化优化器\n",
    "        self.optimizer = self._create_optimizer()\n",
    "        \n",
    "        logger.info(\"模型核心类初始化完成\")\n",
    "\n",
    "    def _load_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"从model_config.py继承的配置加载方法\"\"\"\n",
    "        try:\n",
    "            if self.config_path and os.path.exists(self.config_path):\n",
    "                with open(self.config_path, 'r', encoding='utf-8') as f:\n",
    "                    config = json.load(f)\n",
    "                logger.info(f\"从{self.config_path}加载配置\")\n",
    "                return config\n",
    "            \n",
    "            # 使用默认配置\n",
    "            return self._get_default_config()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"加载配置失败: {str(e)}\")\n",
    "            return self._get_default_config()\n",
    "\n",
    "    def _get_default_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取默认配置\"\"\"\n",
    "        return {\n",
    "            'optimizer_config': {\n",
    "                'learning_rate': 0.001,\n",
    "                'beta_1': 0.9,\n",
    "                'beta_2': 0.999\n",
    "            },\n",
    "            'architecture_config': {\n",
    "                'model_1': {'filters': 64, 'units': 128, 'dense_units': 64}\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def get_model_config(self, model_index: int) -> Dict[str, Any]:\n",
    "        \"\"\"获取指定模型配置 (from model_config.py)\"\"\"\n",
    "        try:\n",
    "            return self.config['architecture_config'][f'model_{model_index}']\n",
    "        except KeyError:\n",
    "            logger.error(f\"未找到模型{model_index}的配置\")\n",
    "            return {}\n",
    "\n",
    "    def validate_config(self) -> bool:\n",
    "        \"\"\"验证配置有效性 (from model_config.py)\"\"\"\n",
    "        try:\n",
    "            # 验证基础配置\n",
    "            base_config = self.config['base_config']\n",
    "            assert base_config['sequence_length'] > 0\n",
    "            assert base_config['feature_dim'] > 0\n",
    "            assert base_config['batch_size'] > 0\n",
    "            \n",
    "            # 验证优化器配置\n",
    "            optimizer_config = self.config['optimizer_config']\n",
    "            assert optimizer_config['learning_rate'] > 0\n",
    "            assert 0 < optimizer_config['beta_1'] < 1\n",
    "            assert 0 < optimizer_config['beta_2'] < 1\n",
    "            \n",
    "            # 验证模型架构配置\n",
    "            for model_name, model_config in self.config['architecture_config'].items():\n",
    "                assert all(value > 0 for value in model_config.values() if isinstance(value, (int, float)))\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"配置验证失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def update_config(self, new_config: Dict[str, Any]) -> None:\n",
    "        \"\"\"更新配置 (from model_config.py)\"\"\"\n",
    "        try:\n",
    "            self.config.update(new_config)\n",
    "            self._save_config()\n",
    "            logger.info(\"配置更新成功\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"更新配置失败: {str(e)}\")\n",
    "\n",
    "    def _save_config(self) -> None:\n",
    "        \"\"\"保存配置到文件 (from model_config.py)\"\"\"\n",
    "        try:\n",
    "            if self.config_path:\n",
    "                with open(self.config_path, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(self.config, f, indent=4)\n",
    "                logger.info(f\"配置已保存到{self.config_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"保存配置失败: {str(e)}\")\n",
    "\n",
    "    def get_optimizer_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取优化器配置\"\"\"\n",
    "        return self.config['optimizer_config']\n",
    "    \n",
    "    def get_loss_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取损失函数配置\"\"\"\n",
    "        return self.config['loss_config']\n",
    "    \n",
    "    def get_training_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取训练配置\"\"\"\n",
    "        return self.config['training_config']\n",
    "    \n",
    "    def get_ensemble_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取集成配置\"\"\"\n",
    "        return self.config['ensemble_config']\n",
    "    \n",
    "    def get_monitor_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取监控配置\"\"\"\n",
    "        return self.config['monitor_config']\n",
    "\n",
    "    def _validate_model_architecture(self, architecture: Dict[str, Any]) -> bool:\n",
    "        \"\"\"验证模型架构配置\"\"\"\n",
    "        try:\n",
    "            # 检查必需的层配置\n",
    "            required_layers = ['lstm_units', 'attention_heads', 'dense_units']\n",
    "            for layer in required_layers:\n",
    "                assert any(layer in model for model in architecture.values())\n",
    "            \n",
    "            # 检查层参数范围\n",
    "            for model_config in architecture.values():\n",
    "                assert 32 <= model_config.get('lstm_units', 64) <= 512\n",
    "                assert 2 <= model_config.get('attention_heads', 4) <= 16\n",
    "                assert 16 <= model_config.get('dense_units', 32) <= 256\n",
    "                \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"模型架构验证失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _validate_training_strategy(self, strategy: Dict[str, Any]) -> bool:\n",
    "        \"\"\"验证训练策略配置\"\"\"\n",
    "        try:\n",
    "            assert 0 < strategy['learning_rate'] < 1\n",
    "            assert 16 <= strategy['batch_size'] <= 512\n",
    "            assert 0 < strategy['dropout_rate'] < 1\n",
    "            assert strategy['early_stopping_patience'] > 0\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"训练策略验证失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _validate_preprocessing_config(self) -> bool:\n",
    "        \"\"\"验证预处理配置\"\"\"\n",
    "        try:\n",
    "            preprocess_cfg = self.config['preprocessing_config']\n",
    "            assert 'sequence_length' in preprocess_cfg\n",
    "            assert 'sliding_window' in preprocess_cfg\n",
    "            assert 'normalization' in preprocess_cfg\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"预处理配置验证失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _validate_ensemble_config(self) -> bool:\n",
    "        \"\"\"验证集成配置\"\"\"\n",
    "        try:\n",
    "            ensemble_cfg = self.config['ensemble_config']\n",
    "            assert 'voting_method' in ensemble_cfg\n",
    "            assert ensemble_cfg['voting_method'] in ['majority', 'weighted', 'average']\n",
    "            assert 0 < ensemble_cfg['min_weight'] < ensemble_cfg['max_weight'] < 1\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"集成配置验证失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    # 2. 特征工程 (from base_model.py)\n",
    "    def _build_basic_features(self, x):\n",
    "        \"\"\"构建基础特征 (from base_model.py)\"\"\"\n",
    "        try:\n",
    "            # 1. 时间特征\n",
    "            time_features = self._add_positional_encoding(x)\n",
    "            \n",
    "            # 2. 多尺度特征\n",
    "            multi_scale = self._build_multi_scale_features(x)\n",
    "            \n",
    "            # 3. 统计特征\n",
    "            statistical = self._build_statistical_features(x)\n",
    "            \n",
    "            # 合并所有基础特征\n",
    "            x = tf.concat([time_features, multi_scale, statistical], axis=-1)\n",
    "            return x\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建基础特征时出错: {str(e)}\")\n",
    "            return x\n",
    "\n",
    "    def _add_positional_encoding(self, x):\n",
    "        \"\"\"时序位置编码 (修正序列长度计算)\"\"\"\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        d_model = tf.shape(x)[-1]\n",
    "        \n",
    "        position = tf.range(seq_len, dtype=tf.float32)[:, tf.newaxis]\n",
    "        \n",
    "        # 修正：先计算常量部分\n",
    "        log_value = tf.cast(-math.log(10000.0), tf.float32)\n",
    "        d_model_float = tf.cast(d_model, tf.float32)\n",
    "        angle_factor = log_value / d_model_float\n",
    "        \n",
    "        # 修正：计算正确的序列长度\n",
    "        d_model_half = tf.cast(tf.math.floor(d_model_float / 2), tf.int32)\n",
    "        total_length = seq_len * d_model_half\n",
    "        \n",
    "        # 生成角度基数\n",
    "        angle_rads = tf.range(0, d_model_float, 2.0) * angle_factor\n",
    "        div_term = tf.exp(angle_rads)\n",
    "        \n",
    "        pe = tf.zeros((seq_len, d_model))\n",
    "        \n",
    "        # 正弦部分 - 确保长度匹配\n",
    "        indices_sin = tf.stack([\n",
    "            tf.repeat(tf.range(seq_len), d_model_half),\n",
    "            tf.tile(tf.range(0, d_model, 2)[:d_model_half], [seq_len])\n",
    "        ], axis=1)\n",
    "        \n",
    "        updates_sin = tf.sin(tf.reshape(position, [-1, 1]) * div_term[:d_model_half])\n",
    "        pe = tf.tensor_scatter_nd_update(pe, indices_sin, tf.reshape(updates_sin, [-1]))\n",
    "        \n",
    "        # 余弦部分 - 确保长度匹配\n",
    "        indices_cos = tf.stack([\n",
    "            tf.repeat(tf.range(seq_len), d_model_half),\n",
    "            tf.tile(tf.range(1, d_model, 2)[:d_model_half], [seq_len])\n",
    "        ], axis=1)\n",
    "        \n",
    "        updates_cos = tf.cos(tf.reshape(position, [-1, 1]) * div_term[:d_model_half])\n",
    "        pe = tf.tensor_scatter_nd_update(pe, indices_cos, tf.reshape(updates_cos, [-1]))\n",
    "        \n",
    "        return x + pe[tf.newaxis, :, :]\n",
    "\n",
    "    def _build_multi_scale_features(self, x):\n",
    "        \"\"\"多尺度特征提取 (from base_model.py)\"\"\"\n",
    "        try:\n",
    "            conv1 = Conv1D(32, kernel_size=3, padding='same')(x)\n",
    "            conv2 = Conv1D(32, kernel_size=5, padding='same')(x)\n",
    "            conv3 = Conv1D(32, kernel_size=7, padding='same')(x)\n",
    "            dconv1 = Conv1D(32, kernel_size=3, dilation_rate=2, padding='same')(x)\n",
    "            dconv2 = Conv1D(32, kernel_size=5, dilation_rate=2, padding='same')(x)\n",
    "            return tf.keras.layers.Concatenate()([conv1, conv2, conv3, dconv1, dconv2])\n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建多尺度特征时出错: {str(e)}\")\n",
    "            return x\n",
    "\n",
    "    def _build_statistical_features(self, x):\n",
    "        \"\"\"统计特征提取 (from base_model.py)\"\"\"\n",
    "        try:\n",
    "            mean = tf.reduce_mean(x, axis=1, keepdims=True)\n",
    "            std = tf.math.reduce_std(x, axis=1, keepdims=True)\n",
    "            kurtosis = tf.reduce_mean(tf.pow(x - mean, 4), axis=1, keepdims=True) / tf.pow(std, 4)\n",
    "            skewness = tf.reduce_mean(tf.pow(x - mean, 3), axis=1, keepdims=True) / tf.pow(std, 3)\n",
    "            return tf.keras.layers.Concatenate()([mean, std, kurtosis, skewness])\n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建统计特征时出错: {str(e)}\")\n",
    "            return x\n",
    "\n",
    "    def _build_temporal_features(self, x):\n",
    "        \"\"\"构建时序特征\"\"\"\n",
    "        try:\n",
    "            # 1. 时间编码\n",
    "            time_encoding = self._add_temporal_encoding(x)\n",
    "            \n",
    "            # 2. 周期特征\n",
    "            periodic = self._build_periodic_features(x)\n",
    "            \n",
    "            # 3. 趋势特征\n",
    "            trend = self._build_trend_features(x)\n",
    "            \n",
    "            # 4. 相关性特征\n",
    "            correlation = self._build_correlation_features(x)\n",
    "            \n",
    "            return tf.concat([time_encoding, periodic, trend, correlation], axis=-1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建时序特征时出错: {str(e)}\")\n",
    "            return x\n",
    "\n",
    "    def _build_periodic_features(self, x, periods=[60, 120, 360, 720, 1440]):\n",
    "        \"\"\"构建周期性特征\"\"\"\n",
    "        features = []\n",
    "        for period in periods:\n",
    "            pattern = self._extract_periodic_pattern(x, period)\n",
    "            deviation = x - pattern\n",
    "            strength = tf.reduce_mean(tf.abs(pattern), axis=-1, keepdims=True)\n",
    "            features.extend([pattern, deviation, strength])\n",
    "        return tf.keras.layers.Concatenate()(features)\n",
    "\n",
    "    def _build_correlation_features(self, x):\n",
    "        \"\"\"构建相关性特征\"\"\"\n",
    "        # 1. 位置间相关性\n",
    "        correlations = []\n",
    "        for i in range(5):\n",
    "            for j in range(i+1, 5):\n",
    "                corr = self._compute_correlation(x[..., i], x[..., j])\n",
    "                correlations.append(corr)\n",
    "        \n",
    "        # 2. 滞后相关性\n",
    "        lag_correlations = []\n",
    "        for lag in [1, 2, 3, 5, 10]:\n",
    "            lagged_corr = self._compute_lag_correlation(x, lag)\n",
    "            lag_correlations.append(lagged_corr)\n",
    "        \n",
    "        return tf.concat([*correlations, *lag_correlations], axis=-1)\n",
    "\n",
    "    def _compute_correlation(self, x1, x2):\n",
    "        \"\"\"计算相关系数\"\"\"\n",
    "        x1_norm = (x1 - tf.reduce_mean(x1)) / tf.math.reduce_std(x1)\n",
    "        x2_norm = (x2 - tf.reduce_mean(x2)) / tf.math.reduce_std(x2)\n",
    "        return tf.reduce_mean(x1_norm * x2_norm, axis=-1, keepdims=True)\n",
    "\n",
    "    def _compute_lag_correlation(self, x, lag):\n",
    "        \"\"\"计算滞后相关性\"\"\"\n",
    "        x_current = x[:, lag:]\n",
    "        x_lagged = x[:, :-lag]\n",
    "        return self._compute_correlation(x_current, x_lagged)\n",
    "\n",
    "    def _extract_periodic_pattern(self, x, period):\n",
    "        \"\"\"提取周期性模式\"\"\"\n",
    "        try:\n",
    "            batch_size = tf.shape(x)[0]\n",
    "            length = tf.shape(x)[1]\n",
    "            n_periods = length // period\n",
    "            \n",
    "            x_reshaped = tf.reshape(x[:, :n_periods*period], \n",
    "                               (batch_size, n_periods, period, -1))\n",
    "            pattern = tf.reduce_mean(x_reshaped, axis=1)\n",
    "            return pattern\n",
    "        except Exception as e:\n",
    "            logger.error(f\"提取周期模式时出错: {str(e)}\")\n",
    "            return x\n",
    "\n",
    "    def _add_temporal_encoding(self, x):\n",
    "        \"\"\"添加时间编码\"\"\"\n",
    "        try:\n",
    "            seq_len = tf.shape(x)[1]\n",
    "            d_model = tf.shape(x)[-1]\n",
    "            \n",
    "            # 1. 位置编码\n",
    "            position = tf.range(seq_len, dtype=tf.float32)[:, tf.newaxis]\n",
    "            div_term = tf.exp(\n",
    "                tf.range(0, d_model, 2, dtype=tf.float32) * \n",
    "                (-math.log(10000.0) / d_model)\n",
    "            )\n",
    "            \n",
    "            pe = tf.zeros((seq_len, d_model))\n",
    "            pe = tf.tensor_scatter_nd_update(\n",
    "                pe,\n",
    "                tf.stack([tf.range(seq_len), tf.range(0, d_model, 2)], axis=1),\n",
    "                tf.sin(position * div_term)\n",
    "            )\n",
    "            pe = tf.tensor_scatter_nd_update(\n",
    "                pe,\n",
    "                tf.stack([tf.range(seq_len), tf.range(1, d_model, 2)], axis=1),\n",
    "                tf.cos(position * div_term)\n",
    "            )\n",
    "            \n",
    "            # 2. 添加时间周期特征\n",
    "            day_in_week = tf.cast(tf.math.floormod(position, 7), tf.float32) / 7.0\n",
    "            hour_in_day = tf.cast(tf.math.floormod(position, 24), tf.float32) / 24.0\n",
    "            minute_in_hour = tf.cast(tf.math.floormod(position, 60), tf.float32) / 60.0\n",
    "            \n",
    "            time_features = tf.concat([\n",
    "                pe,\n",
    "                day_in_week,\n",
    "                hour_in_day,\n",
    "                minute_in_hour\n",
    "            ], axis=-1)\n",
    "            \n",
    "            return time_features[tf.newaxis, :, :]\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"添加时间编码时出错: {str(e)}\")\n",
    "            return x\n",
    "\n",
    "    def _adjust_sequence_length(self, x):\n",
    "        \"\"\"调整序列长度\"\"\"\n",
    "        current_length = x.shape[1]\n",
    "        if current_length > self.prediction_range:\n",
    "            x = tf.keras.layers.AveragePooling1D(\n",
    "                pool_size=current_length // self.prediction_range)(x)\n",
    "        elif current_length < self.prediction_range:\n",
    "            x = tf.keras.layers.UpSampling1D(\n",
    "                size=self.prediction_range // current_length)(x)\n",
    "        return x\n",
    "\n",
    "    def _build_pattern_features(self, x):\n",
    "        \"\"\"构建形态特征\"\"\"\n",
    "        try:\n",
    "            # 1. 连号分析\n",
    "            consecutive = tf.reduce_sum(tf.cast(\n",
    "                x[:, 1:] == x[:, :-1] + 1, tf.float32\n",
    "            ), axis=-1, keepdims=True)\n",
    "            \n",
    "            # 2. 重复号分析\n",
    "            unique_counts = tf.reduce_sum(\n",
    "                tf.one_hot(tf.cast(x, tf.int32), 10),\n",
    "                axis=-2\n",
    "            )\n",
    "            repeats = tf.reduce_sum(\n",
    "                tf.cast(unique_counts > 1, tf.float32),\n",
    "                axis=-1, keepdims=True\n",
    "            )\n",
    "            \n",
    "            # 3. 号码分布特征\n",
    "            distribution = self._analyze_number_distribution(x)\n",
    "            \n",
    "            # 4. 形态组合特征\n",
    "            combinations = self._analyze_pattern_combinations(x)\n",
    "            \n",
    "            # 5. 形态周期规律\n",
    "            periodicity = self._analyze_pattern_periodicity(x)\n",
    "            \n",
    "            return tf.concat([\n",
    "                consecutive, \n",
    "                repeats, \n",
    "                distribution,\n",
    "                combinations,\n",
    "                periodicity\n",
    "            ], axis=-1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建形态特征时出错: {str(e)}\")\n",
    "            return x\n",
    "    \n",
    "    def _analyze_number_distribution(self, x):\n",
    "        \"\"\"分析号码分布特征\"\"\"\n",
    "        try:\n",
    "            # 1. 大小比例\n",
    "            big_nums = tf.reduce_mean(tf.cast(x >= 5, tf.float32), axis=-1, keepdims=True)\n",
    "            \n",
    "            # 2. 奇偶比例\n",
    "            odd_nums = tf.reduce_mean(tf.cast(x % 2 == 1, tf.float32), axis=-1, keepdims=True)\n",
    "            \n",
    "            # 3. 012路数分析\n",
    "            mod_3 = tf.cast(x % 3, tf.float32)\n",
    "            route_0 = tf.reduce_mean(tf.cast(mod_3 == 0, tf.float32), axis=-1, keepdims=True)\n",
    "            route_1 = tf.reduce_mean(tf.cast(mod_3 == 1, tf.float32), axis=-1, keepdims=True)\n",
    "            route_2 = tf.reduce_mean(tf.cast(mod_3 == 2, tf.float32), axis=-1, keepdims=True)\n",
    "            \n",
    "            # 4. 和值分析\n",
    "            sum_value = tf.reduce_sum(x, axis=-1, keepdims=True)\n",
    "            \n",
    "            return tf.concat([\n",
    "                big_nums, odd_nums,\n",
    "                route_0, route_1, route_2,\n",
    "                sum_value\n",
    "            ], axis=-1)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"分析号码分布特征时出错: {str(e)}\")\n",
    "            return tf.zeros_like(x[..., :1])\n",
    "\n",
    "    # 3. 模型构建 (from model_builder.py)\n",
    "    def build_model(self, model_num=None, params=None):\n",
    "        \"\"\"整合后的模型构建方法\"\"\"\n",
    "        try:\n",
    "            inputs = tf.keras.Input(shape=(self.sequence_length, self.feature_dim))\n",
    "            \n",
    "            # 1. 基础特征提取 (from base_model.py)\n",
    "            x = self._build_basic_features(inputs)\n",
    "            \n",
    "            # 2. 根据模型编号选择不同的特征提取方法 (from model_builder.py)\n",
    "            if model_num is not None:\n",
    "                x = self._build_model_specific_features(x, model_num, params or {})\n",
    "            \n",
    "            # 3. 预测输出头 (from base_model.py)\n",
    "            outputs = self._build_prediction_head(x)\n",
    "            \n",
    "            model = Model(inputs=inputs, outputs=outputs)\n",
    "            self.compile_model(model)\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建模型时出错: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _build_model_specific_features(self, x, model_num, params):\n",
    "        \"\"\"不同模型架构的特征构建 (from model_builder.py)\"\"\"\n",
    "        if model_num == 1:\n",
    "            return self._build_lstm_gru_attention(x, params)\n",
    "        elif model_num == 2:\n",
    "            return self._build_bilstm_residual(x, params)\n",
    "        elif model_num == 3:\n",
    "            return self._build_temporal_conv_lstm(x, params)\n",
    "        elif model_num == 4:\n",
    "            return self._build_transformer(x, params)\n",
    "        elif model_num == 5:\n",
    "            return self._build_gru_attention_skip(x, params)\n",
    "        elif model_num == 6:\n",
    "            return self._build_digit_correlation_model(x, params)\n",
    "        elif model_num == 7:\n",
    "            return self._build_probability_model(x, params)\n",
    "        else:\n",
    "            return self._build_lstm_cnn(x, params)\n",
    "\n",
    "    def _build_lstm_gru_attention(self, x, params):\n",
    "        \"\"\"LSTM+GRU+注意力模型 (from model_builder.py)\"\"\"\n",
    "        x = LSTM(params.get('lstm_units', 128), return_sequences=True)(x)\n",
    "        x = GRU(params.get('gru_units', 64), return_sequences=True)(x)\n",
    "        x = MultiHeadAttention(\n",
    "            num_heads=params.get('attention_heads', 4),\n",
    "            key_dim=params.get('key_dim', 16),\n",
    "            value_dim=params.get('value_dim', 16)\n",
    "        )(x, x)\n",
    "        return x\n",
    "\n",
    "    def _build_bilstm_residual(self, x, params):\n",
    "        \"\"\"BiLSTM残差网络 (from model_builder.py)\"\"\"\n",
    "        try:\n",
    "            main = Bidirectional(LSTM(params['lstm_units'], return_sequences=True))(x)\n",
    "            residual = Conv1D(params['lstm_units']*2, kernel_size=1)(x)\n",
    "            x = Add()([main, residual])\n",
    "            return LayerNormalization()(x)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建BiLSTM残差网络时出错: {str(e)}\")\n",
    "            return x\n",
    "\n",
    "    def _build_temporal_conv_lstm(self, x, params):\n",
    "        \"\"\"时空卷积LSTM (from model_builder.py)\"\"\"\n",
    "        try:\n",
    "            x = Conv1D(64, kernel_size=3, padding='same')(x)\n",
    "            x = Conv1D(64, kernel_size=3, padding='causal', dilation_rate=2)(x)\n",
    "            x = tf.keras.layers.PReLU()(x)\n",
    "            x = LSTM(64, return_sequences=True)(x)\n",
    "            return x\n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建时空卷积LSTM时出错: {str(e)}\")\n",
    "            return x\n",
    "\n",
    "    def _build_transformer(self, x, params):\n",
    "        \"\"\"构建Transformer模型 (from model_builder.py)\"\"\"\n",
    "        try:\n",
    "            x = MultiHeadAttention(\n",
    "                num_heads=params['num_heads'],\n",
    "                key_dim=params['key_dim']\n",
    "            )(x, x)\n",
    "            return LayerNormalization()(x)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建Transformer时出错: {str(e)}\")\n",
    "            return x\n",
    "\n",
    "    def _build_gru_attention_skip(self, x, params):\n",
    "        \"\"\"GRU + 自注意力 + 跳跃连接 (from model_builder.py)\"\"\"\n",
    "        try:\n",
    "            gru_out = GRU(params['gru_units'], return_sequences=True)(x)\n",
    "            att = MultiHeadAttention(num_heads=4, key_dim=16)(gru_out, gru_out)\n",
    "            x = Add()([att, x])\n",
    "            return LayerNormalization()(x)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建GRU注意力网络时出错: {str(e)}\")\n",
    "            return x\n",
    "\n",
    "    def _build_lstm_cnn(self, x, params):\n",
    "        \"\"\"LSTM + CNN模型 (from model_builder.py)\"\"\"\n",
    "        try:\n",
    "            x = LSTM(params['lstm_units'], return_sequences=True)(x)\n",
    "            x = Conv1D(filters=16, kernel_size=3, padding='same')(x)\n",
    "            return tf.keras.layers.BatchNormalization()(x)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建LSTM-CNN时出错: {str(e)}\")\n",
    "            return x\n",
    "\n",
    "    def _build_digit_correlation_model(self, x, params):\n",
    "        \"\"\"数字关联分析模型\"\"\"\n",
    "        try:\n",
    "            # 1. 相邻数字关系\n",
    "            adjacent_patterns = Conv1D(64, kernel_size=2, strides=1, padding='same')(x)\n",
    "            \n",
    "            # 2. 数字组合模式\n",
    "            combination_patterns = []\n",
    "            for window in [3, 5, 7]:\n",
    "                pattern = Conv1D(32, kernel_size=window, padding='same')(x)\n",
    "                combination_patterns.append(pattern)\n",
    "            \n",
    "            # 3. 合并所有模式\n",
    "            x = tf.keras.layers.Concatenate()([adjacent_patterns, *combination_patterns])\n",
    "            x = LSTM(128, return_sequences=True)(x)\n",
    "            return x\n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建数字关联分析模型时出错: {str(e)}\")\n",
    "            return x\n",
    "\n",
    "    def _build_probability_model(self, x, params):\n",
    "        \"\"\"概率分布学习模型\"\"\"\n",
    "        try:\n",
    "            # 1. 历史概率分布\n",
    "            hist_probs = self._build_historical_probabilities(x)\n",
    "            \n",
    "            # 2. 条件概率特征\n",
    "            cond_probs = self._build_conditional_probabilities(x)\n",
    "            \n",
    "            # 3. 组合概率模型\n",
    "            x = tf.keras.layers.Concatenate()([hist_probs, cond_probs])\n",
    "            x = Dense(256, activation='relu')(x)\n",
    "            x = Dense(128, activation='relu')(x)\n",
    "            return x\n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建概率模型时出错: {str(e)}\")\n",
    "            return x\n",
    "\n",
    "    def _build_historical_probabilities(self, x):\n",
    "        \"\"\"构建历史概率分布特征\"\"\"\n",
    "        try:\n",
    "            # 1. 计算历史频率分布\n",
    "            freqs = tf.zeros((10, 5))  # 10个数字在5个位置的频率\n",
    "            for i in range(5):\n",
    "                digit_freqs = tf.keras.layers.Lambda(\n",
    "                    lambda x: tf.cast(\n",
    "                        tf.histogram_fixed_width(x[..., i], [0, 9], nbins=10),\n",
    "                        tf.float32\n",
    "                    )\n",
    "                )(x)\n",
    "                freqs = tf.tensor_scatter_nd_update(\n",
    "                    freqs,\n",
    "                    [[j, i] for j in range(10)],\n",
    "                    digit_freqs\n",
    "                )\n",
    "            \n",
    "            # 2. 计算条件概率\n",
    "            cond_probs = self._calculate_conditional_probs(x)\n",
    "            \n",
    "            return tf.concat([freqs, cond_probs], axis=-1)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建历史概率分布特征时出错: {str(e)}\")\n",
    "            return x\n",
    "\n",
    "    def _build_conditional_probabilities(self, x):\n",
    "        \"\"\"构建条件概率特征\"\"\"\n",
    "        try:\n",
    "            # 1. 计算相邻位置条件概率\n",
    "            adjacent_probs = []\n",
    "            for i in range(4):\n",
    "                curr = tf.cast(x[..., i], tf.int32)\n",
    "                next_digit = tf.cast(x[..., i+1], tf.int32)\n",
    "                probs = self._compute_transition_probs(curr, next_digit)\n",
    "                adjacent_probs.append(probs)\n",
    "            \n",
    "            # 2. 计算跳跃位置条件概率\n",
    "            skip_probs = []\n",
    "            for i in range(3):\n",
    "                curr = tf.cast(x[..., i], tf.int32)\n",
    "                next_digit = tf.cast(x[..., i+2], tf.int32)\n",
    "                probs = self._compute_transition_probs(curr, next_digit)\n",
    "                skip_probs.append(probs)\n",
    "            \n",
    "            return tf.concat([*adjacent_probs, *skip_probs], axis=-1)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建条件概率特征时出错: {str(e)}\")\n",
    "            return x\n",
    "\n",
    "    def _compute_transition_probs(self, curr_digits, next_digits):\n",
    "        \"\"\"计算转移概率矩阵\"\"\"\n",
    "        try:\n",
    "            # 创建10x10的转移矩阵\n",
    "            transition_matrix = tf.zeros((10, 10))\n",
    "            \n",
    "            # 统计转移次数\n",
    "            for i in range(10):\n",
    "                for j in range(10):\n",
    "                    mask_curr = tf.cast(curr_digits == i, tf.float32)\n",
    "                    mask_next = tf.cast(next_digits == j, tf.float32)\n",
    "                    count = tf.reduce_sum(mask_curr * mask_next)\n",
    "                    transition_matrix = tf.tensor_scatter_nd_update(\n",
    "                        transition_matrix,\n",
    "                        [[i, j]],\n",
    "                        [count]\n",
    "                    )\n",
    "            \n",
    "            # 计算概率\n",
    "            row_sums = tf.reduce_sum(transition_matrix, axis=1, keepdims=True)\n",
    "            probs = transition_matrix / (row_sums + 1e-7)\n",
    "            \n",
    "            return probs\n",
    "        except Exception as e:\n",
    "            logger.error(f\"计算转移概率时出错: {str(e)}\")\n",
    "            return tf.zeros((10, 10))\n",
    "\n",
    "    def _build_combined_features(self, feature_list):\n",
    "        \"\"\"构建组合特征 (from model_builder.py)\"\"\"\n",
    "        try:\n",
    "            # 1. 特征连接\n",
    "            x = tf.keras.layers.Concatenate()(feature_list)\n",
    "            \n",
    "            # 2. 非线性变换\n",
    "            x = Dense(256, activation='relu')(x)\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "            x = tf.keras.layers.Dropout(0.2)(x)\n",
    "            \n",
    "            # 3. 特征交互\n",
    "            x = self._build_feature_interactions(x)\n",
    "            \n",
    "            return x\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建组合特征时出错: {str(e)}\")\n",
    "            return tf.zeros_like(x)\n",
    "\n",
    "    def _build_feature_interactions(self, x):\n",
    "        \"\"\"构建特征交互 (from model_builder.py)\"\"\"\n",
    "        try:\n",
    "            # 1. 自注意力交互\n",
    "            att = MultiHeadAttention(\n",
    "                num_heads=4,\n",
    "                key_dim=32\n",
    "            )(x, x)\n",
    "            x = Add()([x, att])\n",
    "            x = LayerNormalization()(x)\n",
    "            \n",
    "            # 2. 非线性特征组合\n",
    "            x = Dense(128, activation='relu')(x)\n",
    "            x = Dense(64, activation='relu')(x)\n",
    "            \n",
    "            return x\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建特征交互时出错: {str(e)}\")\n",
    "            return tf.zeros_like(x)\n",
    "\n",
    "    def _build_ensemble_predictor(self, x, params):\n",
    "        \"\"\"集成预测器\"\"\"\n",
    "        # 1. 多模型特征\n",
    "        features = []\n",
    "        \n",
    "        # 组合模式特征\n",
    "        comb_features = self._build_combination_predictor(x, params)\n",
    "        # 概率特征\n",
    "        prob_features = self._build_probability_predictor(x, params)\n",
    "        # 周期特征  \n",
    "        period_features = self._build_periodic_predictor(x, params)\n",
    "        # 趋势特征\n",
    "        trend_features = self._build_trend_predictor(x, params)\n",
    "        # 统计特征\n",
    "        stat_features = self._build_statistical_predictor(x, params)\n",
    "        \n",
    "        features.extend([\n",
    "            comb_features, prob_features, period_features,\n",
    "            trend_features, stat_features\n",
    "        ])\n",
    "        \n",
    "        # 2. 特征融合\n",
    "        x = tf.keras.layers.Concatenate()(features)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def _build_attention_residual_block(self, x, params):\n",
    "        \"\"\"注意力残差块\"\"\"\n",
    "        # 多头自注意力\n",
    "        att = MultiHeadAttention(\n",
    "            num_heads=params.get('num_heads', 4),\n",
    "            key_dim=params.get('key_dim', 32)\n",
    "        )(x, x)\n",
    "        \n",
    "        # 残差连接\n",
    "        x = Add()([x, att])\n",
    "        x = LayerNormalization()(x)\n",
    "        \n",
    "        # FFN\n",
    "        ffn = Dense(params.get('ffn_dim', 256), activation='relu')(x)\n",
    "        ffn = Dense(x.shape[-1])(ffn)\n",
    "        \n",
    "        # 残差连接\n",
    "        x = Add()([x, ffn])\n",
    "        x = LayerNormalization()(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def _build_combination_predictor(self, x, params):\n",
    "        \"\"\"组合数字预测器\"\"\"\n",
    "        try:\n",
    "            # 1. 局部组合模式\n",
    "            local_patterns = []\n",
    "            for window in [2, 3, 4]:\n",
    "                pattern = Conv1D(32, kernel_size=window, padding='same')(x)\n",
    "                local_patterns.append(pattern)\n",
    "            \n",
    "            # 2. 全局组合模式\n",
    "            global_pattern = self._build_attention_residual_block(\n",
    "                tf.concat(local_patterns, axis=-1),\n",
    "                params\n",
    "            )\n",
    "            \n",
    "            return global_pattern\n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建组合预测器出错: {str(e)}\")\n",
    "            return x\n",
    "\n",
    "    def _build_trend_predictor(self, x, params):\n",
    "        \"\"\"趋势预测器\"\"\"\n",
    "        try:\n",
    "            # 1. 多尺度趋势分析 \n",
    "            trends = []\n",
    "            windows = [60, 360, 720, 1440]\n",
    "            \n",
    "            for window in windows:\n",
    "                ma = tf.keras.layers.AveragePooling1D(\n",
    "                    pool_size=window, strides=1, padding='same')(x)\n",
    "                trend = tf.sign(x - ma)\n",
    "                trends.append(trend)\n",
    "            \n",
    "            # 2. 趋势特征融合\n",
    "            x = tf.keras.layers.Concatenate()(trends)\n",
    "            x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "            return x\n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建趋势预测器出错: {str(e)}\")\n",
    "            return x\n",
    "\n",
    "    def _build_statistical_predictor(self, x, params):\n",
    "        \"\"\"统计模式预测器\"\"\"\n",
    "        try:\n",
    "            stats = []\n",
    "            \n",
    "            # 均值特征\n",
    "            mean = tf.reduce_mean(x, axis=1, keepdims=True)\n",
    "            # 标准差特征\n",
    "            std = tf.math.reduce_std(x, axis=1, keepdims=True)\n",
    "            # 峰度\n",
    "            kurtosis = tf.reduce_mean(tf.pow(x - mean, 4), axis=1, keepdims=True) / tf.pow(std, 4)\n",
    "            # 偏度\n",
    "            skewness = tf.reduce_mean(tf.pow(x - mean, 3), axis=1, keepdims=True) / tf.pow(std, 3)\n",
    "            \n",
    "            stats.extend([mean, std, kurtosis, skewness])\n",
    "            \n",
    "            x = tf.keras.layers.Concatenate()(stats)\n",
    "            x = Dense(128, activation='relu')(x)\n",
    "            return x\n",
    "        except Exception as e:\n",
    "            logger.error(f\"构建统计预测器出错: {str(e)}\")\n",
    "            return x\n",
    "\n",
    "    # 4. 训练评估 (from base_model.py)\n",
    "    def _build_prediction_head(self, x):\n",
    "        \"\"\"预测输出头 (from base_model.py)\"\"\"\n",
    "        # 1. 每位数字的概率分布\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        digit_predictions = []\n",
    "        for i in range(5):\n",
    "            digit_pred = Dense(10, activation='softmax', name=f'digit_{i}')(x)\n",
    "            digit_predictions.append(digit_pred)\n",
    "\n",
    "        # 2. 完整号码匹配概率\n",
    "        match_prob = Dense(self.prediction_range, activation='sigmoid', name='match_prob')(x)\n",
    "\n",
    "        # 3. 预测置信度\n",
    "        confidence = Dense(1, activation='sigmoid', name='confidence')(x)\n",
    "        \n",
    "        return {\n",
    "            'digits': digit_predictions,\n",
    "            'match_prob': match_prob,\n",
    "            'confidence': confidence\n",
    "        }\n",
    "\n",
    "    def enhanced_match_loss(self, y_true, y_pred):\n",
    "        \"\"\"增强型匹配损失函数 (from base_model.py)\"\"\"\n",
    "        try:\n",
    "            # 1. 预处理\n",
    "            y_pred_expanded = tf.expand_dims(y_pred, axis=1)\n",
    "            y_pred_rounded = tf.round(y_pred_expanded)\n",
    "            \n",
    "            # 2. 计算匹配情况\n",
    "            matches = tf.cast(tf.equal(y_true, y_pred_rounded), tf.float32)\n",
    "            match_counts = tf.reduce_sum(matches, axis=-1)\n",
    "            best_match_indices = tf.argmax(match_counts, axis=1)\n",
    "            best_targets = tf.gather(y_true, best_match_indices, batch_dims=1)\n",
    "            best_match_counts = tf.reduce_max(match_counts, axis=1)\n",
    "            \n",
    "            # 3. 计算基础匹配损失\n",
    "            base_loss = tf.reduce_mean(tf.abs(y_pred - best_targets), axis=1)\n",
    "            \n",
    "            # 4. 计算方向性损失\n",
    "            direction_loss = self._calculate_direction_loss(y_pred, best_targets)\n",
    "            \n",
    "            # 5. 完全匹配时损失为0\n",
    "            perfect_match = tf.cast(tf.equal(best_match_counts, 5.0), tf.float32)\n",
    "            \n",
    "            # 6. 组合损失(动态权重)\n",
    "            direction_weight = tf.exp(-best_match_counts / 5.0) * 0.5\n",
    "            total_loss = base_loss * (1.0 - perfect_match) + direction_weight * direction_loss\n",
    "            \n",
    "            return total_loss\n",
    "        except Exception as e:\n",
    "            logger.error(f\"计算损失时出错: {str(e)}\")\n",
    "            return 5.0 * tf.ones_like(y_pred[:, 0])\n",
    "\n",
    "    def compile_model(self, model):\n",
    "        \"\"\"编译模型 (from base_model.py)\"\"\"\n",
    "        model.compile(\n",
    "            optimizer=self.optimizer,\n",
    "            loss=self.enhanced_match_loss,\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "        \"\"\"训练步骤 (from base_model.py)\"\"\"\n",
    "        try:\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = self.model(batch_data['input'], training=True)\n",
    "                loss = self.enhanced_match_loss(batch_data['target'], predictions)\n",
    "            gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "            return loss\n",
    "        except Exception as e:\n",
    "            logger.error(f\"训练步骤执行出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def validate_step(self, batch_data):\n",
    "        \"\"\"验证步骤 (from base_model.py)\"\"\"\n",
    "        try:\n",
    "            predictions = self.model(batch_data['input'], training=False)\n",
    "            loss = self.enhanced_match_loss(batch_data['target'], predictions)\n",
    "            matches = self._calculate_matches(predictions, batch_data['target'])\n",
    "            return {\n",
    "                'loss': loss,\n",
    "                'accuracy': tf.reduce_mean(matches)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"验证步骤执行出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _calculate_matches(self, predictions, targets):\n",
    "        \"\"\"计算匹配程度 (from base_model.py)\"\"\"\n",
    "        try:\n",
    "            rounded_preds = tf.round(predictions)\n",
    "            matches = tf.cast(tf.equal(rounded_preds, targets), tf.float32)\n",
    "            full_matches = tf.reduce_all(matches, axis=-1)\n",
    "            return full_matches\n",
    "        except Exception as e:\n",
    "            logger.error(f\"计算匹配程度时出错: {str(e)}\")\n",
    "            return tf.zeros_like(predictions[..., 0])\n",
    "\n",
    "    def _calculate_direction_loss(self, y_pred, best_targets):\n",
    "        \"\"\"计算方向性损失 (from base_model.py)\"\"\"\n",
    "        try:\n",
    "            value_diff = best_targets - y_pred\n",
    "            direction_mask = tf.cast(\n",
    "                tf.not_equal(tf.round(y_pred), best_targets),\n",
    "                tf.float32\n",
    "            )\n",
    "            direction_factor = tf.sigmoid(value_diff * 2.0) * 2.0 - 1.0\n",
    "            return tf.reduce_mean(\n",
    "                direction_mask * direction_factor * tf.abs(value_diff),\n",
    "                axis=-1\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"计算方向性损失时出错: {str(e)}\")\n",
    "            return tf.zeros_like(y_pred[:, 0])\n",
    "\n",
    "    # 5. 模型保存加载 (from base_model.py)\n",
    "    def save_model(self, path: str):\n",
    "        \"\"\"保存模型 (from base_model.py)\"\"\"\n",
    "        try:\n",
    "            self.model.save(path)\n",
    "            logger.info(f\"模型已保存到: {path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"保存模型时出错: {str(e)}\")\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        \"\"\"加载模型 (from base_model.py)\"\"\"\n",
    "        try:\n",
    "            self.model = tf.keras.models.load_model(\n",
    "                path,\n",
    "                custom_objects={'enhanced_match_loss': self.enhanced_match_loss}\n",
    "            )\n",
    "            logger.info(f\"已加载模型: {path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"加载模型时出错: {str(e)}\")\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        \"\"\"执行预测\"\"\"\n",
    "        try:\n",
    "            # 添加输入验证\n",
    "            if input_data.shape[-1] != 8:  # 5个号码+3个时间特征\n",
    "                logger.error(f\"输入特征维度错误，预期8维，实际收到{input_data.shape[-1]}维\")\n",
    "                return None\n",
    "            \n",
    "            # 获取集成预测结果\n",
    "            predictions = []\n",
    "            confidences = []\n",
    "            \n",
    "            for i, model in enumerate(self.models):\n",
    "                pred = model.predict(input_data)\n",
    "                pred_value = pred['digits']\n",
    "                confidence = pred['confidence']\n",
    "                \n",
    "                predictions.append(pred_value * self.weights[i])\n",
    "                confidences.append(confidence)\n",
    "            \n",
    "            # 集成预测结果\n",
    "            ensemble_pred = np.sum(predictions, axis=0)\n",
    "            mean_confidence = np.mean(confidences)\n",
    "            \n",
    "            return {\n",
    "                'prediction': ensemble_pred,\n",
    "                'confidence': mean_confidence\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"预测失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def predict_with_cache(self, X):\n",
    "        \"\"\"带缓存的预测\"\"\"\n",
    "        cache_key = hash(str(X))\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # 检查缓存\n",
    "        if cache_key in self.prediction_cache:\n",
    "            cached_result, cache_time = self.prediction_cache[cache_key]\n",
    "            if current_time - cache_time < self.cache_timeout:\n",
    "                return cached_result\n",
    "        \n",
    "        # 执行预测\n",
    "        result = self.predict(X)\n",
    "        \n",
    "        # 更新缓存\n",
    "        self.prediction_cache[cache_key] = (result, current_time)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def get_prediction_history(self, start_time=None, end_time=None):\n",
    "        \"\"\"获取预测历史\"\"\"\n",
    "        history = list(self.prediction_history)\n",
    "        \n",
    "        if start_time:\n",
    "            history = [h for h in history if h['timestamp'] >= start_time]\n",
    "        if end_time:\n",
    "            history = [h for h in history if h['timestamp'] <= end_time]\n",
    "            \n",
    "        return history\n",
    "\n",
    "    def analyze_prediction_accuracy(self):\n",
    "        \"\"\"分析预测准确率\"\"\"\n",
    "        if not self.prediction_history:\n",
    "            return None\n",
    "            \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for pred in self.prediction_history:\n",
    "            if pred.get('actual') is not None:\n",
    "                correct += int(np.array_equal(\n",
    "                    pred['prediction'],\n",
    "                    pred['actual']\n",
    "                ))\n",
    "                total += 1\n",
    "        \n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'total_predictions': total,\n",
    "            'correct_predictions': correct\n",
    "        }\n",
    "\n",
    "    def _record_prediction(self, prediction, confidence):\n",
    "        \"\"\"记录预测结果\"\"\"\n",
    "        try:\n",
    "            self.prediction_history.append({\n",
    "                'prediction': prediction,\n",
    "                'confidence': confidence,\n",
    "                'timestamp': datetime.now()\n",
    "            })\n",
    "            logger.info(f\"记录预测结果: {prediction}, 置信度: {confidence:.2f}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"记录预测结果失败: {str(e)}\")\n",
    "\n",
    "    def _clean_prediction_cache(self):\n",
    "        \"\"\"清理过期的预测缓存\"\"\"\n",
    "        try:\n",
    "            current_time = time.time()\n",
    "            expired_keys = [\n",
    "                k for k, (_, cache_time) in self.prediction_cache.items()\n",
    "                if current_time - cache_time > self.cache_timeout\n",
    "            ]\n",
    "            for k in expired_keys:\n",
    "                del self.prediction_cache[k]\n",
    "            if expired_keys:\n",
    "                logger.info(f\"清理了{len(expired_keys)}条过期预测缓存\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"清理预测缓存失败: {str(e)}\")\n",
    "\n",
    "    def get_prediction_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取预测统计信息\"\"\"\n",
    "        try:\n",
    "            if not self.prediction_history:\n",
    "                return {}\n",
    "                \n",
    "            recent_predictions = list(self.prediction_history)[-100:]\n",
    "            return {\n",
    "                'total_predictions': len(self.prediction_history),\n",
    "                'recent_avg_confidence': np.mean([p['confidence'] for p in recent_predictions]),\n",
    "                'cache_hit_rate': self._calculate_cache_hit_rate(),\n",
    "                'last_prediction_time': self.prediction_history[-1]['timestamp'],\n",
    "                'accuracy_stats': self.analyze_prediction_accuracy()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取预测统计失败: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "    def _calculate_cache_hit_rate(self) -> float:\n",
    "        \"\"\"计算缓存命中率\"\"\"\n",
    "        try:\n",
    "            if not hasattr(self, '_cache_stats'):\n",
    "                self._cache_stats = {'hits': 0, 'misses': 0}\n",
    "            total = self._cache_stats['hits'] + self._cache_stats['misses']\n",
    "            return self._cache_stats['hits'] / total if total > 0 else 0\n",
    "        except Exception as e:\n",
    "            logger.error(f\"计算缓存命中率失败: {str(e)}\")\n",
    "            return 0.0\n",
    "\n",
    "    def _build_model(self, config):\n",
    "        \"\"\"构建单个模型（添加编译步骤）\"\"\"\n",
    "        try:\n",
    "            with self.graph.as_default():\n",
    "                inputs = tf.keras.Input(shape=(self.input_shape, 5))\n",
    "                x = Conv1D(config['filters'], 3, activation='relu')(inputs)\n",
    "                x = LSTM(config['units'], return_sequences=True)(x)\n",
    "                x = Dense(config['dense_units'], activation='relu')(x)\n",
    "                outputs = Dense(5, activation='softmax')(x)\n",
    "                model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "                \n",
    "                # 编译模型\n",
    "                model.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(\n",
    "                        learning_rate=self.config['optimizer_config']['learning_rate']\n",
    "                    ),\n",
    "                    loss='mse',\n",
    "                    metrics=['mae']\n",
    "                )\n",
    "                return model\n",
    "            \n",
    "        except KeyError as e:\n",
    "            logger.error(f\"配置缺失关键参数: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _create_optimizer(self):\n",
    "        \"\"\"创建优化器\"\"\"\n",
    "        try:\n",
    "            opt_cfg = self.config['optimizer_config']\n",
    "            return tf.keras.optimizers.Adam(\n",
    "                learning_rate=opt_cfg['learning_rate'],\n",
    "                beta_1=opt_cfg['beta_1'],\n",
    "                beta_2=opt_cfg['beta_2']\n",
    "            )\n",
    "        except KeyError as e:\n",
    "            logger.error(f\"配置缺失关键参数: {str(e)}，使用默认优化器\")\n",
    "            return tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    def reset_models(self):\n",
    "        \"\"\"安全重置模型\"\"\"\n",
    "        try:\n",
    "            # 清理现有会话\n",
    "            if self.session:\n",
    "                self.session.close()\n",
    "            # 创建新图和新会话\n",
    "            self.graph = tf.Graph()\n",
    "            with self.graph.as_default():\n",
    "                self.models = [self._build_model(self.default_config) for _ in range(6)]\n",
    "            self.session = tf.compat.v1.Session(graph=self.graph)\n",
    "            tf.compat.v1.keras.backend.set_session(self.session)\n",
    "            logger.info(\"模型重置成功\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"模型重置失败: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# 创建全局模型核心实例\n",
    "model_core = ModelCore()\n",
    "\n",
    "# 生成测试数据\n",
    "test_input = np.random.rand(1, 14400, 5)  # 批次大小1，序列长度14400，特征维度5\n",
    "\n",
    "with model_core.session.as_default():\n",
    "    with model_core.graph.as_default():\n",
    "        # 创建输入占位符\n",
    "        input_tensor = tf.placeholder(tf.float32, shape=(1, 14400, 5))\n",
    "        # 获取编码输出\n",
    "        encoded_output = model_core._add_positional_encoding(input_tensor)\n",
    "        # 运行会话\n",
    "        result = model_core.session.run(encoded_output, feed_dict={input_tensor: test_input})\n",
    "\n",
    "print(\"输入形状:\", test_input.shape)\n",
    "print(\"编码后形状:\", result.shape)\n",
    "print(\"编码示例(前3个时间步):\\n\", result[0, :3, :5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19b90482-7c87-4487-b6f6-18063a3b764e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#7 Data Optimization Module / 数据优化模块\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataOptimizer:\n",
    "    def _evaluate_distribution(self, X):\n",
    "        \"\"\"评估数据分布情况\"\"\"\n",
    "        try:\n",
    "            # 1. 检查数据偏度\n",
    "            skewness = np.abs(np.mean([np.abs(stats.skew(X[:, i])) for i in range(X.shape[1])]))\n",
    "            skewness_score = 1 / (1 + skewness)  # 转换为0-1分数\n",
    "            \n",
    "            # 2. 检查数据峰度\n",
    "            kurtosis = np.abs(np.mean([np.abs(stats.kurtosis(X[:, i])) for i in range(X.shape[1])]))\n",
    "            kurtosis_score = 1 / (1 + kurtosis)  # 转换为0-1分数\n",
    "            \n",
    "            # 3. 检查异常值比例\n",
    "            z_scores = np.abs(stats.zscore(X))\n",
    "            outlier_ratio = np.mean(z_scores > 3)  # 3个标准差以外视为异常值\n",
    "            outlier_score = 1 - outlier_ratio\n",
    "            \n",
    "            # 计算加权平均分数\n",
    "            distribution_score = (\n",
    "                0.4 * skewness_score +\n",
    "                0.3 * kurtosis_score +\n",
    "                0.3 * outlier_score\n",
    "            )\n",
    "            \n",
    "            return distribution_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"评估数据分布时出错: {str(e)}\")\n",
    "            return 0.0\n",
    "\n",
    "    def _evaluate_correlation(self, X):\n",
    "        \"\"\"评估特征相关性\"\"\"\n",
    "        try:\n",
    "            # 1. 计算特征间相关系数矩阵\n",
    "            corr_matrix = np.corrcoef(X.T)\n",
    "            \n",
    "            # 2. 计算特征间的平均相关性\n",
    "            # 去除对角线上的1\n",
    "            mask = ~np.eye(corr_matrix.shape[0], dtype=bool)\n",
    "            avg_correlation = np.mean(np.abs(corr_matrix[mask]))\n",
    "            \n",
    "            # 3. 计算相关性得分\n",
    "            correlation_score = 1 - avg_correlation  # 越小越好\n",
    "            \n",
    "            return correlation_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"评估特征相关性时出错: {str(e)}\")\n",
    "            return 0.0\n",
    "\n",
    "    def _evaluate_time_series(self, X):\n",
    "        \"\"\"评估时间序列特性\"\"\"\n",
    "        try:\n",
    "            # 1. 检查平稳性\n",
    "            stationarity_scores = []\n",
    "            for i in range(X.shape[1]):\n",
    "                # 使用ADF测试检查平稳性\n",
    "                adf_result = adfuller(X[:, i])[1]  # 获取p值\n",
    "                stationarity_scores.append(1 - min(adf_result, 1))  # 转换为0-1分数\n",
    "            \n",
    "            stationarity_score = np.mean(stationarity_scores)\n",
    "            \n",
    "            # 2. 检查自相关性\n",
    "            autocorr_scores = []\n",
    "            for i in range(X.shape[1]):\n",
    "                # 计算滞后1期的自相关系数\n",
    "                autocorr = np.corrcoef(X[1:, i], X[:-1, i])[0, 1]\n",
    "                autocorr_scores.append(abs(autocorr))\n",
    "            \n",
    "            autocorr_score = np.mean(autocorr_scores)\n",
    "            \n",
    "            # 3. 检查趋势性\n",
    "            trend_scores = []\n",
    "            for i in range(X.shape[1]):\n",
    "                # 使用简单线性回归检测趋势\n",
    "                slope = np.polyfit(np.arange(len(X)), X[:, i], 1)[0]\n",
    "                trend_scores.append(abs(slope))\n",
    "            \n",
    "            trend_score = 1 / (1 + np.mean(trend_scores))  # 转换为0-1分数\n",
    "            \n",
    "            # 计算加权平均分数\n",
    "            time_series_score = (\n",
    "                0.4 * stationarity_score +\n",
    "                0.3 * autocorr_score +\n",
    "                0.3 * trend_score\n",
    "            )\n",
    "            \n",
    "            return time_series_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"评估时间序列特性时出错: {str(e)}\")\n",
    "            return 0.0\n",
    "\n",
    "    def _calculate_trend(self, values):\n",
    "        \"\"\"计算趋势\"\"\"\n",
    "        if len(values) < 2:\n",
    "            return \"INSUFFICIENT_DATA\"\n",
    "            \n",
    "        # 使用简单线性回归\n",
    "        x = np.arange(len(values))\n",
    "        slope = np.polyfit(x, values, 1)[0]\n",
    "        \n",
    "        if slope < -0.01:\n",
    "            return \"IMPROVING\"\n",
    "        elif slope > 0.01:\n",
    "            return \"DEGRADING\"\n",
    "        else:\n",
    "            return \"STABLE\"\n",
    "\n",
    "    def _compute_correlation(self, x1, x2):\n",
    "        \"\"\"计算相关系数\"\"\"\n",
    "        try:\n",
    "            # 标准化\n",
    "            x1_norm = (x1 - np.mean(x1)) / np.std(x1)\n",
    "            x2_norm = (x2 - np.mean(x2)) / np.std(x2)\n",
    "            \n",
    "            # 计算相关系数\n",
    "            corr = np.mean(x1_norm * x2_norm)\n",
    "            return corr\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"计算相关系数时出错: {str(e)}\")\n",
    "            return 0.0\n",
    "\n",
    "    def optimize_feature_selection(self, X, y, n_features=10):\n",
    "        \"\"\"优化特征选择\"\"\"\n",
    "        try:\n",
    "            # 1. 计算特征重要性\n",
    "            importance = self._calculate_feature_importance(X, y)\n",
    "            \n",
    "            # 2. 计算特征冗余度\n",
    "            redundancy = self._calculate_feature_redundancy(X)\n",
    "            \n",
    "            # 3. 综合评分\n",
    "            final_score = importance * (1 - redundancy)\n",
    "            \n",
    "            # 4. 选择最优特征\n",
    "            selected = np.argsort(final_score)[-n_features:]\n",
    "            \n",
    "            return selected, final_score[selected]\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"优化特征选择时出错: {str(e)}\")\n",
    "            return None, None\n",
    "    \n",
    "    def _calculate_feature_importance(self, X, y):\n",
    "        \"\"\"计算特征重要性分数\"\"\"\n",
    "        try:\n",
    "            correlations = []\n",
    "            for i in range(X.shape[1]):\n",
    "                corr = np.abs(np.corrcoef(X[:, i], y)[0, 1])\n",
    "                correlations.append(corr)\n",
    "            return np.array(correlations)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"计算特征重要性时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _calculate_feature_redundancy(self, X):\n",
    "        \"\"\"计算特征冗余度\"\"\"\n",
    "        try:\n",
    "            n_features = X.shape[1]\n",
    "            redundancy = np.zeros(n_features)\n",
    "            \n",
    "            for i in range(n_features):\n",
    "                correlations = []\n",
    "                for j in range(n_features):\n",
    "                    if i != j:\n",
    "                        corr = np.abs(np.corrcoef(X[:, i], X[:, j])[0, 1])\n",
    "                        correlations.append(corr)\n",
    "                redundancy[i] = np.mean(correlations)\n",
    "            \n",
    "            return redundancy\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"计算特征冗余度时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def evaluate_feature_correlation(self, features):\n",
    "        \"\"\"评估特征相关性矩阵\"\"\"\n",
    "        try:\n",
    "            corr_matrix = np.corrcoef(features.T)\n",
    "            return corr_matrix\n",
    "        except Exception as e:\n",
    "            logger.error(f\"评估特征相关性时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def optimize_feature_combination(self, features, target):\n",
    "        \"\"\"优化特征组合\"\"\"\n",
    "        try:\n",
    "            # 1. 计算相关性\n",
    "            correlations = self.evaluate_feature_correlation(features)\n",
    "            \n",
    "            # 2. 计算稳定性\n",
    "            stability = self._analyze_feature_stability(features)\n",
    "            \n",
    "            # 3. 计算目标相关性\n",
    "            target_corr = np.array([\n",
    "                abs(np.corrcoef(features[:, i], target)[0, 1])\n",
    "                for i in range(features.shape[1])\n",
    "            ])\n",
    "            \n",
    "            # 4. 优化组合\n",
    "            scores = target_corr * stability\n",
    "            return scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"优化特征组合时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _analyze_feature_stability(self, features):\n",
    "        \"\"\"分析特征稳定性\"\"\"\n",
    "        try:\n",
    "            stability_scores = []\n",
    "            for i in range(features.shape[1]):\n",
    "                # 计算特征的变异系数\n",
    "                cv = np.std(features[:, i]) / np.mean(np.abs(features[:, i]))\n",
    "                # 转换为稳定性分数\n",
    "                stability = 1 / (1 + cv)\n",
    "                stability_scores.append(stability)\n",
    "            return np.array(stability_scores)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"分析特征稳定性时出错: {str(e)}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45cf539d-81e3-4929-8f51-d3797e625314",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ruanjiananzhuang\\anaconda\\envs\\ace\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#8 Parameter Tuning System / 参数调优系统\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import Events\n",
    "import optuna\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from typing import Dict, Any, Optional\n",
    "from collections import deque\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# 获取logger实例\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class OptimizerManager:\n",
    "    \"\"\"优化器管理类 - 整合模型、动态和集成优化\"\"\"\n",
    "    \n",
    "    def __init__(self, model_ensemble, data_processor, performance_monitor):\n",
    "        \"\"\"\n",
    "        初始化优化器管理器\n",
    "        Args:\n",
    "            model_ensemble: 模型集成实例\n",
    "            data_processor: 数据处理器实例\n",
    "            performance_monitor: 性能监控器实例\n",
    "        \"\"\"\n",
    "        self.model_ensemble = model_ensemble\n",
    "        self.data_processor = data_processor\n",
    "        self.performance_monitor = performance_monitor\n",
    "        \n",
    "        # 优化历史记录\n",
    "        self.optimization_history = []\n",
    "        \n",
    "        # 初始化参数范围\n",
    "        self._init_param_ranges()\n",
    "        \n",
    "        # 初始化调整阈值\n",
    "        self._init_thresholds()\n",
    "        \n",
    "        logger.info(\"优化器管理器初始化完成\")\n",
    "\n",
    "    def _init_param_ranges(self):\n",
    "        \"\"\"初始化所有参数范围\"\"\"\n",
    "        self.param_ranges = {\n",
    "            # 模型架构参数\n",
    "            'model_params': {\n",
    "                'lstm_units': (64, 256),\n",
    "                'lstm_layers': (1, 4),\n",
    "                'cnn_filters': (32, 256),\n",
    "                'transformer_heads': (4, 16),\n",
    "                'dense_units': (32, 128)\n",
    "            },\n",
    "            \n",
    "            # 动态调整参数\n",
    "            'dynamic_params': {\n",
    "                'learning_rate': (0.0001, 0.01),\n",
    "                'batch_size': (16, 128),\n",
    "                'dropout_rate': (0.1, 0.5)\n",
    "            },\n",
    "            \n",
    "            # 集成参数\n",
    "            'ensemble_params': {\n",
    "                'initial_weights': (0.1, 0.3),\n",
    "                'diversity_weight': (0.1, 0.5),\n",
    "                'adaptation_rate': (0.1, 0.5)\n",
    "            },\n",
    "            \n",
    "            # 添加训练优化参数范围\n",
    "            'training_params': {\n",
    "                'optimizer_params': {\n",
    "                    'learning_rate': (1e-5, 1e-2),\n",
    "                    'beta_1': (0.8, 0.999),\n",
    "                    'beta_2': (0.8, 0.999),\n",
    "                    'epsilon': (1e-8, 1e-6)\n",
    "                },\n",
    "                'lr_schedule_params': {\n",
    "                    'decay_rate': (0.9, 0.99),\n",
    "                    'decay_steps': (100, 1000),\n",
    "                    'warmup_steps': (0, 100),\n",
    "                    'min_lr': (1e-6, 1e-4)\n",
    "                },\n",
    "                'training_control': {\n",
    "                    'batch_size': (16, 128),\n",
    "                    'epochs_per_iteration': (1, 10),\n",
    "                    'validation_frequency': (1, 10),\n",
    "                    'early_stopping_patience': (10, 50)\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 添加离散参数选项\n",
    "        self.discrete_params = {\n",
    "            'optimizer_type': ['adam', 'adamw', 'radam'],\n",
    "            'scheduler_type': ['exponential', 'cosine', 'step']\n",
    "        }\n",
    "\n",
    "    def _init_thresholds(self):\n",
    "        \"\"\"初始化调整阈值\"\"\"\n",
    "        self.thresholds = {\n",
    "            'performance_drop': 0.1,    # 性能下降阈值\n",
    "            'loss_spike': 0.5,         # 损失突增阈值\n",
    "            'diversity_min': 0.3,      # 最小多样性要求\n",
    "            'weight_change': 0.2       # 权重调整阈值\n",
    "        }\n",
    "\n",
    "    def optimize_all(self, n_iter=50):\n",
    "        \"\"\"执行全面优化\"\"\"\n",
    "        try:\n",
    "            # 1. 模型架构优化\n",
    "            model_params = self._optimize_model_architecture(n_iter)\n",
    "            \n",
    "            # 2. 动态参数优化\n",
    "            dynamic_params = self._optimize_dynamic_params(n_iter)\n",
    "            \n",
    "            # 3. 集成策略优化\n",
    "            ensemble_params = self._optimize_ensemble_strategy(n_iter)\n",
    "            \n",
    "            # 整合优化结果\n",
    "            optimized_params = {\n",
    "                'model_params': model_params,\n",
    "                'dynamic_params': dynamic_params,\n",
    "                'ensemble_params': ensemble_params\n",
    "            }\n",
    "            \n",
    "            # 保存优化结果\n",
    "            self._save_optimization_results(optimized_params)\n",
    "            \n",
    "            return optimized_params\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"执行全面优化时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _optimize_model_architecture(self, n_iter):\n",
    "        \"\"\"模型架构优化\"\"\"\n",
    "        try:\n",
    "            optimizer = BayesianOptimization(\n",
    "                f=self._model_objective,\n",
    "                pbounds=self.param_ranges['model_params'],\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            optimizer.maximize(init_points=5, n_iter=n_iter)\n",
    "            return optimizer.max['params']\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"模型架构优化失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _optimize_dynamic_params(self, n_iter):\n",
    "        \"\"\"动态参数优化\"\"\"\n",
    "        try:\n",
    "            optimizer = BayesianOptimization(\n",
    "                f=self._dynamic_objective,\n",
    "                pbounds=self.param_ranges['dynamic_params'],\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            optimizer.maximize(init_points=5, n_iter=n_iter)\n",
    "            return optimizer.max['params']\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"动态参数优化失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _optimize_ensemble_strategy(self, n_iter):\n",
    "        \"\"\"集成策略优化\"\"\"\n",
    "        try:\n",
    "            optimizer = BayesianOptimization(\n",
    "                f=self._ensemble_objective,\n",
    "                pbounds=self.param_ranges['ensemble_params'],\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            optimizer.maximize(init_points=5, n_iter=n_iter)\n",
    "            return optimizer.max['params']\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"集成策略优化失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _model_objective(self, **params):\n",
    "        \"\"\"模型优化目标函数\"\"\"\n",
    "        try:\n",
    "            self.model_ensemble.update_architecture(params)\n",
    "            return self._evaluate_performance()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"模型目标函数评估失败: {str(e)}\")\n",
    "            return float('-inf')\n",
    "\n",
    "    def _dynamic_objective(self, **params):\n",
    "        \"\"\"动态优化目标函数\"\"\"\n",
    "        try:\n",
    "            self.model_ensemble.update_dynamic_params(params)\n",
    "            return self._evaluate_performance()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"动态目标函数评估失败: {str(e)}\")\n",
    "            return float('-inf')\n",
    "\n",
    "    def _ensemble_objective(self, **params):\n",
    "        \"\"\"集成优化目标函数\"\"\"\n",
    "        try:\n",
    "            self.model_ensemble.update_ensemble_params(params)\n",
    "            performance = self._evaluate_performance()\n",
    "            diversity = self._calculate_diversity()\n",
    "            return 0.7 * performance + 0.3 * diversity\n",
    "        except Exception as e:\n",
    "            logger.error(f\"集成目标函数评估失败: {str(e)}\")\n",
    "            return float('-inf')\n",
    "\n",
    "    def _evaluate_performance(self):\n",
    "        \"\"\"评估性能\"\"\"\n",
    "        try:\n",
    "            X_val, y_val = self.data_processor.get_validation_data()\n",
    "            predictions = self.model_ensemble.predict(X_val)\n",
    "            matches = np.any(np.round(predictions) == y_val, axis=1)\n",
    "            return np.mean(matches)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"性能评估失败: {str(e)}\")\n",
    "            return 0.0\n",
    "\n",
    "    def _calculate_diversity(self):\n",
    "        \"\"\"计算模型多样性\"\"\"\n",
    "        try:\n",
    "            predictions = []\n",
    "            X_val, _ = self.data_processor.get_validation_data()\n",
    "            \n",
    "            for model in self.model_ensemble.models:\n",
    "                pred = model.predict(X_val)\n",
    "                predictions.append(pred)\n",
    "            \n",
    "            diversity_scores = []\n",
    "            n_models = len(predictions)\n",
    "            \n",
    "            for i in range(n_models):\n",
    "                for j in range(i + 1, n_models):\n",
    "                    mi_score = mutual_info_score(\n",
    "                        predictions[i].ravel(),\n",
    "                        predictions[j].ravel()\n",
    "                    )\n",
    "                    diversity_scores.append(1 - mi_score)\n",
    "            \n",
    "            return np.mean(diversity_scores)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"多样性计算失败: {str(e)}\")\n",
    "            return 0.0\n",
    "\n",
    "    def _save_optimization_results(self, results):\n",
    "        \"\"\"保存优化结果\"\"\"\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"optimization_results_{timestamp}.json\"\n",
    "            \n",
    "            save_path = os.path.join(os.getcwd(), 'optimization_results', filename)\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            \n",
    "            with open(save_path, 'w') as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "                \n",
    "            logger.info(f\"优化结果已保存到: {save_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"保存优化结果失败: {str(e)}\")\n",
    "\n",
    "    def dynamic_adjust(self, metrics):\n",
    "        \"\"\"动态参数调整\"\"\"\n",
    "        try:\n",
    "            if self._needs_adjustment(metrics):\n",
    "                suggestions = self._get_adjustment_suggestions(metrics)\n",
    "                return self._apply_adjustments(suggestions)\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"动态调整失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _needs_adjustment(self, metrics):\n",
    "        \"\"\"检查是否需要调整\"\"\"\n",
    "        return any([\n",
    "            metrics['performance_change'] < -self.thresholds['performance_drop'],\n",
    "            metrics['loss_change'] > self.thresholds['loss_spike'],\n",
    "            metrics['diversity'] < self.thresholds['diversity_min']\n",
    "        ])\n",
    "\n",
    "    def _get_adjustment_suggestions(self, metrics):\n",
    "        \"\"\"获取参数调整建议\"\"\"\n",
    "        try:\n",
    "            suggestions = {}\n",
    "            \n",
    "            # 基于性能变化的学习率调整\n",
    "            if metrics['performance_change'] < 0:\n",
    "                current_lr = metrics['current_lr']\n",
    "                suggestions['learning_rate'] = self._adjust_learning_rate(\n",
    "                    current_lr, \n",
    "                    metrics['performance_change']\n",
    "                )\n",
    "            \n",
    "            # 基于内存使用的批次大小调整    \n",
    "            if metrics['memory_usage'] > 0.9:\n",
    "                current_batch = metrics['current_batch_size']\n",
    "                suggestions['batch_size'] = self._adjust_batch_size(current_batch)\n",
    "            \n",
    "            # 基于过拟合风险的正则化调整\n",
    "            if metrics['validation_loss'] > metrics['training_loss'] * 1.2:\n",
    "                suggestions['dropout_rate'] = self._adjust_dropout_rate(\n",
    "                    metrics['current_dropout']\n",
    "                )\n",
    "                \n",
    "            return suggestions\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"生成调整建议失败: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "    def _adjust_learning_rate(self, current_lr, performance_change):\n",
    "        \"\"\"调整学习率\"\"\"\n",
    "        try:\n",
    "            if performance_change < -0.2:  # 性能显著下降\n",
    "                return current_lr * 0.5\n",
    "            elif performance_change < -0.1:  # 性能轻微下降\n",
    "                return current_lr * 0.8\n",
    "            return current_lr\n",
    "        except Exception as e:\n",
    "            logger.error(f\"调整学习率失败: {str(e)}\")\n",
    "            return current_lr\n",
    "\n",
    "    def _adjust_batch_size(self, current_batch_size):\n",
    "        \"\"\"调整批次大小\"\"\"\n",
    "        try:\n",
    "            return max(16, current_batch_size // 2)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"调整批次大小失败: {str(e)}\")\n",
    "            return current_batch_size\n",
    "\n",
    "    def _adjust_dropout_rate(self, current_dropout):\n",
    "        \"\"\"调整dropout率\"\"\"\n",
    "        try:\n",
    "            return min(0.5, current_dropout + 0.1)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"调整dropout率失败: {str(e)}\")\n",
    "            return current_dropout\n",
    "\n",
    "    def _apply_adjustments(self, suggestions):\n",
    "        \"\"\"应用参数调整\"\"\"\n",
    "        try:\n",
    "            new_params = {}\n",
    "            \n",
    "            for param_name, new_value in suggestions.items():\n",
    "                # 验证参数范围\n",
    "                if param_name in self.param_ranges['dynamic_params']:\n",
    "                    min_val, max_val = self.param_ranges['dynamic_params'][param_name]\n",
    "                    new_value = np.clip(new_value, min_val, max_val)\n",
    "                    new_params[param_name] = new_value\n",
    "                    \n",
    "            # 更新模型参数\n",
    "            if new_params:\n",
    "                self.model_ensemble.update_dynamic_params(new_params)\n",
    "                logger.info(f\"应用参数调整: {new_params}\")\n",
    "                \n",
    "            return new_params\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"应用参数调整失败: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "    def adjust_ensemble_weights(self, performance_metrics):\n",
    "        \"\"\"调整集成权重\"\"\"\n",
    "        try:\n",
    "            weights = []\n",
    "            for model_idx, metrics in enumerate(performance_metrics):\n",
    "                # 基于性能计算新权重\n",
    "                performance_score = 1.0 - metrics['loss']\n",
    "                diversity_score = self._calculate_model_diversity(model_idx)\n",
    "                weight = 0.7 * performance_score + 0.3 * diversity_score\n",
    "                weights.append(weight)\n",
    "            \n",
    "            # 归一化权重\n",
    "            weights = np.array(weights)\n",
    "            weights = weights / np.sum(weights)\n",
    "            \n",
    "            # 更新模型集成权重\n",
    "            self.model_ensemble.update_weights(weights)\n",
    "            logger.info(f\"更新集成权重: {weights}\")\n",
    "            \n",
    "            return weights\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"调整集成权重失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _calculate_model_diversity(self, model_idx):\n",
    "        \"\"\"计算单个模型的多样性得分\"\"\"\n",
    "        try:\n",
    "            predictions = []\n",
    "            X_val, _ = self.data_processor.get_validation_data()\n",
    "            \n",
    "            # 获取当前模型和其他模型的预测\n",
    "            current_pred = self.model_ensemble.models[model_idx].predict(X_val)\n",
    "            other_preds = []\n",
    "            for i, model in enumerate(self.model_ensemble.models):\n",
    "                if i != model_idx:\n",
    "                    other_preds.append(model.predict(X_val))\n",
    "            \n",
    "            # 计算与其他模型的平均互信息分数\n",
    "            diversity_scores = []\n",
    "            for other_pred in other_preds:\n",
    "                mi_score = mutual_info_score(\n",
    "                    current_pred.ravel(),\n",
    "                    other_pred.ravel()\n",
    "                )\n",
    "                diversity_scores.append(1 - mi_score)\n",
    "            \n",
    "            return np.mean(diversity_scores)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"计算模型多样性失败: {str(e)}\")\n",
    "            return 0.0\n",
    "\n",
    "    def get_optimization_summary(self):\n",
    "        \"\"\"获取优化过程摘要\"\"\"\n",
    "        try:\n",
    "            if not self.optimization_history:\n",
    "                return None\n",
    "                \n",
    "            latest_results = self.optimization_history[-1]\n",
    "            best_results = max(\n",
    "                self.optimization_history,\n",
    "                key=lambda x: x.get('final_score', 0)\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'latest': {\n",
    "                    'params': latest_results['params'],\n",
    "                    'performance': latest_results.get('final_score', 0)\n",
    "                },\n",
    "                'best': {\n",
    "                    'params': best_results['params'],\n",
    "                    'performance': best_results.get('final_score', 0)\n",
    "                },\n",
    "                'total_iterations': len(self.optimization_history)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取优化摘要失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def reset_optimization(self):\n",
    "        \"\"\"重置优化状态\"\"\"\n",
    "        try:\n",
    "            self.optimization_history.clear()\n",
    "            self._init_param_ranges()\n",
    "            self._init_thresholds()\n",
    "            logger.info(\"优化状态已重置\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"重置优化状态失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def optimize_model_params(self, training_direction):\n",
    "        \"\"\"根据训练方向优化模型参数 (from cell13)\"\"\"\n",
    "        try:\n",
    "            if isinstance(training_direction, dict):\n",
    "                # 1. 学习率调整\n",
    "                if training_direction['learning_rate'] == 'INCREASE':\n",
    "                    self.current_lr *= 1.5\n",
    "                elif training_direction['learning_rate'] == 'DECREASE':\n",
    "                    self.current_lr *= 0.7\n",
    "                \n",
    "                # 2. 批次大小调整\n",
    "                if training_direction['batch_size'] == 'DECREASE':\n",
    "                    self.batch_size = max(16, self.batch_size // 2)\n",
    "                \n",
    "                # 3. 模型复杂度调整\n",
    "                if training_direction['model_complexity'] == 'INCREASE':\n",
    "                    self._increase_model_complexity()\n",
    "                \n",
    "                # 4. 正则化调整\n",
    "                if training_direction.get('regularization') == 'INCREASE':\n",
    "                    self._increase_regularization()\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"优化模型参数时出错: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _increase_model_complexity(self):\n",
    "        \"\"\"增加模型复杂度 (from cell13)\"\"\"\n",
    "        try:\n",
    "            current_params = self.model_ensemble.get_current_params()\n",
    "            new_params = {\n",
    "                'lstm_units': int(current_params['lstm_units'] * 1.5),\n",
    "                'transformer_heads': current_params['transformer_heads'] + 2,\n",
    "                'cnn_filters': int(current_params['cnn_filters'] * 1.3)\n",
    "            }\n",
    "            self.model_ensemble.update_architecture(new_params)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"增加模型复杂度失败: {str(e)}\")\n",
    "\n",
    "    def _increase_regularization(self):\n",
    "        \"\"\"增加正则化强度 (from cell13)\"\"\"\n",
    "        try:\n",
    "            current_params = self.model_ensemble.get_current_params()\n",
    "            new_params = {\n",
    "                'dropout_rate': min(0.5, current_params['dropout_rate'] + 0.1),\n",
    "                'weight_decay': current_params['weight_decay'] * 2\n",
    "            }\n",
    "            self.model_ensemble.update_dynamic_params(new_params)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"增加正则化强度失败: {str(e)}\")\n",
    "\n",
    "    def adjust_ensemble_strategy(self, match_distribution):\n",
    "        \"\"\"调整集成策略 (from cell15)\"\"\"\n",
    "        try:\n",
    "            total_samples = sum(match_distribution.values())\n",
    "            \n",
    "            # 1. 分析集成效果\n",
    "            high_match_ratio = (match_distribution[4] + match_distribution[5]) / total_samples\n",
    "            low_match_ratio = (match_distribution[0] + match_distribution[1]) / total_samples\n",
    "            \n",
    "            # 2. 根据分布调整集成策略\n",
    "            if high_match_ratio < 0.1:  # 高匹配率太低\n",
    "                # 增加模型多样性\n",
    "                self._increase_model_diversity()\n",
    "                # 调整模型权重\n",
    "                self._adjust_model_weights()\n",
    "                \n",
    "            elif low_match_ratio > 0.5:  # 低匹配率太高\n",
    "                # 强化表现好的模型\n",
    "                self._strengthen_best_models()\n",
    "                # 重新训练表现差的模型\n",
    "                self._retrain_weak_models()\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"调整集成策略时出错: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _increase_model_diversity(self):\n",
    "        \"\"\"增加模型多样性 (from cell15)\"\"\"\n",
    "        try:\n",
    "            # 1. 计算当前多样性矩阵\n",
    "            diversity_matrix = self._calculate_diversity_matrix()\n",
    "            \n",
    "            # 2. 找出相似度最高的模型对\n",
    "            similar_pairs = self._find_similar_model_pairs(diversity_matrix)\n",
    "            \n",
    "            # 3. 对相似模型进行差异化训练\n",
    "            for model_i, model_j in similar_pairs:\n",
    "                self._differentiate_models(model_i, model_j)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"增加模型多样性失败: {str(e)}\")\n",
    "\n",
    "    def _calculate_diversity_matrix(self):\n",
    "        \"\"\"计算模型间多样性矩阵 (from cell15)\"\"\"\n",
    "        try:\n",
    "            n_models = len(self.model_ensemble.models)\n",
    "            diversity_matrix = np.zeros((n_models, n_models))\n",
    "            \n",
    "            X_val, _ = self.data_processor.get_validation_data()\n",
    "            predictions = [model.predict(X_val) for model in self.model_ensemble.models]\n",
    "            \n",
    "            for i in range(n_models):\n",
    "                for j in range(i+1, n_models):\n",
    "                    mi_score = mutual_info_score(\n",
    "                        predictions[i].ravel(),\n",
    "                        predictions[j].ravel()\n",
    "                    )\n",
    "                    diversity_matrix[i, j] = mi_score\n",
    "                    diversity_matrix[j, i] = mi_score\n",
    "                    \n",
    "            return diversity_matrix\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"计算多样性矩阵失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _find_similar_model_pairs(self, diversity_matrix):\n",
    "        \"\"\"找出相似度高的模型对 (from cell15)\"\"\"\n",
    "        try:\n",
    "            n_models = len(self.model_ensemble.models)\n",
    "            similar_pairs = []\n",
    "            \n",
    "            for i in range(n_models):\n",
    "                for j in range(i+1, n_models):\n",
    "                    if diversity_matrix[i, j] > 0.8:  # 相似度阈值\n",
    "                        similar_pairs.append((i, j))\n",
    "                        \n",
    "            return similar_pairs\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"寻找相似模型对失败: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _differentiate_models(self, model_i, model_j):\n",
    "        \"\"\"对相似模型进行差异化训练 (from cell15)\"\"\"\n",
    "        try:\n",
    "            # 1. 调整模型架构\n",
    "            self.model_ensemble.update_model_architecture({\n",
    "                model_i: {'dropout_rate': 0.3},\n",
    "                model_j: {'dropout_rate': 0.5}\n",
    "            })\n",
    "            \n",
    "            # 2. 使用不同的优化器\n",
    "            self.model_ensemble.update_optimizer_settings({\n",
    "                model_i: {'learning_rate': 0.001},\n",
    "                model_j: {'learning_rate': 0.0005}\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"模型差异化失败: {str(e)}\")\n",
    "\n",
    "    def _strengthen_best_models(self):\n",
    "        \"\"\"强化表现好的模型 (from cell15)\"\"\"\n",
    "        try:\n",
    "            performance_metrics = self.performance_monitor.get_model_metrics()\n",
    "            best_models = self._identify_best_models(performance_metrics)\n",
    "            \n",
    "            for model_idx in best_models:\n",
    "                # 增加模型权重\n",
    "                self.model_ensemble.increase_model_weight(model_idx)\n",
    "                # 微调学习率\n",
    "                self.model_ensemble.fine_tune_model(model_idx)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"强化最佳模型失败: {str(e)}\")\n",
    "\n",
    "    def _retrain_weak_models(self):\n",
    "        \"\"\"重新训练表现差的模型 (from cell15)\"\"\"\n",
    "        try:\n",
    "            performance_metrics = self.performance_monitor.get_model_metrics()\n",
    "            weak_models = self._identify_weak_models(performance_metrics)\n",
    "            \n",
    "            for model_idx in weak_models:\n",
    "                # 重置模型参数\n",
    "                self.model_ensemble.reset_model(model_idx)\n",
    "                # 使用新的训练策略\n",
    "                self.model_ensemble.retrain_model(\n",
    "                    model_idx, \n",
    "                    strategy='adaptive'\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"重训弱模型失败: {str(e)}\")\n",
    "\n",
    "    def _identify_best_models(self, metrics):\n",
    "        \"\"\"识别最佳模型 (from cell15)\"\"\"\n",
    "        try:\n",
    "            scores = [m['performance'] for m in metrics]\n",
    "            threshold = np.percentile(scores, 75)  # 上四分位数\n",
    "            return [i for i, score in enumerate(scores) if score >= threshold]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"识别最佳模型失败: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _identify_weak_models(self, metrics):\n",
    "        \"\"\"识别弱模型 (from cell15)\"\"\"\n",
    "        try:\n",
    "            scores = [m['performance'] for m in metrics]\n",
    "            threshold = np.percentile(scores, 25)  # 下四分位数\n",
    "            return [i for i, score in enumerate(scores) if score <= threshold]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"识别弱模型失败: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def adjust_after_sample(self, model, sample, current_params):\n",
    "        \"\"\"基于样本梯度调整参数 (from cell13)\"\"\"\n",
    "        try:\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(sample['input'])\n",
    "                loss = tf.keras.losses.MSE(sample['target'], predictions)\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            \n",
    "            # 生成参数调整建议\n",
    "            adjusted_params = {\n",
    "                'learning_rate': self._adjust_lr_from_grads(grads, current_params),\n",
    "                'batch_size': current_params['batch_size']\n",
    "            }\n",
    "            return adjusted_params\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"样本级参数调整失败: {str(e)}\")\n",
    "            return current_params\n",
    "\n",
    "    def _adjust_lr_from_grads(self, grads, current_params):\n",
    "        \"\"\"根据梯度调整学习率\"\"\"\n",
    "        try:\n",
    "            grad_norm = tf.linalg.global_norm(grads)\n",
    "            if grad_norm > 10.0:  # 梯度爆炸\n",
    "                return current_params['learning_rate'] * 0.5\n",
    "            elif grad_norm < 0.1:  # 梯度消失\n",
    "                return current_params['learning_rate'] * 1.5\n",
    "            return current_params['learning_rate']\n",
    "        except Exception as e:\n",
    "            logger.error(f\"学习率梯度调整失败: {str(e)}\")\n",
    "            return current_params['learning_rate']\n",
    "\n",
    "    def on_train_end(self):\n",
    "        \"\"\"训练结束时的优化操作 (from cell13)\"\"\"\n",
    "        try:\n",
    "            # 获取新的参数建议\n",
    "            new_params = self.suggest_next_params()\n",
    "            # 更新集成模型参数\n",
    "            self.model_ensemble.update_params(new_params)\n",
    "            # 保存优化记录\n",
    "            self._save_optimization_record()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"训练结束优化操作失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def suggest_next_params(self):\n",
    "        \"\"\"使用Optuna生成下一组参数\"\"\"\n",
    "        try:\n",
    "            study = optuna.create_study(\n",
    "                study_name=\"model_optim_v1\",\n",
    "                storage=\"sqlite:///optuna.db\",\n",
    "                load_if_exists=True\n",
    "            )\n",
    "            \n",
    "            trial = study.ask()\n",
    "            params = {\n",
    "                'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-2),\n",
    "                'batch_size': trial.suggest_int('batch_size', 16, 128),\n",
    "                'dropout_rate': trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "            }\n",
    "            return params\n",
    "        except Exception as e:\n",
    "            logger.error(f\"生成参数建议失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def optimize_parameters(self):\n",
    "        \"\"\"执行参数优化 (from cell14)\"\"\"\n",
    "        try:\n",
    "            # 贝叶斯优化初始化基础参数\n",
    "            initial_params = self.bayesian_optimization()\n",
    "            # 使用Optuna进行细粒度优化\n",
    "            final_params = self._optuna_optimization(initial_params)\n",
    "            \n",
    "            # 更新并保存最佳参数\n",
    "            self.best_params = final_params\n",
    "            self.save_best_params()\n",
    "            \n",
    "            logger.info(f\"参数优化完成: {final_params}\")\n",
    "            return final_params\n",
    "        except Exception as e:\n",
    "            logger.error(f\"参数优化失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def save_best_params(self):\n",
    "        \"\"\"保存最佳参数配置 (from cell14)\"\"\"\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"best_params_{timestamp}.json\"\n",
    "            save_path = os.path.join(os.getcwd(), 'optimization_params', filename)\n",
    "            \n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            with open(save_path, 'w') as f:\n",
    "                json.dump(self.best_params, f, indent=4)\n",
    "            \n",
    "            logger.info(f\"最佳参数已保存到: {save_path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"保存最佳参数失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _save_optimization_record(self):\n",
    "        \"\"\"保存优化记录\"\"\"\n",
    "        try:\n",
    "            record = {\n",
    "                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'history': self.optimization_history,\n",
    "                'best_params': self.best_params,\n",
    "                'performance_summary': self.get_optimization_summary()\n",
    "            }\n",
    "            \n",
    "            filename = f\"optimization_record_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "            save_path = os.path.join(os.getcwd(), 'optimization_records', filename)\n",
    "            \n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            with open(save_path, 'w') as f:\n",
    "                json.dump(record, f, indent=4)\n",
    "                \n",
    "            logger.info(f\"优化记录已保存到: {save_path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"保存优化记录失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def analyze_training_direction(self, match_counts, current_params):\n",
    "        \"\"\"分析训练方向\"\"\"\n",
    "        try:\n",
    "            # 1. 初始化/更新匹配分布\n",
    "            if not hasattr(self, 'match_distribution'):\n",
    "                self.match_distribution = {i: 0 for i in range(6)}\n",
    "                \n",
    "            # 2. 更新分布\n",
    "            for count in match_counts:\n",
    "                self.match_distribution[count] += 1\n",
    "            \n",
    "            # 3. 判断当前状态\n",
    "            if self.match_distribution[5] > 0:\n",
    "                return \"OPTIMAL\"\n",
    "                \n",
    "            avg_match = sum(k * v for k, v in self.match_distribution.items()) / sum(self.match_distribution.values())\n",
    "            \n",
    "            # 4. 根据匹配分布给出调整建议\n",
    "            if avg_match < 2:\n",
    "                return {\n",
    "                    'learning_rate': 'INCREASE',\n",
    "                    'batch_size': 'DECREASE',\n",
    "                    'model_complexity': 'INCREASE'\n",
    "                }\n",
    "            elif avg_match > 3:\n",
    "                return {\n",
    "                    'learning_rate': 'DECREASE',\n",
    "                    'regularization': 'INCREASE',\n",
    "                    'ensemble_diversity': 'INCREASE'\n",
    "                }\n",
    "            \n",
    "            return \"CONTINUE\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"分析训练方向时出错: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def optimize_training_flow(self):\n",
    "        \"\"\"优化训练流程\"\"\"\n",
    "        try:\n",
    "            self._dynamic_resource_adjust()\n",
    "            self._dynamic_batch_adjust()\n",
    "            self._enable_mixed_precision()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"优化训练流程时出错: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _dynamic_resource_adjust(self):\n",
    "        \"\"\"根据硬件资源动态调整参数\"\"\"\n",
    "        try:\n",
    "            # 获取资源信息\n",
    "            mem_info = memory_manager.get_memory_info()\n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            \n",
    "            # 内存调整策略\n",
    "            if mem_info['percent'] > 75:\n",
    "                new_batch = max(4, self.batch_size // 2)\n",
    "                logger.info(f\"内存使用{mem_info['percent']}% → 批次从{self.batch_size}调整为{new_batch}\")\n",
    "                self.batch_size = new_batch\n",
    "            \n",
    "            # CPU线程调整策略\n",
    "            if hasattr(self, 'threads'):\n",
    "                if cpu_usage < 60:\n",
    "                    self.threads = min(12, self.threads + 2)\n",
    "                else:\n",
    "                    self.threads = max(4, self.threads - 2)\n",
    "            \n",
    "            # GPU显存优化\n",
    "            if tf.config.list_physical_devices('GPU'):\n",
    "                gpu_mem = tf.config.experimental.get_memory_info('GPU:0')\n",
    "                used_percent = gpu_mem['current'] / gpu_mem['total']\n",
    "                if used_percent > 0.8:\n",
    "                    tf.config.experimental.set_memory_growth(True)\n",
    "                    \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"资源调整失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _dynamic_batch_adjust(self):\n",
    "        \"\"\"动态调整批次大小\"\"\"\n",
    "        try:\n",
    "            if hasattr(self, 'batch_size'):\n",
    "                mem_usage = memory_manager.get_memory_info()\n",
    "                if mem_usage['percent'] > 80:\n",
    "                    new_size = max(8, self.batch_size // 2)\n",
    "                    logger.info(f\"批次大小从{self.batch_size}调整为{new_size}\")\n",
    "                    self.batch_size = new_size\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"批次调整失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _enable_mixed_precision(self):\n",
    "        \"\"\"启用混合精度训练\"\"\"\n",
    "        try:\n",
    "            if tf.config.list_physical_devices('GPU'):\n",
    "                policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "                tf.keras.mixed_precision.set_global_policy(policy)\n",
    "                logger.info(\"已启用混合精度训练\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"启用混合精度失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def setup_mixed_precision(self):\n",
    "        \"\"\"配置混合精度训练\"\"\"\n",
    "        try:\n",
    "            if tf.config.list_physical_devices('GPU'):\n",
    "                # 启用mixed precision policy\n",
    "                policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "                tf.keras.mixed_precision.set_global_policy(policy)\n",
    "                logger.info(\"已启用混合精度训练\")\n",
    "                \n",
    "                # 配置优化器\n",
    "                self.model_ensemble.update_optimizer_settings({\n",
    "                    'mixed_precision': True,\n",
    "                    'loss_scale': 'dynamic'\n",
    "                })\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"配置混合精度训练失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def setup_checkpoints(self):\n",
    "        \"\"\"配置检查点\"\"\"\n",
    "        try:\n",
    "            checkpoint_config = {\n",
    "                'save_freq': 100,  # 每100步保存一次\n",
    "                'max_to_keep': 5,  # 保留最新的5个检查点\n",
    "                'include_optimizer': True,\n",
    "                'save_best_only': True,\n",
    "                'monitor': 'val_accuracy'\n",
    "            }\n",
    "            self.model_ensemble.setup_model_checkpoint(checkpoint_config)\n",
    "            logger.info(\"检查点配置完成\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"配置检查点失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def adjust_learning_rate(self, metrics):\n",
    "        \"\"\"智能调整学习率\"\"\"\n",
    "        try:\n",
    "            accuracy = metrics.get('accuracy', 0)\n",
    "            loss_change = metrics.get('loss_change', 0)\n",
    "            \n",
    "            # 基于性能调整学习率\n",
    "            if accuracy < 0.5 and loss_change > 0:\n",
    "                # 性能差且损失在增加，大幅降低学习率\n",
    "                return self.current_lr * 0.5\n",
    "            elif accuracy < 0.7 and loss_change > 0:\n",
    "                # 性能一般且损失增加，小幅降低学习率\n",
    "                return self.current_lr * 0.8\n",
    "            elif accuracy > 0.9 and loss_change < 0:\n",
    "                # 性能好且损失在下降，小幅提高学习率\n",
    "                return self.current_lr * 1.1\n",
    "            \n",
    "            return self.current_lr\n",
    "        except Exception as e:\n",
    "            logger.error(f\"调整学习率失败: {str(e)}\")\n",
    "            return self.current_lr\n",
    "\n",
    "    def _update_training_params(self, params):\n",
    "        \"\"\"更新训练参数\"\"\"\n",
    "        try:\n",
    "            nested_params = self._process_params(params)\n",
    "            \n",
    "            # 1. 更新优化器参数\n",
    "            optimizer_params = nested_params['optimizer_params']\n",
    "            for model in self.model_ensemble.models:\n",
    "                model.optimizer.learning_rate = optimizer_params['learning_rate']\n",
    "                if hasattr(model.optimizer, 'beta_1'):\n",
    "                    model.optimizer.beta_1 = optimizer_params['beta_1']\n",
    "                if hasattr(model.optimizer, 'beta_2'):\n",
    "                    model.optimizer.beta_2 = optimizer_params['beta_2']\n",
    "            \n",
    "            # 2. 更新学习率调度\n",
    "            lr_params = nested_params['lr_schedule_params']\n",
    "            self.model_ensemble.update_learning_rate_schedule(\n",
    "                decay_rate=lr_params['decay_rate'],\n",
    "                decay_steps=int(lr_params['decay_steps']),\n",
    "                warmup_steps=int(lr_params['warmup_steps']),\n",
    "                min_lr=lr_params['min_lr']\n",
    "            )\n",
    "            \n",
    "            # 3. 更新训练控制参数\n",
    "            training_params = nested_params['training_control']\n",
    "            self.model_ensemble.batch_size = int(training_params['batch_size'])\n",
    "            self.model_ensemble.epochs_per_iteration = int(training_params['epochs_per_iteration'])\n",
    "            self.model_ensemble.validation_frequency = int(training_params['validation_frequency'])\n",
    "            \n",
    "            logger.info(\"训练参数已更新\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"更新训练参数时出错: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "class DynamicTuner:\n",
    "    \"\"\"动态参数调优器\"\"\"\n",
    "    def __init__(self):\n",
    "        self.history = deque(maxlen=100)\n",
    "        self.param_ranges = {\n",
    "            'learning_rate': (1e-5, 1e-3),\n",
    "            'batch_size': (16, 128),\n",
    "            'dropout': (0.1, 0.5)\n",
    "        }\n",
    "        \n",
    "    def adapt_parameters(self, model, recent_loss):\n",
    "        \"\"\"自适应调整参数\"\"\"\n",
    "        if len(self.history) < 10:\n",
    "            return\n",
    "        \n",
    "        # 计算损失变化趋势\n",
    "        loss_diff = np.diff(self.history)\n",
    "        trend = np.mean(loss_diff[-3:])\n",
    "        \n",
    "        # 动态调整学习率\n",
    "        if trend > 0:  # 损失上升\n",
    "            K.set_value(\n",
    "                model.optimizer.learning_rate, \n",
    "                max(self.param_ranges['learning_rate'][0], \n",
    "                    K.get_value(model.optimizer.learning_rate) * 0.9)\n",
    "            )\n",
    "        else:  # 损失下降\n",
    "            K.set_value(\n",
    "                model.optimizer.learning_rate,\n",
    "                min(self.param_ranges['learning_rate'][1],\n",
    "                    K.get_value(model.optimizer.learning_rate) * 1.1)\n",
    "            )\n",
    "\n",
    "# 创建全局实例\n",
    "optimizer_manager = OptimizerManager(\n",
    "    model_ensemble=None,\n",
    "    data_processor=None,\n",
    "    performance_monitor=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2519871-cfcd-4816-b6f3-f9673b1e48bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#9 State Management System / 状态管理系统\n",
    "import os\n",
    "import signal\n",
    "import logging\n",
    "import threading\n",
    "from typing import Optional, Any, Dict\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from bayes_opt.logger import Events  # 拆分导入\n",
    "from bayes_opt.util import load_logs\n",
    "import tensorflow as tf\n",
    "\n",
    "# 获取logger实例\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class StateManager:\n",
    "    \"\"\"全局状态管理器 - 单例模式\"\"\"\n",
    "    _instance = None\n",
    "    \n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self):\n",
    "        if not hasattr(self, 'initialized'):\n",
    "            # 训练相关状态\n",
    "            self.trainer = None  # 全局trainer实例\n",
    "            self.training_state = 'idle'  # 训练状态: idle/training/paused/stopped\n",
    "            self.current_epoch = 0\n",
    "            self.current_batch = 0\n",
    "            self.best_performance = float('inf')\n",
    "            \n",
    "            # 显示相关状态\n",
    "            self.display_running = True  # 显示线程运行标志\n",
    "            self.display_thread = None  # 显示线程实例\n",
    "            self.log_buffer = deque(maxlen=100)  # 日志缓冲区\n",
    "            self.show_print = False  # 控制是否显示打印信息\n",
    "            \n",
    "            # 性能监控状态\n",
    "            self.performance_metrics = {\n",
    "                'loss': deque(maxlen=1000),\n",
    "                'accuracy': deque(maxlen=1000),\n",
    "                'learning_rate': 0.001\n",
    "            }\n",
    "            \n",
    "            # 资源监控状态\n",
    "            self.resource_metrics = {\n",
    "                'memory_usage': 0,\n",
    "                'cpu_usage': 0,\n",
    "                'gpu_usage': 0\n",
    "            }\n",
    "            \n",
    "            # 注册信号处理器\n",
    "            self._register_signal_handlers()\n",
    "            \n",
    "            self.initialized = True\n",
    "            logger.info(\"状态管理器初始化完成\")\n",
    "    \n",
    "    def _register_signal_handlers(self):\n",
    "        \"\"\"注册信号处理器\"\"\"\n",
    "        signal.signal(signal.SIGINT, self._signal_handler)   # Ctrl+C\n",
    "        signal.signal(signal.SIGTERM, self._signal_handler)  # 终止信号\n",
    "    \n",
    "    def _signal_handler(self, signum, frame):\n",
    "        \"\"\"信号处理器\"\"\"\n",
    "        logger.info(f\"接收到信号: {signum}, 开始保存进度...\")\n",
    "        self.save_all_progress()\n",
    "        import sys\n",
    "        sys.exit(0)\n",
    "    \n",
    "    def save_all_progress(self):\n",
    "        \"\"\"保存所有进度和参数\"\"\"\n",
    "        if self.trainer:\n",
    "            try:\n",
    "                # 保存训练进度\n",
    "                self.trainer.save_training_progress()\n",
    "                # 保存模型参数\n",
    "                self.trainer.save_model_weights()\n",
    "                # 保存性能指标\n",
    "                self.save_performance_metrics()\n",
    "                logger.info(\"所有进度和参数已保存\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"保存进度时出错: {str(e)}\")\n",
    "    \n",
    "    def update_training_state(self, new_state: str):\n",
    "        \"\"\"更新训练状态\"\"\"\n",
    "        valid_states = {'idle', 'training', 'paused', 'stopped'}\n",
    "        if new_state not in valid_states:\n",
    "            logger.error(f\"无效的训练状态: {new_state}\")\n",
    "            return\n",
    "            \n",
    "        old_state = self.training_state\n",
    "        self.training_state = new_state\n",
    "        logger.info(f\"训练状态从 {old_state} 变更为 {new_state}\")\n",
    "    \n",
    "    def update_performance_metrics(self, metrics: Dict[str, float]):\n",
    "        \"\"\"更新性能指标\"\"\"\n",
    "        try:\n",
    "            for key, value in metrics.items():\n",
    "                if key in self.performance_metrics:\n",
    "                    self.performance_metrics[key].append(value)\n",
    "                    \n",
    "            # 更新最佳性能\n",
    "            if 'loss' in metrics and metrics['loss'] < self.best_performance:\n",
    "                self.best_performance = metrics['loss']\n",
    "                logger.info(f\"更新最佳性能: {self.best_performance:.4f}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"更新性能指标时出错: {str(e)}\")\n",
    "    \n",
    "    def update_resource_metrics(self, metrics: Dict[str, float]):\n",
    "        \"\"\"更新资源使用指标\"\"\"\n",
    "        try:\n",
    "            self.resource_metrics.update(metrics)\n",
    "            # 检查资源使用是否超过警戒线\n",
    "            if metrics.get('memory_usage', 0) > 90:\n",
    "                logger.warning(\"内存使用率超过90%!\")\n",
    "            if metrics.get('gpu_usage', 0) > 90:\n",
    "                logger.warning(\"GPU使用率超过90%!\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"更新资源指标时出错: {str(e)}\")\n",
    "    \n",
    "    def set_trainer(self, trainer: Any):\n",
    "        \"\"\"设置trainer实例\"\"\"\n",
    "        self.trainer = trainer\n",
    "    \n",
    "    def set_display_thread(self, thread: threading.Thread):\n",
    "        \"\"\"设置显示线程\"\"\"\n",
    "        self.display_thread = thread\n",
    "    \n",
    "    def stop_display(self):\n",
    "        \"\"\"停止显示线程\"\"\"\n",
    "        self.display_running = False\n",
    "        if self.display_thread and self.display_thread.is_alive():\n",
    "            self.display_thread.join()\n",
    "            logger.info(\"显示线程已停止\")\n",
    "    \n",
    "    def save_performance_metrics(self):\n",
    "        \"\"\"保存性能指标到文件\"\"\"\n",
    "        try:\n",
    "            metrics_file = os.path.join('logs', f'metrics_{datetime.now():%Y%m%d_%H%M%S}.json')\n",
    "            import json\n",
    "            with open(metrics_file, 'w') as f:\n",
    "                # 将deque转换为list后保存\n",
    "                metrics_to_save = {\n",
    "                    k: list(v) if isinstance(v, deque) else v \n",
    "                    for k, v in self.performance_metrics.items()\n",
    "                }\n",
    "                json.dump(metrics_to_save, f, indent=4)\n",
    "            logger.info(f\"性能指标已保存到: {metrics_file}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"保存性能指标时出错: {str(e)}\")\n",
    "\n",
    "    def save_training_state(self):\n",
    "        \"\"\"保存训练状态\"\"\"\n",
    "        try:\n",
    "            tf.keras.models.save_model(self.model, 'training_state.h5')\n",
    "            self.logger.info(\"训练状态已保存\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"保存训练状态失败: {str(e)}\")\n",
    "\n",
    "    def restore_training_state(self):\n",
    "        \"\"\"恢复训练状态\"\"\"\n",
    "        if os.path.exists('training_state.h5'):\n",
    "            self.model = tf.keras.models.load_model('training_state.h5')\n",
    "\n",
    "# 创建全局实例\n",
    "state_manager = StateManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de50181c-5946-419c-b693-533fbf0d8b1b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: (1, 14400, 5)\n",
      "编码后形状: (1, 14400, 5)\n",
      "编码示例(前3个时间步):\n",
      " [[0.4515651  1.7493052  0.00732224 1.769947   0.44020802]\n",
      " [1.4587454  0.9580512  0.7141876  1.2719253  0.6423419 ]\n",
      " [1.6264706  0.3802063  0.7045929  1.995584   0.6268969 ]]\n"
     ]
    }
   ],
   "source": [
    "#10 Training Management System / 训练管理系统\n",
    "import traceback\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "import threading\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from cell1_core import core_manager\n",
    "from cell3_monitor import resource_monitor, performance_monitor\n",
    "from datetime import datetime, timedelta\n",
    "from collections import deque\n",
    "from cell6_model import model_core\n",
    "import gc\n",
    "from cell4_data import data_manager\n",
    "\n",
    "# 获取logger实例\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TrainingManager:\n",
    "    \"\"\"训练管理器类\"\"\"\n",
    "    \n",
    "    def __init__(self, model_ensemble):\n",
    "        self.model_ensemble = model_ensemble\n",
    "        \n",
    "        # 获取训练配置\n",
    "        self.config = core_manager.SYSTEM_CONFIG['TRAINING_CONFIG']\n",
    "        self.batch_size = self.config['batch_size']\n",
    "        self.epochs = self.config['max_epochs']\n",
    "        \n",
    "        # 添加数据管理器\n",
    "        self.data_manager = data_manager\n",
    "        \n",
    "        # 训练状态\n",
    "        self.is_training = False\n",
    "        self.pause_training = False\n",
    "        self._training_loop_running = False\n",
    "        self._training_thread = None\n",
    "        \n",
    "        # 训练进度\n",
    "        self.current_epoch = 0\n",
    "        self.current_batch = 0\n",
    "        self.total_batches = 0\n",
    "        \n",
    "        # 训练性能监控\n",
    "        self.performance_history = deque(maxlen=1000)\n",
    "        self.resource_monitor = resource_monitor\n",
    "        self.performance_monitor = performance_monitor\n",
    "        \n",
    "        # 训练同步机制\n",
    "        self.training_lock = threading.Lock()\n",
    "        self.finished_models = 0\n",
    "        self.total_models = 6\n",
    "        \n",
    "        logger.info(\"训练管理器初始化完成\")\n",
    "        \n",
    "        self.issue_file = \"D:\\\\JupyterWork\\\\notebooks\\\\period_number\\\\issue.txt\"\n",
    "        self.current_target_period = None\n",
    "    \n",
    "    def start_training(self, training_data):\n",
    "        \"\"\"启动训练（增加数据量检查）\"\"\"\n",
    "        try:\n",
    "            # 修改后的数据量检查\n",
    "            if training_data.shape[0] != 14400 + 2880:  # 检查数组第一维长度\n",
    "                logger.error(f\"数据量不匹配，预期{14400+2880}条，实际{training_data.shape[0]}条\")\n",
    "                return False\n",
    "            \n",
    "            # 添加数据校验\n",
    "            if training_data is None:\n",
    "                logger.error(\"训练数据为空\")\n",
    "                return False\n",
    "            if len(training_data) == 0:\n",
    "                logger.error(\"训练数据长度为零\")\n",
    "                return False\n",
    "            \n",
    "            if self.is_training:\n",
    "                logger.warning(\"训练已在进行中\")\n",
    "                return False\n",
    "            \n",
    "            self.is_training = True\n",
    "            self.pause_training = False\n",
    "            \n",
    "            # 初始化训练状态\n",
    "            self._init_training(training_data)\n",
    "            \n",
    "            # 启动训练线程\n",
    "            self._training_thread = threading.Thread(\n",
    "                target=self._training_loop,\n",
    "                args=(training_data,)\n",
    "            )\n",
    "            self._training_thread.start()\n",
    "            \n",
    "            logger.info(\"训练已启动\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"启动训练失败: {str(e)}\")\n",
    "            self.is_training = False\n",
    "            return False\n",
    "    \n",
    "    def stop_training(self):\n",
    "        \"\"\"停止训练\"\"\"\n",
    "        try:\n",
    "            self.is_training = False\n",
    "            if self._training_thread and self._training_thread.is_alive():\n",
    "                self._training_thread.join()\n",
    "            logger.info(\"训练已停止\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"停止训练失败: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def pause_resume_training(self):\n",
    "        \"\"\"暂停/恢复训练\"\"\"\n",
    "        try:\n",
    "            self.pause_training = not self.pause_training\n",
    "            status = \"暂停\" if self.pause_training else \"恢复\"\n",
    "            logger.info(f\"训练已{status}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"训练暂停/恢复失败: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def _init_training(self, training_data):\n",
    "        \"\"\"初始化训练\"\"\"\n",
    "        self.current_epoch = 0\n",
    "        self.current_batch = 0\n",
    "        self.total_batches = len(training_data) // self.batch_size\n",
    "        self.performance_history.clear()\n",
    "        \n",
    "        # 初始化资源监控\n",
    "        self.resource_monitor.start()\n",
    "        \n",
    "        # 初始化性能监控\n",
    "        self.performance_monitor.reset()\n",
    "    \n",
    "    def _training_loop(self, training_data):\n",
    "        \"\"\"训练主循环\"\"\"\n",
    "        try:\n",
    "            while self.is_training and self.current_epoch < self.epochs:\n",
    "                \n",
    "                # 检查暂停状态\n",
    "                if self.pause_training:\n",
    "                    time.sleep(1)\n",
    "                    continue\n",
    "                \n",
    "                # 检查系统资源\n",
    "                if not self._check_resources():\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "                \n",
    "                # 获取训练批次\n",
    "                batch_data = self._get_next_batch(training_data)\n",
    "                if batch_data is None:\n",
    "                    continue\n",
    "                \n",
    "                # 并行训练模型\n",
    "                self._parallel_train_models(batch_data)\n",
    "                \n",
    "                # 更新训练状态\n",
    "                self._update_training_status()\n",
    "                \n",
    "                # 记录训练性能\n",
    "                self._record_performance()\n",
    "                \n",
    "            logger.info(\"训练完成\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"训练循环出错: {str(e)}\")\n",
    "            self.is_training = False\n",
    "    \n",
    "    def _check_resources(self):\n",
    "        \"\"\"检查系统资源\"\"\"\n",
    "        try:\n",
    "            # 检查CPU使用率\n",
    "            if self.resource_monitor.cpu_usage > 90:\n",
    "                logger.warning(\"CPU使用率过高,暂停训练\")\n",
    "                return False\n",
    "            \n",
    "            # 检查内存使用率    \n",
    "            if self.resource_monitor.memory_usage > 90:\n",
    "                logger.warning(\"内存使用率过高,暂停训练\")\n",
    "                return False\n",
    "            \n",
    "            # 检查GPU使用率\n",
    "            if self.resource_monitor.gpu_usage > 90:\n",
    "                logger.warning(\"GPU使用率过高,暂停训练\")\n",
    "                return False\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"资源检查失败: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def _get_next_batch(self, training_data):\n",
    "        \"\"\"获取下一个训练批次\"\"\"\n",
    "        try:\n",
    "            start_idx = self.current_batch * self.batch_size\n",
    "            end_idx = start_idx + self.batch_size\n",
    "            \n",
    "            if end_idx > len(training_data):\n",
    "                self.current_epoch += 1\n",
    "                self.current_batch = 0\n",
    "                return None\n",
    "                \n",
    "            batch = training_data[start_idx:end_idx]\n",
    "            self.current_batch += 1\n",
    "            \n",
    "            return self._preprocess_batch(batch)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取训练批次失败: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _parallel_train_models(self, batch_data):\n",
    "        \"\"\"并行训练模型\"\"\"\n",
    "        try:\n",
    "            strategy = tf.distribute.MirroredStrategy()\n",
    "            with strategy.scope():\n",
    "                with ThreadPoolExecutor(max_workers=self.total_models) as executor:\n",
    "                    futures = []\n",
    "                    for i, model in enumerate(self.model_ensemble.models):\n",
    "                        future = executor.submit(\n",
    "                            self._train_single_model,\n",
    "                            model,\n",
    "                            batch_data,\n",
    "                            i\n",
    "                        )\n",
    "                        futures.append(future)\n",
    "                    \n",
    "                    # 等待所有模型训练完成\n",
    "                    for future in futures:\n",
    "                        future.result()\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"并行训练失败: {str(e)}\")\n",
    "    \n",
    "    def _train_single_model(self, model, batch_data, model_idx):\n",
    "        \"\"\"训练单个模型\"\"\"\n",
    "        try:\n",
    "            # 1. 前向传播\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(batch_data['input'])\n",
    "                loss = self._calculate_loss(predictions, batch_data['target'])\n",
    "            \n",
    "            # 2. 反向传播\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            self.model_ensemble.optimizer.apply_gradients(\n",
    "                zip(gradients, model.trainable_variables)\n",
    "            )\n",
    "            \n",
    "            # 3. 更新模型权重\n",
    "            self._update_model_weights(model_idx, loss.numpy())\n",
    "            \n",
    "            # 4. 记录训练进度\n",
    "            with self.training_lock:\n",
    "                self.finished_models += 1\n",
    "                if self.finished_models == self.total_models:\n",
    "                    self.finished_models = 0\n",
    "                    self._on_batch_complete()\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"训练模型 {model_idx} 失败: {str(e)}\")\n",
    "    \n",
    "    def _calculate_loss(self, predictions, targets):\n",
    "        \"\"\"计算训练损失\"\"\"\n",
    "        try:\n",
    "            # 使用增强型匹配损失函数\n",
    "            return self.model_ensemble.enhanced_match_loss(targets, predictions)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"计算损失失败: {str(e)}\")\n",
    "            return tf.constant(0.0)\n",
    "    \n",
    "    def _update_model_weights(self, model_idx, loss):\n",
    "        \"\"\"更新模型权重\"\"\"\n",
    "        try:\n",
    "            # 记录性能\n",
    "            self.performance_history.append({\n",
    "                'model_idx': model_idx,\n",
    "                'loss': loss,\n",
    "                'timestamp': datetime.now()\n",
    "            })\n",
    "            \n",
    "            # 更新权重\n",
    "            performance = np.exp(-loss)  # 损失越小,性能越好\n",
    "            self.model_ensemble.weights[model_idx] = performance\n",
    "            \n",
    "            # 归一化权重\n",
    "            total = np.sum(self.model_ensemble.weights)\n",
    "            self.model_ensemble.weights /= total\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"更新模型权重失败: {str(e)}\")\n",
    "    \n",
    "    def _update_training_status(self):\n",
    "        \"\"\"更新训练状态\"\"\"\n",
    "        try:\n",
    "            # 计算训练进度\n",
    "            total_steps = self.epochs * self.total_batches\n",
    "            current_steps = self.current_epoch * self.total_batches + self.current_batch\n",
    "            progress = current_steps / total_steps\n",
    "            \n",
    "            # 更新监控指标\n",
    "            self.performance_monitor.update_metrics({\n",
    "                'progress': progress,\n",
    "                'current_epoch': self.current_epoch,\n",
    "                'current_batch': self.current_batch,\n",
    "                'loss': np.mean([p['loss'] for p in self.performance_history])\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"更新训练状态失败: {str(e)}\")\n",
    "    \n",
    "    def _record_performance(self):\n",
    "        \"\"\"记录训练性能\"\"\"\n",
    "        try:\n",
    "            metrics = {\n",
    "                'timestamp': datetime.now(),\n",
    "                'epoch': self.current_epoch,\n",
    "                'batch': self.current_batch,\n",
    "                'loss': np.mean([p['loss'] for p in self.performance_history]),\n",
    "                'resource_usage': {\n",
    "                    'cpu': self.resource_monitor.cpu_usage,\n",
    "                    'memory': self.resource_monitor.memory_usage,\n",
    "                    'gpu': self.resource_monitor.gpu_usage\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            self.performance_monitor.update_metrics(metrics)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"记录性能失败: {str(e)}\")\n",
    "    \n",
    "    def _preprocess_batch(self, batch):\n",
    "        \"\"\"预处理训练批次\"\"\"\n",
    "        try:\n",
    "            return {\n",
    "                'input': tf.convert_to_tensor(batch['input']),\n",
    "                'target': tf.convert_to_tensor(batch['target'])\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"预处理批次失败: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _on_batch_complete(self):\n",
    "        \"\"\"批次训练完成回调\"\"\"\n",
    "        try:\n",
    "            # 保存检查点\n",
    "            if self.current_batch % self.config['save_frequency'] == 0:\n",
    "                self._save_checkpoint()\n",
    "            \n",
    "            # 评估性能\n",
    "            if self.current_batch % self.config['eval_frequency'] == 0:\n",
    "                self._evaluate_performance()\n",
    "                \n",
    "            # 调整学习率\n",
    "            if self.current_batch % self.config['lr_update_frequency'] == 0:\n",
    "                self._adjust_learning_rate()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"批次完成处理失败: {str(e)}\")\n",
    "    \n",
    "    def _save_checkpoint(self):\n",
    "        \"\"\"保存训练检查点\"\"\"\n",
    "        try:\n",
    "            checkpoint = {\n",
    "                'epoch': self.current_epoch,\n",
    "                'batch': self.current_batch,\n",
    "                'model_states': [model.get_weights() for model in self.model_ensemble.models],\n",
    "                'optimizer_state': self.model_ensemble.optimizer.get_weights(),\n",
    "                'performance_history': list(self.performance_history)\n",
    "            }\n",
    "            \n",
    "            save_path = f\"checkpoints/checkpoint_e{self.current_epoch}_b{self.current_batch}.h5\"\n",
    "            tf.keras.models.save_model(checkpoint, save_path)\n",
    "            logger.info(f\"保存检查点: {save_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"保存检查点失败: {str(e)}\")\n",
    "    \n",
    "    def _evaluate_performance(self):\n",
    "        \"\"\"评估训练性能\"\"\"\n",
    "        try:\n",
    "            # 计算平均损失\n",
    "            avg_loss = np.mean([p['loss'] for p in self.performance_history])\n",
    "            \n",
    "            # 计算性能改进\n",
    "            if len(self.performance_history) > 1:\n",
    "                prev_loss = self.performance_history[-2]['loss']\n",
    "                improvement = (prev_loss - avg_loss) / prev_loss\n",
    "                \n",
    "                if improvement < self.config['min_improvement']:\n",
    "                    logger.warning(\"性能改进不足\")\n",
    "                    \n",
    "            logger.info(f\"当前平均损失: {avg_loss:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"评估性能失败: {str(e)}\")\n",
    "    \n",
    "    def _adjust_learning_rate(self):\n",
    "        \"\"\"调整学习率\"\"\"\n",
    "        try:\n",
    "            if len(self.performance_history) < 2:\n",
    "                return\n",
    "                \n",
    "            # 计算最近的性能变化\n",
    "            recent_loss = np.mean([p['loss'] for p in list(self.performance_history)[-10:]])\n",
    "            previous_loss = np.mean([p['loss'] for p in list(self.performance_history)[-20:-10]])\n",
    "            \n",
    "            # 根据性能变化调整学习率\n",
    "            if recent_loss > previous_loss:\n",
    "                new_lr = self.model_ensemble.optimizer.learning_rate * 0.8\n",
    "                self.model_ensemble.optimizer.learning_rate.assign(new_lr)\n",
    "                logger.info(f\"降低学习率至: {new_lr:.6f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"调整学习率失败: {str(e)}\")\n",
    "\n",
    "    def get_training_status(self):\n",
    "        \"\"\"获取训练状态\"\"\"\n",
    "        return {\n",
    "            'is_training': self.is_training,\n",
    "            'is_paused': self.pause_training,\n",
    "            'current_epoch': self.current_epoch,\n",
    "            'current_batch': self.current_batch,\n",
    "            'total_epochs': self.epochs,\n",
    "            'total_batches': self.total_batches,\n",
    "            'progress': (self.current_epoch * self.total_batches + self.current_batch) / \n",
    "                       (self.epochs * self.total_batches)\n",
    "        }\n",
    "\n",
    "    def get_performance_metrics(self):\n",
    "        \"\"\"获取性能指标\"\"\"\n",
    "        if not self.performance_history:\n",
    "            return None\n",
    "            \n",
    "        recent_records = list(self.performance_history)[-100:]\n",
    "        return {\n",
    "            'average_loss': np.mean([r['loss'] for r in recent_records]),\n",
    "            'min_loss': np.min([r['loss'] for r in recent_records]),\n",
    "            'max_loss': np.max([r['loss'] for r in recent_records]),\n",
    "            'loss_trend': self._calculate_trend([r['loss'] for r in recent_records])\n",
    "        }\n",
    "\n",
    "    def _calculate_trend(self, values):\n",
    "        \"\"\"计算趋势\"\"\"\n",
    "        if len(values) < 2:\n",
    "            return \"INSUFFICIENT_DATA\"\n",
    "            \n",
    "        # 使用简单线性回归\n",
    "        x = np.arange(len(values))\n",
    "        slope = np.polyfit(x, values, 1)[0]\n",
    "        \n",
    "        if slope < -0.01:\n",
    "            return \"IMPROVING\"\n",
    "        elif slope > 0.01:\n",
    "            return \"DEGRADING\"\n",
    "        else:\n",
    "            return \"STABLE\"\n",
    "\n",
    "    def _calculate_next_period(self, current_period):\n",
    "        \"\"\"计算下一期号\"\"\"\n",
    "        try:\n",
    "            date_part, num_part = current_period.split('-')\n",
    "            current_date = datetime.strptime(date_part, \"%Y%m%d\")\n",
    "            current_num = int(num_part)\n",
    "            \n",
    "            if current_num < 1440:\n",
    "                return f\"{date_part}-{current_num+1:04d}\"\n",
    "            else:\n",
    "                next_date = current_date + timedelta(days=1)\n",
    "                return f\"{next_date.strftime('%Y%m%d')}-0001\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"计算下一期号失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _get_last_processed_period(self):\n",
    "        \"\"\"获取最后处理的期号\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.issue_file):\n",
    "                with open(self.issue_file, 'r') as f:\n",
    "                    return f.read().strip()\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"读取期号文件失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _save_processed_period(self, period):\n",
    "        \"\"\"保存处理完成的期号\"\"\"\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(self.issue_file), exist_ok=True)\n",
    "            with open(self.issue_file, 'w') as f:\n",
    "                f.write(period)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"保存期号文件失败: {str(e)}\")\n",
    "\n",
    "    def _generate_sequence_periods(self, start_period, length):\n",
    "        \"\"\"生成连续的期号序列\"\"\"\n",
    "        periods = [start_period]\n",
    "        current = start_period\n",
    "        for _ in range(length-1):\n",
    "            current = self._calculate_next_period(current)\n",
    "            if not current:\n",
    "                return None\n",
    "            periods.append(current)\n",
    "        return periods\n",
    "\n",
    "    def _fetch_training_data(self, start_period):\n",
    "        \"\"\"获取训练数据（增加数据验证）\"\"\"\n",
    "        try:\n",
    "            # 生成需要获取的期号范围\n",
    "            total_periods = self._generate_sequence_periods(start_period, 14400+2880)\n",
    "            if not total_periods or len(total_periods) != 14400+2880:\n",
    "                logger.error(\"期号生成不完整\")\n",
    "                return None\n",
    "                \n",
    "            # 获取数据\n",
    "            query = \"\"\"\n",
    "                SELECT number, date_period FROM admin_tab\n",
    "                WHERE date_period IN %s\n",
    "                ORDER BY date_period ASC\n",
    "            \"\"\"\n",
    "            data = data_manager.execute_query(query, (tuple(total_periods),))\n",
    "            \n",
    "            # 验证数据完整性\n",
    "            if len(data) != 14400+2880:\n",
    "                logger.error(f\"数据不完整，预期{14400+2880}条，实际获取{len(data)}条\")\n",
    "                return None\n",
    "                \n",
    "            # 新增数据量打印\n",
    "            logger.info(f\"获取到训练数据 {len(data)} 条，时间范围: {data[0]['date_period']} 至 {data[-1]['date_period']}\")\n",
    "            \n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取训练数据失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _validate_sequence(self, sequence):\n",
    "        \"\"\"验证训练序列有效性\"\"\"\n",
    "        try:\n",
    "            if sequence is None:\n",
    "                return False\n",
    "                \n",
    "            # 检查是否为numpy数组\n",
    "            if not isinstance(sequence, np.ndarray):\n",
    "                logger.warning(f\"序列类型错误: {type(sequence)}\")\n",
    "                return False\n",
    "                \n",
    "            # 检查数据形状\n",
    "            if sequence.shape != (14400+2880, 5):\n",
    "                logger.warning(f\"无效序列形状: {sequence.shape}\")\n",
    "                return False\n",
    "                \n",
    "            # 检查数值范围\n",
    "            if np.min(sequence) < -1 or np.max(sequence) > 1:\n",
    "                logger.warning(f\"数值范围异常: [{np.min(sequence):.2f}, {np.max(sequence):.2f}]\")\n",
    "                return False\n",
    "                \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"序列验证失败: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def training_loop(self):\n",
    "        \"\"\"训练主循环\"\"\"\n",
    "        logger.info(\"开始训练循环\")\n",
    "        issue_file = \"D:/JupyterWork/notebooks/period_number/issue.txt\"\n",
    "        \n",
    "        # 添加损失值历史记录\n",
    "        loss_history = []\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                # 1. 读取上一次训练的最后期号\n",
    "                try:\n",
    "                    with open(issue_file, 'r') as f:\n",
    "                        last_issue = f.read().strip()\n",
    "                    logger.info(f\"读取到上次期号: {last_issue}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"读取期号文件失败: {str(e)}\")\n",
    "                    time.sleep(60)\n",
    "                    continue\n",
    "                \n",
    "                # 2. 计算下一个目标期号\n",
    "                next_target_issue = self._calculate_next_issue(last_issue)\n",
    "                logger.info(f\"下一目标期号: {next_target_issue}\")\n",
    "                \n",
    "                # 3. 构建训练序列\n",
    "                try:\n",
    "                    sequence = self.data_manager.get_sequence(\n",
    "                        start_issue=self._get_start_issue(next_target_issue),\n",
    "                        end_issue=next_target_issue\n",
    "                    )\n",
    "                    if sequence is None:\n",
    "                        logger.info(f\"等待期号 {next_target_issue} 的数据...\")\n",
    "                        time.sleep(60)\n",
    "                        continue\n",
    "                        \n",
    "                    logger.info(f\"获取到序列数据，形状: {sequence.shape}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"获取训练序列失败: {str(e)}\")\n",
    "                    time.sleep(60)\n",
    "                    continue\n",
    "                \n",
    "                # 4. 训练所有模型\n",
    "                input_data = sequence[:-2880]\n",
    "                target_data = sequence[-2880:]\n",
    "                \n",
    "                # 添加batch维度\n",
    "                input_data = np.expand_dims(input_data, axis=0)\n",
    "                target_data = np.expand_dims(target_data, axis=0)\n",
    "                \n",
    "                # 记录每个模型的训练前损失值\n",
    "                with self.model_ensemble.session.as_default():\n",
    "                    with self.model_ensemble.graph.as_default():\n",
    "                        for i, model in enumerate(self.model_ensemble.models):\n",
    "                            try:\n",
    "                                # 训练前评估\n",
    "                                initial_loss = model.evaluate(input_data, target_data, verbose=0)\n",
    "                                \n",
    "                                # 训练\n",
    "                                loss = model.train_on_batch(input_data, target_data)\n",
    "                                \n",
    "                                # 训练后评估\n",
    "                                final_loss = model.evaluate(input_data, target_data, verbose=0)\n",
    "                                \n",
    "                                logger.info(f\"模型 {i+1} 训练: 初始损失={initial_loss:.4f}, \"\n",
    "                                          f\"最终损失={final_loss:.4f}, 变化={initial_loss-final_loss:.4f}\")\n",
    "                                \n",
    "                                loss_history.append({\n",
    "                                    'model': i+1,\n",
    "                                    'issue': next_target_issue,\n",
    "                                    'initial_loss': initial_loss,\n",
    "                                    'final_loss': final_loss\n",
    "                                })\n",
    "                                \n",
    "                            except Exception as e:\n",
    "                                logger.error(f\"模型 {i+1} 训练失败: {str(e)}\")\n",
    "                                continue\n",
    "                \n",
    "                # 5. 保存新的期号\n",
    "                try:\n",
    "                    with open(issue_file, 'w') as f:\n",
    "                        f.write(next_target_issue)\n",
    "                    logger.info(f\"已更新期号为: {next_target_issue}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"更新期号文件失败: {str(e)}\")\n",
    "                \n",
    "                # 6. 等待一段时间再继续\n",
    "                time.sleep(10)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"训练循环出错: {str(e)}\")\n",
    "                time.sleep(60)\n",
    "                continue\n",
    "            \n",
    "    def _get_start_issue(self, end_issue):\n",
    "        \"\"\"计算起始期号（往前推12天）\"\"\"\n",
    "        date_str, _ = end_issue.split('-')\n",
    "        end_date = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "        start_date = end_date - timedelta(days=12)\n",
    "        return f\"{start_date.strftime('%Y%m%d')}-0001\"\n",
    "\n",
    "    def _calculate_next_issue(self, current_issue):\n",
    "        \"\"\"计算下一个期号\"\"\"\n",
    "        # 解析当前期号 (格式: YYYYMMDD-XXXX)\n",
    "        date_str, period = current_issue.split('-')\n",
    "        year = int(date_str[:4])\n",
    "        month = int(date_str[4:6])\n",
    "        day = int(date_str[6:8])\n",
    "        period_num = int(period)\n",
    "        \n",
    "        # 计算下一期\n",
    "        if period_num < 1440:\n",
    "            # 同一天的下一期\n",
    "            next_period = f\"{period_num + 1:04d}\"\n",
    "            next_date = date_str\n",
    "        else:\n",
    "            # 下一天的第一期\n",
    "            next_period = \"0001\"\n",
    "            next_date = datetime(year, month, day) + timedelta(days=1)\n",
    "            next_date = next_date.strftime(\"%Y%m%d\")\n",
    "            \n",
    "        return f\"{next_date}-{next_period}\"\n",
    "        \n",
    "    def _get_training_sequence(self, target_issue):\n",
    "        \"\"\"获取训练序列\"\"\"\n",
    "        try:\n",
    "            # 计算起始期号（往前推12天）\n",
    "            start_issue = self._get_start_issue(target_issue)\n",
    "            \n",
    "            # 从数据库获取序列\n",
    "            sequence = self.data_manager.get_sequence(start_issue, target_issue)\n",
    "            \n",
    "            # 验证序列长度\n",
    "            if sequence is not None and sequence.shape[0] != 14400 + 2880:\n",
    "                logger.error(f\"序列长度不正确: {sequence.shape[0]}, 应为 {14400 + 2880}\")\n",
    "                return None\n",
    "                \n",
    "            return sequence\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"获取训练序列失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def ensure_training_thread(self):\n",
    "        \"\"\"确保训练线程运行\"\"\"\n",
    "        if not self._training_loop_running:\n",
    "            self._training_loop_running = True\n",
    "            self._training_thread = threading.Thread(\n",
    "                target=self.training_loop,\n",
    "                name=\"TrainingThread\",\n",
    "                daemon=True\n",
    "            )\n",
    "            self._training_thread.start()\n",
    "            logger.info(\"训练线程已启动\")\n",
    "\n",
    "    def _validate_training_effect(self, model, input_data, target_data, model_index):\n",
    "        \"\"\"验证训练效果\"\"\"\n",
    "        try:\n",
    "            # 添加batch维度\n",
    "            input_data = np.expand_dims(input_data, axis=0)  # 转换为 (1, sequence_length, features)\n",
    "            target_data = np.expand_dims(target_data, axis=0)  # 转换为 (1, target_length, features)\n",
    "            \n",
    "            # 训练前预测\n",
    "            pred_before = model.predict(input_data)\n",
    "            \n",
    "            # 训练\n",
    "            loss = model.train_on_batch(input_data, target_data)\n",
    "            \n",
    "            # 训练后预测\n",
    "            pred_after = model.predict(input_data)\n",
    "            \n",
    "            # 计算预测变化\n",
    "            pred_change = np.mean(np.abs(pred_after - pred_before))\n",
    "            \n",
    "            # 计算准确度变化\n",
    "            acc_before = np.mean(np.abs(pred_before - target_data))\n",
    "            acc_after = np.mean(np.abs(pred_after - target_data))\n",
    "            \n",
    "            logger.info(f\"模型 {model_index} 训练效果:\"\n",
    "                       f\"\\n - 预测变化: {pred_change:.4f}\"\n",
    "                       f\"\\n - 准确度提升: {acc_before-acc_after:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"验证训练效果失败: {str(e)}\")\n",
    "\n",
    "    def check_training_status(self):\n",
    "        \"\"\"检查训练状态\"\"\"\n",
    "        try:\n",
    "            # 初始化状态字典\n",
    "            status = {\n",
    "                'is_training': self._training_loop_running,\n",
    "                'thread_alive': False,\n",
    "                'models_compiled': False,\n",
    "                'last_losses': [],\n",
    "                'weights_initialized': []\n",
    "            }\n",
    "            \n",
    "            # 检查训练线程状态\n",
    "            if self._training_thread:\n",
    "                status['thread_alive'] = self._training_thread.is_alive()\n",
    "            \n",
    "            # 检查模型编译状态\n",
    "            if self.model_ensemble and hasattr(self.model_ensemble, 'models'):\n",
    "                with self.model_ensemble.session.as_default():\n",
    "                    with self.model_ensemble.graph.as_default():\n",
    "                        models_compiled = []\n",
    "                        weights_initialized = []\n",
    "                        last_losses = []\n",
    "                        \n",
    "                        for model in self.model_ensemble.models:\n",
    "                            # 检查模型是否已编译\n",
    "                            has_optimizer = hasattr(model, 'optimizer')\n",
    "                            models_compiled.append(has_optimizer)\n",
    "                            \n",
    "                            # 检查权重是否已初始化\n",
    "                            if hasattr(model, 'get_weights'):\n",
    "                                weights = model.get_weights()\n",
    "                                weights_initialized.append(\n",
    "                                    len(weights) > 0 and any(np.any(w != 0) for w in weights)\n",
    "                                )\n",
    "                            else:\n",
    "                                weights_initialized.append(False)\n",
    "                            \n",
    "                            # 尝试获取最后的损失值\n",
    "                            if hasattr(model, 'history') and model.history:\n",
    "                                if model.history.history and 'loss' in model.history.history:\n",
    "                                    last_losses.append(model.history.history['loss'][-1])\n",
    "                                else:\n",
    "                                    last_losses.append(None)\n",
    "                            else:\n",
    "                                last_losses.append(None)\n",
    "                        \n",
    "                        status['models_compiled'] = all(models_compiled)\n",
    "                        status['weights_initialized'] = weights_initialized\n",
    "                        status['last_losses'] = last_losses\n",
    "            \n",
    "            logger.info(f\"训练状态检查完成: {status}\")\n",
    "            return status\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"检查训练状态失败: {str(e)}\")\n",
    "            # 返回基本状态信息\n",
    "            return {\n",
    "                'is_training': False,\n",
    "                'thread_alive': False,\n",
    "                'models_compiled': False,\n",
    "                'last_losses': [],\n",
    "                'weights_initialized': [],\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "# 然后创建训练管理器\n",
    "training_manager = TrainingManager(model_ensemble=model_core)\n",
    "\n",
    "# 最后启动训练线程\n",
    "if not hasattr(training_manager, '_training_loop_running'):\n",
    "    training_manager.ensure_training_thread()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9e47a91-a44c-4332-bdac-4a2b76d29a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "当前训练状态:\n",
      "训练循环运行: False\n",
      "训练线程活跃: False\n",
      "模型已编译: True\n",
      "\n",
      "模型权重状态:\n",
      "模型 1: 已初始化\n",
      "模型 2: 已初始化\n",
      "模型 3: 已初始化\n",
      "模型 4: 已初始化\n",
      "模型 5: 已初始化\n",
      "模型 6: 已初始化\n",
      "\n",
      "最近损失值:\n",
      "模型 1: 无损失记录\n",
      "模型 2: 无损失记录\n",
      "模型 3: 无损失记录\n",
      "模型 4: 无损失记录\n",
      "模型 5: 无损失记录\n",
      "模型 6: 无损失记录\n",
      "\n",
      "启动训练线程...\n",
      "WARNING:tensorflow:From D:\\ruanjiananzhuang\\anaconda\\envs\\ace\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:635: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\ruanjiananzhuang\\anaconda\\envs\\ace\\Lib\\site-packages\\keras\\src\\engine\\training_utils_v1.py:50: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ruanjiananzhuang\\anaconda\\envs\\ace\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n",
      "模型 1 训练失败: Incompatible shapes: [1,14402,5] vs. [1,2880,5]\n",
      "\t [[{{node metrics/mean_absolute_error/sub}}]]\n",
      "模型 2 训练失败: Incompatible shapes: [1,14402,5] vs. [1,2880,5]\n",
      "\t [[{{node metrics_2/mean_absolute_error/sub}}]]\n",
      "模型 3 训练失败: Incompatible shapes: [1,14402,5] vs. [1,2880,5]\n",
      "\t [[{{node metrics_4/mean_absolute_error/sub}}]]\n",
      "模型 4 训练失败: Incompatible shapes: [1,14402,5] vs. [1,2880,5]\n",
      "\t [[{{node metrics_6/mean_absolute_error/sub}}]]\n",
      "模型 5 训练失败: Incompatible shapes: [1,14402,5] vs. [1,2880,5]\n",
      "\t [[{{node loss_4/dense_9_loss/SquaredDifference}}]]\n",
      "模型 6 训练失败: Incompatible shapes: [1,14402,5] vs. [1,2880,5]\n",
      "\t [[{{node metrics_10/mean_absolute_error/sub}}]]\n"
     ]
    }
   ],
   "source": [
    "# 重置训练管理器\n",
    "training_manager = TrainingManager(model_ensemble=model_core)\n",
    "\n",
    "# 检查训练状态\n",
    "status = training_manager.check_training_status()\n",
    "print(\"\\n当前训练状态:\")\n",
    "print(f\"训练循环运行: {status.get('is_training', False)}\")\n",
    "print(f\"训练线程活跃: {status.get('thread_alive', False)}\")\n",
    "print(f\"模型已编译: {status.get('models_compiled', False)}\")\n",
    "\n",
    "if 'error' in status:\n",
    "    print(f\"\\n检查状态时出现错误: {status['error']}\")\n",
    "\n",
    "# 检查每个模型的状态\n",
    "if status.get('weights_initialized'):\n",
    "    print(\"\\n模型权重状态:\")\n",
    "    for i, initialized in enumerate(status['weights_initialized']):\n",
    "        print(f\"模型 {i+1}: {'已初始化' if initialized else '未初始化'}\")\n",
    "\n",
    "# 检查损失值\n",
    "if status.get('last_losses'):\n",
    "    print(\"\\n最近损失值:\")\n",
    "    for i, loss in enumerate(status['last_losses']):\n",
    "        if loss is not None:\n",
    "            print(f\"模型 {i+1}: {loss:.4f}\")\n",
    "        else:\n",
    "            print(f\"模型 {i+1}: 无损失记录\")\n",
    "\n",
    "# 启动训练\n",
    "print(\"\\n启动训练线程...\")\n",
    "training_manager.ensure_training_thread()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (ace)",
   "language": "python",
   "name": "ace"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
